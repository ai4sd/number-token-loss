
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for the `ntloss` python package">
      
      
      
      
        <link rel="prev" href="../..">
      
      
        <link rel="next" href="../utils/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>Loss Classes - Number Token Loss</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"IBM Plex Sans";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/nav-code-style.css">
    
      <link rel="stylesheet" href="../../css/mkdocstrings-chips.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ntlosscore" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Number Token Loss" class="md-header__button md-logo" aria-label="Number Token Loss" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Number Token Loss
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Loss Classes
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/AI4SD/number-token-loss" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    AI4SD/number-token-loss
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Overview

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  
    
  
  API Documentation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Number Token Loss" class="md-nav__button md-logo" aria-label="Number Token Loss" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Number Token Loss
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/AI4SD/number-token-loss" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    AI4SD/number-token-loss
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API Documentation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            API Documentation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Loss Classes
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Loss Classes
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ntloss.core" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-module"></code>&nbsp;core
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" core">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ntloss.core.AbstractNTLoss" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;AbstractNTLoss
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" AbstractNTLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ntloss.core.AbstractNTLoss.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.AbstractNTLoss.setup_number_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;setup_number_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.AbstractNTLoss.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.AbstractNTLoss.reweigh_fn" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;reweigh_fn
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLossDotProduct" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NTLossDotProduct
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NTLossDotProduct">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLossDotProduct.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLossDotProduct.setup_max_dist" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;setup_max_dist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLossDotProduct.predict_numbers" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;predict_numbers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLossDotProduct.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLoss" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NTLoss
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NTLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLoss.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLoss.setup_distance_lookup" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;setup_distance_lookup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NTLoss.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NumberLevelLoss" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NumberLevelLoss
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NumberLevelLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NumberLevelLoss.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NumberLevelLoss.setup_max_dist" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;setup_max_dist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NumberLevelLoss.convert_digits_to_numbers" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;convert_digits_to_numbers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ntloss.core.NumberLevelLoss.forward" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="ntlosscore">ntloss.core<a class="headerlink" href="#ntlosscore" title="Permanent link">&para;</a></h1>


<div class="doc doc-object doc-module">



<h2 id="ntloss.core" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code>            <code>ntloss.core</code>


<a href="#ntloss.core" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="ntloss.core.AbstractNTLoss" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>AbstractNTLoss</code>


<a href="#ntloss.core.AbstractNTLoss" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>








              <details class="quote">
                <summary>Source code in <code>ntloss/core.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-14"> 14</a></span>
<span class="normal"><a href="#__codelineno-0-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-0-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-0-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-0-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-0-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-0-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-0-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-0-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-0-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-0-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-0-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-0-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-0-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-0-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-0-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-0-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-0-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-0-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-0-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-0-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-0-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-0-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-0-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-0-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-0-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-0-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-0-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-0-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span>
<span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span>
<span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span>
<span class="normal"><a href="#__codelineno-0-206">206</a></span>
<span class="normal"><a href="#__codelineno-0-207">207</a></span>
<span class="normal"><a href="#__codelineno-0-208">208</a></span>
<span class="normal"><a href="#__codelineno-0-209">209</a></span>
<span class="normal"><a href="#__codelineno-0-210">210</a></span>
<span class="normal"><a href="#__codelineno-0-211">211</a></span>
<span class="normal"><a href="#__codelineno-0-212">212</a></span>
<span class="normal"><a href="#__codelineno-0-213">213</a></span>
<span class="normal"><a href="#__codelineno-0-214">214</a></span>
<span class="normal"><a href="#__codelineno-0-215">215</a></span>
<span class="normal"><a href="#__codelineno-0-216">216</a></span>
<span class="normal"><a href="#__codelineno-0-217">217</a></span>
<span class="normal"><a href="#__codelineno-0-218">218</a></span>
<span class="normal"><a href="#__codelineno-0-219">219</a></span>
<span class="normal"><a href="#__codelineno-0-220">220</a></span>
<span class="normal"><a href="#__codelineno-0-221">221</a></span>
<span class="normal"><a href="#__codelineno-0-222">222</a></span>
<span class="normal"><a href="#__codelineno-0-223">223</a></span>
<span class="normal"><a href="#__codelineno-0-224">224</a></span>
<span class="normal"><a href="#__codelineno-0-225">225</a></span>
<span class="normal"><a href="#__codelineno-0-226">226</a></span>
<span class="normal"><a href="#__codelineno-0-227">227</a></span>
<span class="normal"><a href="#__codelineno-0-228">228</a></span>
<span class="normal"><a href="#__codelineno-0-229">229</a></span>
<span class="normal"><a href="#__codelineno-0-230">230</a></span>
<span class="normal"><a href="#__codelineno-0-231">231</a></span>
<span class="normal"><a href="#__codelineno-0-232">232</a></span>
<span class="normal"><a href="#__codelineno-0-233">233</a></span>
<span class="normal"><a href="#__codelineno-0-234">234</a></span>
<span class="normal"><a href="#__codelineno-0-235">235</a></span>
<span class="normal"><a href="#__codelineno-0-236">236</a></span>
<span class="normal"><a href="#__codelineno-0-237">237</a></span>
<span class="normal"><a href="#__codelineno-0-238">238</a></span>
<span class="normal"><a href="#__codelineno-0-239">239</a></span>
<span class="normal"><a href="#__codelineno-0-240">240</a></span>
<span class="normal"><a href="#__codelineno-0-241">241</a></span>
<span class="normal"><a href="#__codelineno-0-242">242</a></span>
<span class="normal"><a href="#__codelineno-0-243">243</a></span>
<span class="normal"><a href="#__codelineno-0-244">244</a></span>
<span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-14" name="__codelineno-0-14"></a><span class="k">class</span><span class="w"> </span><span class="nc">AbstractNTLoss</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<a id="__codelineno-0-15" name="__codelineno-0-15"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a>        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a>        <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a>        <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a>    <span class="p">):</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="sd">        NTL constructor.</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="sd">            tokenizer: Standard HF tokenizer.</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="sd">            vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="sd">                tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="sd">            digit_level: Whether to ensure only digits are considered number tokens,</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="sd">                stabilizing training with NTL. Defaults to True. Used for most</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="sd">                experiments in the ICML paper.</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="sd">            reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="sd">                number tokens. Defaults to True.</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="sd">                NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="sd">                incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="k">if</span> <span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size_validated</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span> <span class="o">=</span> <span class="n">digit_level</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span> <span class="o">=</span> <span class="n">reweigh</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">setup_number_tokens</span><span class="p">()</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_number_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Setting up attributes needed by NT loss&quot;&quot;&quot;</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>        <span class="c1"># Add digits to vocab if not there yet.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span><span class="p">:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a>            <span class="n">new_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))))</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a>        <span class="k">if</span> <span class="n">vocab_size</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">new_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a>            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Added </span><span class="si">{</span><span class="n">new_tokens</span><span class="si">}</span><span class="s2"> new tokens for number token loss&quot;</span><span class="p">)</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a>        <span class="n">vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">:</span> <span class="n">FloatTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">))</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a>        <span class="c1"># Try to convert each token to a float after stripping the space prefix</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a>        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a>            <span class="k">if</span> <span class="n">is_number</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span><span class="p">:</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a>                    <span class="c1"># NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a>                    <span class="c1"># This stabilizes training with NTL. Can be altered though, see paper experiments.</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a>                    <span class="c1"># Excludes tokens that are numbers in other languages like ႘ and tokens with space pre-/postfix like ` 2`.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>                    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isascii</span><span class="p">()</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;=</span> <span class="nb">float</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">9</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span> <span class="o">=</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">)</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">):</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>                <span class="s2">&quot;At least one token needs to be not a number, otherwise `ignore_index` cannot be set up safely&quot;</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>            <span class="p">)</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">nan_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_nts</span> <span class="o">:=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">10</span><span class="p">:</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>                <span class="sa">f</span><span class="s2">&quot;You requested digit-level but </span><span class="si">{</span><span class="n">num_nts</span><span class="si">}</span><span class="s2"> number tokens were identified: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>            <span class="p">)</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>    <span class="nd">@abstractmethod</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>        <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>        <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> <span class="o">...</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias to self.forward&quot;&quot;&quot;</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>        <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>        <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a><span class="sd">        Scale the NT loss element-wise using the logit weight on number tokens.</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a><span class="sd">        NOTE: This reweighing ensures that if ground truth is a number token</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a><span class="sd">            but most probability mass is on text tokens, the loss will be *higher*</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a><span class="sd">            than the worst possible number token. This is an edge case in practice.</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a><span class="sd">            logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a><span class="sd">            loss: 1D Tensor over all number tokens in batch.</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a><span class="sd">            number_token_positions: 2D Tensor of shape BS x T indicating for which tokens</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a><span class="sd">                the NT loss was computed.</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a><span class="sd">            A 1D Tensor over all number tokens in batch with the scaled NT losses.</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>        <span class="c1"># Take softmax over logits of all tokens in vocab and compute NT logit weight</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>        <span class="n">softmax_probs_all</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>        <span class="n">nt_logit_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>            <span class="n">softmax_probs_all</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>        <span class="p">)[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>        <span class="c1"># Apply weights for NTL element-wise</span>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>        <span class="n">loss</span> <span class="o">*=</span> <span class="n">nt_logit_weight</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>        <span class="c1"># Apply regularization</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>        <span class="c1"># NOTE: We could consider reweighing here with the max for that label token</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>        <span class="c1"># rather than the global max</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>        <span class="n">loss</span> <span class="o">+=</span> <span class="p">(</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>            <span class="mf">1.01</span>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">nt_logit_weight</span><span class="p">)</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a>        <span class="p">)</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>        <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_validate_inputs</span><span class="p">(</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a>        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LongTensor</span><span class="p">],</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>        <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>    <span class="p">):</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Private method to perform size and type checks.&quot;&quot;&quot;</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a>        <span class="k">if</span> <span class="p">(</span><span class="n">td</span> <span class="o">:=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="p">(</span><span class="n">ne</span> <span class="o">:=</span> <span class="n">logits</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>                <span class="sa">f</span><span class="s2">&quot;Logits have to be non-empty 3D Tensor, not </span><span class="si">{</span><span class="n">td</span><span class="si">}</span><span class="s2">D with </span><span class="si">{</span><span class="n">ne</span><span class="si">}</span><span class="s2"> elements&quot;</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>            <span class="p">)</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Logits have to be FloatTensor.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>            <span class="k">return</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels have to be LongTensor, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>        <span class="k">if</span> <span class="p">(</span><span class="n">b</span> <span class="o">:=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="n">a</span> <span class="o">:=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>                <span class="sa">f</span><span class="s2">&quot;Logit and label sizes of first 2 dims have to match: </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a>            <span class="p">)</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>        <span class="k">if</span> <span class="p">(</span><span class="n">td</span> <span class="o">:=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="p">(</span><span class="n">ne</span> <span class="o">:=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>                <span class="sa">f</span><span class="s2">&quot;Labels have to be non-empty 2D Tensor, not </span><span class="si">{</span><span class="n">td</span><span class="si">}</span><span class="s2">D with </span><span class="si">{</span><span class="n">ne</span><span class="si">}</span><span class="s2"> elements&quot;</span>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a>            <span class="p">)</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a>        <span class="k">if</span> <span class="n">loss_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>            <span class="k">if</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>                    <span class="s2">&quot;Loss mask has to be 2D Tensor of same shape as labels.&quot;</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>                <span class="p">)</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">loss_weights</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;loss_mask must be ≥ 0.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size_validated</span><span class="p">:</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>            <span class="n">logits_vocab_size</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>            <span class="k">if</span> <span class="n">logits_vocab_size</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">:</span>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a>                        <span class="sa">f</span><span class="s2">&quot;The current `vocab_size` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">) does not match the model&#39;s vocab size&quot;</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>                        <span class="sa">f</span><span class="s2">&quot;logit dimension (</span><span class="si">{</span><span class="n">logits_vocab_size</span><span class="si">}</span><span class="s2">). Please check the value.&quot;</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a>                    <span class="p">)</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size_validated</span> <span class="o">=</span> <span class="kc">True</span>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_prepare_number_token_targets</span><span class="p">(</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-188" name="__codelineno-0-188"></a><span class="sd">        Prepare number-token targets and masks.</span>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a><span class="sd">            labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a><span class="sd">            loss_weights: Optional 2D Tensor of shape BS x T with loss weight for each token.</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a><span class="sd">            ignore_index: Label ID to ignore. Defaults to -100.</span>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a><span class="sd">            y: 2D Float Tensor of shape BS x T with target numeric values (NaN for non-number tokens).</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a><span class="sd">            loss_weight: 1D Tensor with a potentially individual loss weight for each number token position.</span>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>        <span class="n">labels</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>            <span class="n">LongTensor</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nan_id</span><span class="p">)</span>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a>        <span class="p">)</span>
<a id="__codelineno-0-202" name="__codelineno-0-202"></a>        <span class="c1"># Create a mask to filter out non-digit tokens</span>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a>        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="n">labels</span><span class="p">]</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a>        <span class="n">number_token_positions</span> <span class="o">=</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>        <span class="n">loss_weights</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-206" name="__codelineno-0-206"></a>            <span class="n">loss_weights</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-207" name="__codelineno-0-207"></a>            <span class="k">if</span> <span class="n">loss_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-208" name="__codelineno-0-208"></a>            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-209" name="__codelineno-0-209"></a>        <span class="p">)</span>
<a id="__codelineno-0-210" name="__codelineno-0-210"></a>        <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">loss_weights</span>
<a id="__codelineno-0-211" name="__codelineno-0-211"></a>
<a id="__codelineno-0-212" name="__codelineno-0-212"></a>    <span class="nd">@staticmethod</span>
<a id="__codelineno-0-213" name="__codelineno-0-213"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-214" name="__codelineno-0-214"></a>        <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-215" name="__codelineno-0-215"></a>        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-216" name="__codelineno-0-216"></a>        <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-217" name="__codelineno-0-217"></a>        <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-218" name="__codelineno-0-218"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-219" name="__codelineno-0-219"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-220" name="__codelineno-0-220"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-221" name="__codelineno-0-221"></a><span class="sd">        Applies the specified reduction type to the calculated loss.</span>
<a id="__codelineno-0-222" name="__codelineno-0-222"></a>
<a id="__codelineno-0-223" name="__codelineno-0-223"></a><span class="sd">        This method handles 3 types of reduction: &quot;mean&quot;, &quot;sum&quot;, and &quot;none&quot;.</span>
<a id="__codelineno-0-224" name="__codelineno-0-224"></a><span class="sd">        For &quot;mean&quot; and &quot;sum&quot;, it applies weighting using `loss_weights`.</span>
<a id="__codelineno-0-225" name="__codelineno-0-225"></a><span class="sd">        For &quot;none&quot;, it reshapes the loss back to the original batch and sequence</span>
<a id="__codelineno-0-226" name="__codelineno-0-226"></a><span class="sd">        dimensions.</span>
<a id="__codelineno-0-227" name="__codelineno-0-227"></a>
<a id="__codelineno-0-228" name="__codelineno-0-228"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-229" name="__codelineno-0-229"></a><span class="sd">            loss: 1D Tensor containing the loss for each number token in the batch.</span>
<a id="__codelineno-0-230" name="__codelineno-0-230"></a><span class="sd">            reduction: The reduction method (&quot;mean&quot;, &quot;sum&quot;, or &quot;none&quot;).</span>
<a id="__codelineno-0-231" name="__codelineno-0-231"></a><span class="sd">            loss_weights: 1D Tensor with a loss weight for each number token.</span>
<a id="__codelineno-0-232" name="__codelineno-0-232"></a><span class="sd">            number_token_positions: 2D boolean tensor of shape BS x T indicating</span>
<a id="__codelineno-0-233" name="__codelineno-0-233"></a><span class="sd">                the positions of number tokens.</span>
<a id="__codelineno-0-234" name="__codelineno-0-234"></a><span class="sd">            logits: 3D Tensor of shape BS x T x V, used to get the original shape</span>
<a id="__codelineno-0-235" name="__codelineno-0-235"></a><span class="sd">                for the &quot;none&quot; reduction.</span>
<a id="__codelineno-0-236" name="__codelineno-0-236"></a>
<a id="__codelineno-0-237" name="__codelineno-0-237"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-238" name="__codelineno-0-238"></a><span class="sd">            A Tensor representing the reduced loss:</span>
<a id="__codelineno-0-239" name="__codelineno-0-239"></a><span class="sd">                - 0D tensor if `reduction` is &quot;mean&quot; or &quot;sum&quot;.</span>
<a id="__codelineno-0-240" name="__codelineno-0-240"></a><span class="sd">                - 2D Tensor of shape BS x T if `reduction` is &quot;none&quot;.</span>
<a id="__codelineno-0-241" name="__codelineno-0-241"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-242" name="__codelineno-0-242"></a>        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
<a id="__codelineno-0-243" name="__codelineno-0-243"></a>            <span class="c1"># Mean pooling (weighted by loss mask)</span>
<a id="__codelineno-0-244" name="__codelineno-0-244"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
<a id="__codelineno-0-245" name="__codelineno-0-245"></a>                <span class="n">loss</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<a id="__codelineno-0-246" name="__codelineno-0-246"></a>            <span class="p">)</span> <span class="o">/</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-0-247" name="__codelineno-0-247"></a>        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
<a id="__codelineno-0-248" name="__codelineno-0-248"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<a id="__codelineno-0-249" name="__codelineno-0-249"></a>        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-250" name="__codelineno-0-250"></a>            <span class="c1"># Cast loss for number tokens back to Tensor of size BS x T</span>
<a id="__codelineno-0-251" name="__codelineno-0-251"></a>            <span class="n">loss_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">number_token_positions</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-252" name="__codelineno-0-252"></a>            <span class="n">loss_</span><span class="p">[</span><span class="n">number_token_positions</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">loss_weights</span>
<a id="__codelineno-0-253" name="__codelineno-0-253"></a>            <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<a id="__codelineno-0-254" name="__codelineno-0-254"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<a id="__codelineno-0-255" name="__codelineno-0-255"></a>
<a id="__codelineno-0-256" name="__codelineno-0-256"></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">[</span><span class="o">~</span><span class="n">number_token_positions</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
<a id="__codelineno-0-257" name="__codelineno-0-257"></a>                <span class="s2">&quot;NumberTokenLoss computed for non-digit tokens!&quot;</span>
<a id="__codelineno-0-258" name="__codelineno-0-258"></a>            <span class="p">)</span>
<a id="__codelineno-0-259" name="__codelineno-0-259"></a>
<a id="__codelineno-0-260" name="__codelineno-0-260"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-261" name="__codelineno-0-261"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-262" name="__codelineno-0-262"></a>
<a id="__codelineno-0-263" name="__codelineno-0-263"></a>        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="ntloss.core.AbstractNTLoss.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

<a href="#ntloss.core.AbstractNTLoss.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>NTL constructor.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td>
                  <code><span title="transformers.PreTrainedTokenizer">PreTrainedTokenizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Standard HF tokenizer.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional user-provided vocab size. If not provided, the
tokenizer's vocab size is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>digit_level</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to ensure only digits are considered number tokens,
stabilizing training with NTL. Defaults to True. Used for most
experiments in the ICML paper.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reweigh</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to scale the NTL using the logit weight on
number tokens. Defaults to True.
NOTE: The ICML paper does <em>not</em> use this option which can lead to
incorrect loss if most mass is placed outside of the number tokens.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-15">15</a></span>
<span class="normal"><a href="#__codelineno-0-16">16</a></span>
<span class="normal"><a href="#__codelineno-0-17">17</a></span>
<span class="normal"><a href="#__codelineno-0-18">18</a></span>
<span class="normal"><a href="#__codelineno-0-19">19</a></span>
<span class="normal"><a href="#__codelineno-0-20">20</a></span>
<span class="normal"><a href="#__codelineno-0-21">21</a></span>
<span class="normal"><a href="#__codelineno-0-22">22</a></span>
<span class="normal"><a href="#__codelineno-0-23">23</a></span>
<span class="normal"><a href="#__codelineno-0-24">24</a></span>
<span class="normal"><a href="#__codelineno-0-25">25</a></span>
<span class="normal"><a href="#__codelineno-0-26">26</a></span>
<span class="normal"><a href="#__codelineno-0-27">27</a></span>
<span class="normal"><a href="#__codelineno-0-28">28</a></span>
<span class="normal"><a href="#__codelineno-0-29">29</a></span>
<span class="normal"><a href="#__codelineno-0-30">30</a></span>
<span class="normal"><a href="#__codelineno-0-31">31</a></span>
<span class="normal"><a href="#__codelineno-0-32">32</a></span>
<span class="normal"><a href="#__codelineno-0-33">33</a></span>
<span class="normal"><a href="#__codelineno-0-34">34</a></span>
<span class="normal"><a href="#__codelineno-0-35">35</a></span>
<span class="normal"><a href="#__codelineno-0-36">36</a></span>
<span class="normal"><a href="#__codelineno-0-37">37</a></span>
<span class="normal"><a href="#__codelineno-0-38">38</a></span>
<span class="normal"><a href="#__codelineno-0-39">39</a></span>
<span class="normal"><a href="#__codelineno-0-40">40</a></span>
<span class="normal"><a href="#__codelineno-0-41">41</a></span>
<span class="normal"><a href="#__codelineno-0-42">42</a></span>
<span class="normal"><a href="#__codelineno-0-43">43</a></span>
<span class="normal"><a href="#__codelineno-0-44">44</a></span>
<span class="normal"><a href="#__codelineno-0-45">45</a></span>
<span class="normal"><a href="#__codelineno-0-46">46</a></span>
<span class="normal"><a href="#__codelineno-0-47">47</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-15" name="__codelineno-0-15"></a><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a>    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a>    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a>    <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a>    <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="p">):</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="sd">    NTL constructor.</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="sd">        tokenizer: Standard HF tokenizer.</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="sd">        vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="sd">            tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="sd">        digit_level: Whether to ensure only digits are considered number tokens,</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="sd">            stabilizing training with NTL. Defaults to True. Used for most</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="sd">            experiments in the ICML paper.</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="sd">        reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="sd">            number tokens. Defaults to True.</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="sd">            NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="sd">            incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="k">if</span> <span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_size_validated</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span> <span class="o">=</span> <span class="n">digit_level</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span> <span class="o">=</span> <span class="n">reweigh</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">setup_number_tokens</span><span class="p">()</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.AbstractNTLoss.setup_number_tokens" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">setup_number_tokens</span><span class="p">()</span></code>

<a href="#ntloss.core.AbstractNTLoss.setup_number_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Setting up attributes needed by NT loss</p>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-49">49</a></span>
<span class="normal"><a href="#__codelineno-0-50">50</a></span>
<span class="normal"><a href="#__codelineno-0-51">51</a></span>
<span class="normal"><a href="#__codelineno-0-52">52</a></span>
<span class="normal"><a href="#__codelineno-0-53">53</a></span>
<span class="normal"><a href="#__codelineno-0-54">54</a></span>
<span class="normal"><a href="#__codelineno-0-55">55</a></span>
<span class="normal"><a href="#__codelineno-0-56">56</a></span>
<span class="normal"><a href="#__codelineno-0-57">57</a></span>
<span class="normal"><a href="#__codelineno-0-58">58</a></span>
<span class="normal"><a href="#__codelineno-0-59">59</a></span>
<span class="normal"><a href="#__codelineno-0-60">60</a></span>
<span class="normal"><a href="#__codelineno-0-61">61</a></span>
<span class="normal"><a href="#__codelineno-0-62">62</a></span>
<span class="normal"><a href="#__codelineno-0-63">63</a></span>
<span class="normal"><a href="#__codelineno-0-64">64</a></span>
<span class="normal"><a href="#__codelineno-0-65">65</a></span>
<span class="normal"><a href="#__codelineno-0-66">66</a></span>
<span class="normal"><a href="#__codelineno-0-67">67</a></span>
<span class="normal"><a href="#__codelineno-0-68">68</a></span>
<span class="normal"><a href="#__codelineno-0-69">69</a></span>
<span class="normal"><a href="#__codelineno-0-70">70</a></span>
<span class="normal"><a href="#__codelineno-0-71">71</a></span>
<span class="normal"><a href="#__codelineno-0-72">72</a></span>
<span class="normal"><a href="#__codelineno-0-73">73</a></span>
<span class="normal"><a href="#__codelineno-0-74">74</a></span>
<span class="normal"><a href="#__codelineno-0-75">75</a></span>
<span class="normal"><a href="#__codelineno-0-76">76</a></span>
<span class="normal"><a href="#__codelineno-0-77">77</a></span>
<span class="normal"><a href="#__codelineno-0-78">78</a></span>
<span class="normal"><a href="#__codelineno-0-79">79</a></span>
<span class="normal"><a href="#__codelineno-0-80">80</a></span>
<span class="normal"><a href="#__codelineno-0-81">81</a></span>
<span class="normal"><a href="#__codelineno-0-82">82</a></span>
<span class="normal"><a href="#__codelineno-0-83">83</a></span>
<span class="normal"><a href="#__codelineno-0-84">84</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-49" name="__codelineno-0-49"></a><span class="k">def</span><span class="w"> </span><span class="nf">setup_number_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Setting up attributes needed by NT loss&quot;&quot;&quot;</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>    <span class="c1"># Add digits to vocab if not there yet.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span><span class="p">:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a>        <span class="n">new_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))))</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a>    <span class="k">if</span> <span class="n">vocab_size</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span> <span class="ow">and</span> <span class="n">new_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a>        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Added </span><span class="si">{</span><span class="n">new_tokens</span><span class="si">}</span><span class="s2"> new tokens for number token loss&quot;</span><span class="p">)</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a>    <span class="n">vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">:</span> <span class="n">FloatTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">))</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a>    <span class="c1"># Try to convert each token to a float after stripping the space prefix</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a>    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a>        <span class="k">if</span> <span class="n">is_number</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span><span class="p">:</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a>                <span class="c1"># NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a>                <span class="c1"># This stabilizes training with NTL. Can be altered though, see paper experiments.</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a>                <span class="c1"># Excludes tokens that are numbers in other languages like ႘ and tokens with space pre-/postfix like ` 2`.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>                <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isascii</span><span class="p">()</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;=</span> <span class="nb">float</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">9</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span> <span class="o">=</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">)</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">):</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>            <span class="s2">&quot;At least one token needs to be not a number, otherwise `ignore_index` cannot be set up safely&quot;</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>        <span class="p">)</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">nan_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">digit_level</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_nts</span> <span class="o">:=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">10</span><span class="p">:</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>            <span class="sa">f</span><span class="s2">&quot;You requested digit-level but </span><span class="si">{</span><span class="n">num_nts</span><span class="si">}</span><span class="s2"> number tokens were identified: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.AbstractNTLoss.__call__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#ntloss.core.AbstractNTLoss.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Alias to self.forward</p>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-95">95</a></span>
<span class="normal"><a href="#__codelineno-0-96">96</a></span>
<span class="normal"><a href="#__codelineno-0-97">97</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-95" name="__codelineno-0-95"></a><span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Alias to self.forward&quot;&quot;&quot;</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.AbstractNTLoss.reweigh_fn" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">reweigh_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span></code>

<a href="#ntloss.core.AbstractNTLoss.reweigh_fn" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Scale the NT loss element-wise using the logit weight on number tokens.
NOTE: This reweighing ensures that if ground truth is a number token
    but most probability mass is on text tokens, the loss will be <em>higher</em>
    than the worst possible number token. This is an edge case in practice.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>3D Tensor of shape BS x T x V.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>1D Tensor over all number tokens in batch.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>number_token_positions</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D Tensor of shape BS x T indicating for which tokens
the NT loss was computed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A 1D Tensor over all number tokens in batch with the scaled NT losses.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-99" name="__codelineno-0-99"></a><span class="k">def</span><span class="w"> </span><span class="nf">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>    <span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>    <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a><span class="sd">    Scale the NT loss element-wise using the logit weight on number tokens.</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a><span class="sd">    NOTE: This reweighing ensures that if ground truth is a number token</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a><span class="sd">        but most probability mass is on text tokens, the loss will be *higher*</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a><span class="sd">        than the worst possible number token. This is an edge case in practice.</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a><span class="sd">        logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a><span class="sd">        loss: 1D Tensor over all number tokens in batch.</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a><span class="sd">        number_token_positions: 2D Tensor of shape BS x T indicating for which tokens</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a><span class="sd">            the NT loss was computed.</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a><span class="sd">        A 1D Tensor over all number tokens in batch with the scaled NT losses.</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>    <span class="c1"># Take softmax over logits of all tokens in vocab and compute NT logit weight</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>    <span class="n">softmax_probs_all</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>    <span class="n">nt_logit_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>        <span class="n">softmax_probs_all</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>    <span class="p">)[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>    <span class="c1"># Apply weights for NTL element-wise</span>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>    <span class="n">loss</span> <span class="o">*=</span> <span class="n">nt_logit_weight</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>    <span class="c1"># Apply regularization</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>    <span class="c1"># NOTE: We could consider reweighing here with the max for that label token</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>    <span class="c1"># rather than the global max</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>    <span class="n">loss</span> <span class="o">+=</span> <span class="p">(</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>        <span class="mf">1.01</span>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>        <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">nt_logit_weight</span><span class="p">)</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a>    <span class="p">)</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="ntloss.core.NTLossDotProduct" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>NTLossDotProduct</code>


<a href="#ntloss.core.NTLossDotProduct" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;AbstractNTLoss&lt;/code&gt; (&lt;code&gt;ntloss.core.AbstractNTLoss&lt;/code&gt;)" href="../#ntloss.core.AbstractNTLoss">AbstractNTLoss</a></code></p>


        <p>Class for NT losses that produce a token-wise numerical output.</p>







              <details class="quote">
                <summary>Source code in <code>ntloss/core.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-266">266</a></span>
<span class="normal"><a href="#__codelineno-0-267">267</a></span>
<span class="normal"><a href="#__codelineno-0-268">268</a></span>
<span class="normal"><a href="#__codelineno-0-269">269</a></span>
<span class="normal"><a href="#__codelineno-0-270">270</a></span>
<span class="normal"><a href="#__codelineno-0-271">271</a></span>
<span class="normal"><a href="#__codelineno-0-272">272</a></span>
<span class="normal"><a href="#__codelineno-0-273">273</a></span>
<span class="normal"><a href="#__codelineno-0-274">274</a></span>
<span class="normal"><a href="#__codelineno-0-275">275</a></span>
<span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span>
<span class="normal"><a href="#__codelineno-0-329">329</a></span>
<span class="normal"><a href="#__codelineno-0-330">330</a></span>
<span class="normal"><a href="#__codelineno-0-331">331</a></span>
<span class="normal"><a href="#__codelineno-0-332">332</a></span>
<span class="normal"><a href="#__codelineno-0-333">333</a></span>
<span class="normal"><a href="#__codelineno-0-334">334</a></span>
<span class="normal"><a href="#__codelineno-0-335">335</a></span>
<span class="normal"><a href="#__codelineno-0-336">336</a></span>
<span class="normal"><a href="#__codelineno-0-337">337</a></span>
<span class="normal"><a href="#__codelineno-0-338">338</a></span>
<span class="normal"><a href="#__codelineno-0-339">339</a></span>
<span class="normal"><a href="#__codelineno-0-340">340</a></span>
<span class="normal"><a href="#__codelineno-0-341">341</a></span>
<span class="normal"><a href="#__codelineno-0-342">342</a></span>
<span class="normal"><a href="#__codelineno-0-343">343</a></span>
<span class="normal"><a href="#__codelineno-0-344">344</a></span>
<span class="normal"><a href="#__codelineno-0-345">345</a></span>
<span class="normal"><a href="#__codelineno-0-346">346</a></span>
<span class="normal"><a href="#__codelineno-0-347">347</a></span>
<span class="normal"><a href="#__codelineno-0-348">348</a></span>
<span class="normal"><a href="#__codelineno-0-349">349</a></span>
<span class="normal"><a href="#__codelineno-0-350">350</a></span>
<span class="normal"><a href="#__codelineno-0-351">351</a></span>
<span class="normal"><a href="#__codelineno-0-352">352</a></span>
<span class="normal"><a href="#__codelineno-0-353">353</a></span>
<span class="normal"><a href="#__codelineno-0-354">354</a></span>
<span class="normal"><a href="#__codelineno-0-355">355</a></span>
<span class="normal"><a href="#__codelineno-0-356">356</a></span>
<span class="normal"><a href="#__codelineno-0-357">357</a></span>
<span class="normal"><a href="#__codelineno-0-358">358</a></span>
<span class="normal"><a href="#__codelineno-0-359">359</a></span>
<span class="normal"><a href="#__codelineno-0-360">360</a></span>
<span class="normal"><a href="#__codelineno-0-361">361</a></span>
<span class="normal"><a href="#__codelineno-0-362">362</a></span>
<span class="normal"><a href="#__codelineno-0-363">363</a></span>
<span class="normal"><a href="#__codelineno-0-364">364</a></span>
<span class="normal"><a href="#__codelineno-0-365">365</a></span>
<span class="normal"><a href="#__codelineno-0-366">366</a></span>
<span class="normal"><a href="#__codelineno-0-367">367</a></span>
<span class="normal"><a href="#__codelineno-0-368">368</a></span>
<span class="normal"><a href="#__codelineno-0-369">369</a></span>
<span class="normal"><a href="#__codelineno-0-370">370</a></span>
<span class="normal"><a href="#__codelineno-0-371">371</a></span>
<span class="normal"><a href="#__codelineno-0-372">372</a></span>
<span class="normal"><a href="#__codelineno-0-373">373</a></span>
<span class="normal"><a href="#__codelineno-0-374">374</a></span>
<span class="normal"><a href="#__codelineno-0-375">375</a></span>
<span class="normal"><a href="#__codelineno-0-376">376</a></span>
<span class="normal"><a href="#__codelineno-0-377">377</a></span>
<span class="normal"><a href="#__codelineno-0-378">378</a></span>
<span class="normal"><a href="#__codelineno-0-379">379</a></span>
<span class="normal"><a href="#__codelineno-0-380">380</a></span>
<span class="normal"><a href="#__codelineno-0-381">381</a></span>
<span class="normal"><a href="#__codelineno-0-382">382</a></span>
<span class="normal"><a href="#__codelineno-0-383">383</a></span>
<span class="normal"><a href="#__codelineno-0-384">384</a></span>
<span class="normal"><a href="#__codelineno-0-385">385</a></span>
<span class="normal"><a href="#__codelineno-0-386">386</a></span>
<span class="normal"><a href="#__codelineno-0-387">387</a></span>
<span class="normal"><a href="#__codelineno-0-388">388</a></span>
<span class="normal"><a href="#__codelineno-0-389">389</a></span>
<span class="normal"><a href="#__codelineno-0-390">390</a></span>
<span class="normal"><a href="#__codelineno-0-391">391</a></span>
<span class="normal"><a href="#__codelineno-0-392">392</a></span>
<span class="normal"><a href="#__codelineno-0-393">393</a></span>
<span class="normal"><a href="#__codelineno-0-394">394</a></span>
<span class="normal"><a href="#__codelineno-0-395">395</a></span>
<span class="normal"><a href="#__codelineno-0-396">396</a></span>
<span class="normal"><a href="#__codelineno-0-397">397</a></span>
<span class="normal"><a href="#__codelineno-0-398">398</a></span>
<span class="normal"><a href="#__codelineno-0-399">399</a></span>
<span class="normal"><a href="#__codelineno-0-400">400</a></span>
<span class="normal"><a href="#__codelineno-0-401">401</a></span>
<span class="normal"><a href="#__codelineno-0-402">402</a></span>
<span class="normal"><a href="#__codelineno-0-403">403</a></span>
<span class="normal"><a href="#__codelineno-0-404">404</a></span>
<span class="normal"><a href="#__codelineno-0-405">405</a></span>
<span class="normal"><a href="#__codelineno-0-406">406</a></span>
<span class="normal"><a href="#__codelineno-0-407">407</a></span>
<span class="normal"><a href="#__codelineno-0-408">408</a></span>
<span class="normal"><a href="#__codelineno-0-409">409</a></span>
<span class="normal"><a href="#__codelineno-0-410">410</a></span>
<span class="normal"><a href="#__codelineno-0-411">411</a></span>
<span class="normal"><a href="#__codelineno-0-412">412</a></span>
<span class="normal"><a href="#__codelineno-0-413">413</a></span>
<span class="normal"><a href="#__codelineno-0-414">414</a></span>
<span class="normal"><a href="#__codelineno-0-415">415</a></span>
<span class="normal"><a href="#__codelineno-0-416">416</a></span>
<span class="normal"><a href="#__codelineno-0-417">417</a></span>
<span class="normal"><a href="#__codelineno-0-418">418</a></span>
<span class="normal"><a href="#__codelineno-0-419">419</a></span>
<span class="normal"><a href="#__codelineno-0-420">420</a></span>
<span class="normal"><a href="#__codelineno-0-421">421</a></span>
<span class="normal"><a href="#__codelineno-0-422">422</a></span>
<span class="normal"><a href="#__codelineno-0-423">423</a></span>
<span class="normal"><a href="#__codelineno-0-424">424</a></span>
<span class="normal"><a href="#__codelineno-0-425">425</a></span>
<span class="normal"><a href="#__codelineno-0-426">426</a></span>
<span class="normal"><a href="#__codelineno-0-427">427</a></span>
<span class="normal"><a href="#__codelineno-0-428">428</a></span>
<span class="normal"><a href="#__codelineno-0-429">429</a></span>
<span class="normal"><a href="#__codelineno-0-430">430</a></span>
<span class="normal"><a href="#__codelineno-0-431">431</a></span>
<span class="normal"><a href="#__codelineno-0-432">432</a></span>
<span class="normal"><a href="#__codelineno-0-433">433</a></span>
<span class="normal"><a href="#__codelineno-0-434">434</a></span>
<span class="normal"><a href="#__codelineno-0-435">435</a></span>
<span class="normal"><a href="#__codelineno-0-436">436</a></span>
<span class="normal"><a href="#__codelineno-0-437">437</a></span>
<span class="normal"><a href="#__codelineno-0-438">438</a></span>
<span class="normal"><a href="#__codelineno-0-439">439</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-266" name="__codelineno-0-266"></a><span class="k">class</span><span class="w"> </span><span class="nc">NTLossDotProduct</span><span class="p">(</span><span class="n">AbstractNTLoss</span><span class="p">):</span>
<a id="__codelineno-0-267" name="__codelineno-0-267"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for NT losses that produce a token-wise numerical output.&quot;&quot;&quot;</span>
<a id="__codelineno-0-268" name="__codelineno-0-268"></a>
<a id="__codelineno-0-269" name="__codelineno-0-269"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-270" name="__codelineno-0-270"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-271" name="__codelineno-0-271"></a>        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-272" name="__codelineno-0-272"></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-273" name="__codelineno-0-273"></a>        <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-274" name="__codelineno-0-274"></a>        <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-275" name="__codelineno-0-275"></a>        <span class="n">loss_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">,</span>
<a id="__codelineno-0-276" name="__codelineno-0-276"></a>    <span class="p">):</span>
<a id="__codelineno-0-277" name="__codelineno-0-277"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-278" name="__codelineno-0-278"></a><span class="sd">        Referred to as NTL-L_p in the paper.</span>
<a id="__codelineno-0-279" name="__codelineno-0-279"></a>
<a id="__codelineno-0-280" name="__codelineno-0-280"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-281" name="__codelineno-0-281"></a><span class="sd">            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.</span>
<a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="sd">            vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="sd">                tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">            digit_level: Whether to ensure only digits are considered number tokens,</span>
<a id="__codelineno-0-285" name="__codelineno-0-285"></a><span class="sd">                stabilizing training with NTL. Defaults to True. Used for most</span>
<a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">                experiments in the ICML paper.</span>
<a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">            reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-288" name="__codelineno-0-288"></a><span class="sd">                number tokens. Defaults to True.</span>
<a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">                NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-290" name="__codelineno-0-290"></a><span class="sd">                incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">            loss_function: Function to apply on the delta between the ground truth number</span>
<a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">                and the obtained dot product (nt-probs * token-values). Defaults to</span>
<a id="__codelineno-0-293" name="__codelineno-0-293"></a><span class="sd">                MSE, but MAE, Huber etc are also compatible.</span>
<a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-295" name="__codelineno-0-295"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-296" name="__codelineno-0-296"></a>            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<a id="__codelineno-0-297" name="__codelineno-0-297"></a>            <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-0-298" name="__codelineno-0-298"></a>            <span class="n">digit_level</span><span class="o">=</span><span class="n">digit_level</span><span class="p">,</span>
<a id="__codelineno-0-299" name="__codelineno-0-299"></a>            <span class="n">reweigh</span><span class="o">=</span><span class="n">reweigh</span><span class="p">,</span>
<a id="__codelineno-0-300" name="__codelineno-0-300"></a>        <span class="p">)</span>
<a id="__codelineno-0-301" name="__codelineno-0-301"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">loss_function</span>
<a id="__codelineno-0-302" name="__codelineno-0-302"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">setup_max_dist</span><span class="p">()</span>
<a id="__codelineno-0-303" name="__codelineno-0-303"></a>
<a id="__codelineno-0-304" name="__codelineno-0-304"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_max_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        Set up the maximum distance between the number tokens based on the selected loss function.</span>
<a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-308" name="__codelineno-0-308"></a>
<a id="__codelineno-0-309" name="__codelineno-0-309"></a>        <span class="c1"># Extract the number token values and get the minimum and maximum</span>
<a id="__codelineno-0-310" name="__codelineno-0-310"></a>        <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-311" name="__codelineno-0-311"></a>        <span class="n">max_val</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a id="__codelineno-0-312" name="__codelineno-0-312"></a>        <span class="n">min_val</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<a id="__codelineno-0-313" name="__codelineno-0-313"></a>
<a id="__codelineno-0-314" name="__codelineno-0-314"></a>        <span class="c1"># Compute the largest value the loss function used in NT loss computation can get</span>
<a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="c1"># Make sure to account for possibility of asymmetrical loss function</span>
<a id="__codelineno-0-316" name="__codelineno-0-316"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)),</span>
<a id="__codelineno-0-318" name="__codelineno-0-318"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)),</span>
<a id="__codelineno-0-319" name="__codelineno-0-319"></a>        <span class="p">)</span>
<a id="__codelineno-0-320" name="__codelineno-0-320"></a>
<a id="__codelineno-0-321" name="__codelineno-0-321"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict_numbers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">FloatTensor</span><span class="p">]:</span>
<a id="__codelineno-0-322" name="__codelineno-0-322"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-323" name="__codelineno-0-323"></a><span class="sd">        Calculates token-level numerical prediction.</span>
<a id="__codelineno-0-324" name="__codelineno-0-324"></a><span class="sd">        NOTE: This calculates numerical predictions for *all* tokens, not just where</span>
<a id="__codelineno-0-325" name="__codelineno-0-325"></a><span class="sd">        label is a number token.</span>
<a id="__codelineno-0-326" name="__codelineno-0-326"></a>
<a id="__codelineno-0-327" name="__codelineno-0-327"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-328" name="__codelineno-0-328"></a><span class="sd">            logits: 3D FloatTensor of shape BS x T x V.</span>
<a id="__codelineno-0-329" name="__codelineno-0-329"></a>
<a id="__codelineno-0-330" name="__codelineno-0-330"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-331" name="__codelineno-0-331"></a><span class="sd">            yhat: 2D FloatTensor BS x T containing numerical predictions.</span>
<a id="__codelineno-0-332" name="__codelineno-0-332"></a><span class="sd">            nt_mass: 2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.</span>
<a id="__codelineno-0-333" name="__codelineno-0-333"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-334" name="__codelineno-0-334"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<a id="__codelineno-0-335" name="__codelineno-0-335"></a>
<a id="__codelineno-0-336" name="__codelineno-0-336"></a>        <span class="c1"># Calculate the token-level predictions</span>
<a id="__codelineno-0-337" name="__codelineno-0-337"></a>        <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dot_product</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<a id="__codelineno-0-338" name="__codelineno-0-338"></a>
<a id="__codelineno-0-339" name="__codelineno-0-339"></a>        <span class="n">probs_all</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-340" name="__codelineno-0-340"></a>        <span class="n">probs_nt</span> <span class="o">=</span> <span class="n">probs_all</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-341" name="__codelineno-0-341"></a>        <span class="n">nt_mass</span> <span class="o">=</span> <span class="n">probs_nt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-342" name="__codelineno-0-342"></a>        <span class="k">return</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">nt_mass</span><span class="p">)</span>
<a id="__codelineno-0-343" name="__codelineno-0-343"></a>
<a id="__codelineno-0-344" name="__codelineno-0-344"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_get_dot_product</span><span class="p">(</span>
<a id="__codelineno-0-345" name="__codelineno-0-345"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-346" name="__codelineno-0-346"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FloatTensor</span><span class="p">:</span>
<a id="__codelineno-0-347" name="__codelineno-0-347"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-348" name="__codelineno-0-348"></a><span class="sd">        Applies dot product of number token values and their predicted probabilites.</span>
<a id="__codelineno-0-349" name="__codelineno-0-349"></a>
<a id="__codelineno-0-350" name="__codelineno-0-350"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-351" name="__codelineno-0-351"></a><span class="sd">            logits: 3D FloatTensor of shape BS x T x V.</span>
<a id="__codelineno-0-352" name="__codelineno-0-352"></a><span class="sd">            number_token_positions: Optional 2D BoolTensor (BS x T) containing locations</span>
<a id="__codelineno-0-353" name="__codelineno-0-353"></a><span class="sd">                of number tokens.</span>
<a id="__codelineno-0-354" name="__codelineno-0-354"></a>
<a id="__codelineno-0-355" name="__codelineno-0-355"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-356" name="__codelineno-0-356"></a><span class="sd">            If `number_token_positions` is None, 2D FloatTensor of shape BS x T.</span>
<a id="__codelineno-0-357" name="__codelineno-0-357"></a><span class="sd">            Otherwise, 1D FloatTensor containing the predictions for the number tokens.</span>
<a id="__codelineno-0-358" name="__codelineno-0-358"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-359" name="__codelineno-0-359"></a>        <span class="c1"># apply softmax solely over the number token indices</span>
<a id="__codelineno-0-360" name="__codelineno-0-360"></a>        <span class="n">nt_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-361" name="__codelineno-0-361"></a>        <span class="n">softmax_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nt_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-362" name="__codelineno-0-362"></a>        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-363" name="__codelineno-0-363"></a>
<a id="__codelineno-0-364" name="__codelineno-0-364"></a>        <span class="c1"># compute the weighted average of number tokens</span>
<a id="__codelineno-0-365" name="__codelineno-0-365"></a>        <span class="k">if</span> <span class="n">number_token_positions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-366" name="__codelineno-0-366"></a>            <span class="c1"># Calculate for all tokens</span>
<a id="__codelineno-0-367" name="__codelineno-0-367"></a>            <span class="n">yhat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">softmax_probs</span> <span class="o">*</span> <span class="n">values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-368" name="__codelineno-0-368"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-369" name="__codelineno-0-369"></a>            <span class="c1"># Calculate selectively where labels are number tokens</span>
<a id="__codelineno-0-370" name="__codelineno-0-370"></a>            <span class="n">yhat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">softmax_probs</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span> <span class="o">*</span> <span class="n">values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-371" name="__codelineno-0-371"></a>        <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<a id="__codelineno-0-372" name="__codelineno-0-372"></a>
<a id="__codelineno-0-373" name="__codelineno-0-373"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-374" name="__codelineno-0-374"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-375" name="__codelineno-0-375"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-376" name="__codelineno-0-376"></a>        <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-377" name="__codelineno-0-377"></a>        <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-378" name="__codelineno-0-378"></a>        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-379" name="__codelineno-0-379"></a>        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-380" name="__codelineno-0-380"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-381" name="__codelineno-0-381"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-382" name="__codelineno-0-382"></a><span class="sd">        Computes the NTL based on the dot product between token values and their probs.</span>
<a id="__codelineno-0-383" name="__codelineno-0-383"></a>
<a id="__codelineno-0-384" name="__codelineno-0-384"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-385" name="__codelineno-0-385"></a><span class="sd">            logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-386" name="__codelineno-0-386"></a><span class="sd">            labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-387" name="__codelineno-0-387"></a><span class="sd">            loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.</span>
<a id="__codelineno-0-388" name="__codelineno-0-388"></a><span class="sd">            reduction: Optional string specifying the reduction to apply to the</span>
<a id="__codelineno-0-389" name="__codelineno-0-389"></a><span class="sd">                output. Defaults to &quot;mean&quot;, options are &quot;mean&quot;, &quot;sum&quot;, &quot;none&quot;.</span>
<a id="__codelineno-0-390" name="__codelineno-0-390"></a><span class="sd">            ignore_index: The token ID to ignore in the labels. Defaults to -100.</span>
<a id="__codelineno-0-391" name="__codelineno-0-391"></a>
<a id="__codelineno-0-392" name="__codelineno-0-392"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-393" name="__codelineno-0-393"></a><span class="sd">            Loss tensor</span>
<a id="__codelineno-0-394" name="__codelineno-0-394"></a><span class="sd">                OD if reduction==&quot;mean&quot;|&quot;sum&quot;</span>
<a id="__codelineno-0-395" name="__codelineno-0-395"></a><span class="sd">                BS x T if reduction==&quot;none&quot;</span>
<a id="__codelineno-0-396" name="__codelineno-0-396"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-397" name="__codelineno-0-397"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
<a id="__codelineno-0-398" name="__codelineno-0-398"></a>
<a id="__codelineno-0-399" name="__codelineno-0-399"></a>        <span class="n">y</span><span class="p">,</span> <span class="n">loss_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_number_token_targets</span><span class="p">(</span>
<a id="__codelineno-0-400" name="__codelineno-0-400"></a>            <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">ignore_index</span>
<a id="__codelineno-0-401" name="__codelineno-0-401"></a>        <span class="p">)</span>
<a id="__codelineno-0-402" name="__codelineno-0-402"></a>        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-403" name="__codelineno-0-403"></a>        <span class="n">number_token_positions</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<a id="__codelineno-0-404" name="__codelineno-0-404"></a>
<a id="__codelineno-0-405" name="__codelineno-0-405"></a>        <span class="c1"># If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations</span>
<a id="__codelineno-0-406" name="__codelineno-0-406"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">number_token_positions</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
<a id="__codelineno-0-407" name="__codelineno-0-407"></a>            <span class="k">if</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">):</span>
<a id="__codelineno-0-408" name="__codelineno-0-408"></a>                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-409" name="__codelineno-0-409"></a>            <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-410" name="__codelineno-0-410"></a>                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
<a id="__codelineno-0-411" name="__codelineno-0-411"></a>                    <span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span>
<a id="__codelineno-0-412" name="__codelineno-0-412"></a>                <span class="p">)</span>
<a id="__codelineno-0-413" name="__codelineno-0-413"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-414" name="__codelineno-0-414"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-415" name="__codelineno-0-415"></a>
<a id="__codelineno-0-416" name="__codelineno-0-416"></a>            <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-417" name="__codelineno-0-417"></a>
<a id="__codelineno-0-418" name="__codelineno-0-418"></a>        <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dot_product</span><span class="p">(</span>
<a id="__codelineno-0-419" name="__codelineno-0-419"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-420" name="__codelineno-0-420"></a>        <span class="p">)</span>
<a id="__codelineno-0-421" name="__codelineno-0-421"></a>
<a id="__codelineno-0-422" name="__codelineno-0-422"></a>        <span class="c1"># Apply specified loss function to y and yhat</span>
<a id="__codelineno-0-423" name="__codelineno-0-423"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">],</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
<a id="__codelineno-0-424" name="__codelineno-0-424"></a>
<a id="__codelineno-0-425" name="__codelineno-0-425"></a>        <span class="c1"># If reweigh: compute weights for NTL based on logits</span>
<a id="__codelineno-0-426" name="__codelineno-0-426"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span><span class="p">:</span>
<a id="__codelineno-0-427" name="__codelineno-0-427"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-428" name="__codelineno-0-428"></a>                <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-429" name="__codelineno-0-429"></a>            <span class="p">)</span>
<a id="__codelineno-0-430" name="__codelineno-0-430"></a>
<a id="__codelineno-0-431" name="__codelineno-0-431"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-432" name="__codelineno-0-432"></a>            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-433" name="__codelineno-0-433"></a>            <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
<a id="__codelineno-0-434" name="__codelineno-0-434"></a>            <span class="n">loss_weights</span><span class="o">=</span><span class="n">loss_weights</span><span class="p">,</span>
<a id="__codelineno-0-435" name="__codelineno-0-435"></a>            <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span><span class="p">,</span>
<a id="__codelineno-0-436" name="__codelineno-0-436"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
<a id="__codelineno-0-437" name="__codelineno-0-437"></a>        <span class="p">)</span>
<a id="__codelineno-0-438" name="__codelineno-0-438"></a>
<a id="__codelineno-0-439" name="__codelineno-0-439"></a>        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLossDotProduct.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">)</span></code>

<a href="#ntloss.core.NTLossDotProduct.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Referred to as NTL-L_p in the paper.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td>
                  <code><span title="transformers.PreTrainedTokenizer">PreTrainedTokenizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>NTLTokenizer with necessary attributes like is_number_token etc.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional user-provided vocab size. If not provided, the
tokenizer's vocab size is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>digit_level</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to ensure only digits are considered number tokens,
stabilizing training with NTL. Defaults to True. Used for most
experiments in the ICML paper.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reweigh</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to scale the NTL using the logit weight on
number tokens. Defaults to True.
NOTE: The ICML paper does <em>not</em> use this option which can lead to
incorrect loss if most mass is placed outside of the number tokens.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss_function</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Function to apply on the delta between the ground truth number
and the obtained dot product (nt-probs * token-values). Defaults to
MSE, but MAE, Huber etc are also compatible.</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.nn.functional.mse_loss">mse_loss</span></code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-269">269</a></span>
<span class="normal"><a href="#__codelineno-0-270">270</a></span>
<span class="normal"><a href="#__codelineno-0-271">271</a></span>
<span class="normal"><a href="#__codelineno-0-272">272</a></span>
<span class="normal"><a href="#__codelineno-0-273">273</a></span>
<span class="normal"><a href="#__codelineno-0-274">274</a></span>
<span class="normal"><a href="#__codelineno-0-275">275</a></span>
<span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-269" name="__codelineno-0-269"></a><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-270" name="__codelineno-0-270"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-271" name="__codelineno-0-271"></a>    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-272" name="__codelineno-0-272"></a>    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-273" name="__codelineno-0-273"></a>    <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-274" name="__codelineno-0-274"></a>    <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-275" name="__codelineno-0-275"></a>    <span class="n">loss_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">,</span>
<a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="p">):</span>
<a id="__codelineno-0-277" name="__codelineno-0-277"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-278" name="__codelineno-0-278"></a><span class="sd">    Referred to as NTL-L_p in the paper.</span>
<a id="__codelineno-0-279" name="__codelineno-0-279"></a>
<a id="__codelineno-0-280" name="__codelineno-0-280"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-281" name="__codelineno-0-281"></a><span class="sd">        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.</span>
<a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="sd">        vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="sd">            tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">        digit_level: Whether to ensure only digits are considered number tokens,</span>
<a id="__codelineno-0-285" name="__codelineno-0-285"></a><span class="sd">            stabilizing training with NTL. Defaults to True. Used for most</span>
<a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">            experiments in the ICML paper.</span>
<a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-288" name="__codelineno-0-288"></a><span class="sd">            number tokens. Defaults to True.</span>
<a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">            NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-290" name="__codelineno-0-290"></a><span class="sd">            incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">        loss_function: Function to apply on the delta between the ground truth number</span>
<a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">            and the obtained dot product (nt-probs * token-values). Defaults to</span>
<a id="__codelineno-0-293" name="__codelineno-0-293"></a><span class="sd">            MSE, but MAE, Huber etc are also compatible.</span>
<a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-295" name="__codelineno-0-295"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-296" name="__codelineno-0-296"></a>        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<a id="__codelineno-0-297" name="__codelineno-0-297"></a>        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-0-298" name="__codelineno-0-298"></a>        <span class="n">digit_level</span><span class="o">=</span><span class="n">digit_level</span><span class="p">,</span>
<a id="__codelineno-0-299" name="__codelineno-0-299"></a>        <span class="n">reweigh</span><span class="o">=</span><span class="n">reweigh</span><span class="p">,</span>
<a id="__codelineno-0-300" name="__codelineno-0-300"></a>    <span class="p">)</span>
<a id="__codelineno-0-301" name="__codelineno-0-301"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">loss_function</span>
<a id="__codelineno-0-302" name="__codelineno-0-302"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">setup_max_dist</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLossDotProduct.setup_max_dist" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">setup_max_dist</span><span class="p">()</span></code>

<a href="#ntloss.core.NTLossDotProduct.setup_max_dist" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set up the maximum distance between the number tokens based on the selected loss function.</p>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="k">def</span><span class="w"> </span><span class="nf">setup_max_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">    Set up the maximum distance between the number tokens based on the selected loss function.</span>
<a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-308" name="__codelineno-0-308"></a>
<a id="__codelineno-0-309" name="__codelineno-0-309"></a>    <span class="c1"># Extract the number token values and get the minimum and maximum</span>
<a id="__codelineno-0-310" name="__codelineno-0-310"></a>    <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-311" name="__codelineno-0-311"></a>    <span class="n">max_val</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a id="__codelineno-0-312" name="__codelineno-0-312"></a>    <span class="n">min_val</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<a id="__codelineno-0-313" name="__codelineno-0-313"></a>
<a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="c1"># Compute the largest value the loss function used in NT loss computation can get</span>
<a id="__codelineno-0-315" name="__codelineno-0-315"></a>    <span class="c1"># Make sure to account for possibility of asymmetrical loss function</span>
<a id="__codelineno-0-316" name="__codelineno-0-316"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-0-317" name="__codelineno-0-317"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)),</span>
<a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)),</span>
<a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLossDotProduct.predict_numbers" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">predict_numbers</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">FloatTensor</span><span class="p">]</span></code>

<a href="#ntloss.core.NTLossDotProduct.predict_numbers" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Calculates token-level numerical prediction.
NOTE: This calculates numerical predictions for <em>all</em> tokens, not just where
label is a number token.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>3D FloatTensor of shape BS x T x V.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>yhat</code></td>            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D FloatTensor BS x T containing numerical predictions.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>nt_mass</code></td>            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span>
<span class="normal"><a href="#__codelineno-0-329">329</a></span>
<span class="normal"><a href="#__codelineno-0-330">330</a></span>
<span class="normal"><a href="#__codelineno-0-331">331</a></span>
<span class="normal"><a href="#__codelineno-0-332">332</a></span>
<span class="normal"><a href="#__codelineno-0-333">333</a></span>
<span class="normal"><a href="#__codelineno-0-334">334</a></span>
<span class="normal"><a href="#__codelineno-0-335">335</a></span>
<span class="normal"><a href="#__codelineno-0-336">336</a></span>
<span class="normal"><a href="#__codelineno-0-337">337</a></span>
<span class="normal"><a href="#__codelineno-0-338">338</a></span>
<span class="normal"><a href="#__codelineno-0-339">339</a></span>
<span class="normal"><a href="#__codelineno-0-340">340</a></span>
<span class="normal"><a href="#__codelineno-0-341">341</a></span>
<span class="normal"><a href="#__codelineno-0-342">342</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-321" name="__codelineno-0-321"></a><span class="k">def</span><span class="w"> </span><span class="nf">predict_numbers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">FloatTensor</span><span class="p">]:</span>
<a id="__codelineno-0-322" name="__codelineno-0-322"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-323" name="__codelineno-0-323"></a><span class="sd">    Calculates token-level numerical prediction.</span>
<a id="__codelineno-0-324" name="__codelineno-0-324"></a><span class="sd">    NOTE: This calculates numerical predictions for *all* tokens, not just where</span>
<a id="__codelineno-0-325" name="__codelineno-0-325"></a><span class="sd">    label is a number token.</span>
<a id="__codelineno-0-326" name="__codelineno-0-326"></a>
<a id="__codelineno-0-327" name="__codelineno-0-327"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-328" name="__codelineno-0-328"></a><span class="sd">        logits: 3D FloatTensor of shape BS x T x V.</span>
<a id="__codelineno-0-329" name="__codelineno-0-329"></a>
<a id="__codelineno-0-330" name="__codelineno-0-330"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-331" name="__codelineno-0-331"></a><span class="sd">        yhat: 2D FloatTensor BS x T containing numerical predictions.</span>
<a id="__codelineno-0-332" name="__codelineno-0-332"></a><span class="sd">        nt_mass: 2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.</span>
<a id="__codelineno-0-333" name="__codelineno-0-333"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-334" name="__codelineno-0-334"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<a id="__codelineno-0-335" name="__codelineno-0-335"></a>
<a id="__codelineno-0-336" name="__codelineno-0-336"></a>    <span class="c1"># Calculate the token-level predictions</span>
<a id="__codelineno-0-337" name="__codelineno-0-337"></a>    <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dot_product</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<a id="__codelineno-0-338" name="__codelineno-0-338"></a>
<a id="__codelineno-0-339" name="__codelineno-0-339"></a>    <span class="n">probs_all</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-340" name="__codelineno-0-340"></a>    <span class="n">probs_nt</span> <span class="o">=</span> <span class="n">probs_all</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-341" name="__codelineno-0-341"></a>    <span class="n">nt_mass</span> <span class="o">=</span> <span class="n">probs_nt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-342" name="__codelineno-0-342"></a>    <span class="k">return</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">nt_mass</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLossDotProduct.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span></code>

<a href="#ntloss.core.NTLossDotProduct.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Computes the NTL based on the dot product between token values and their probs.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>3D Tensor of shape BS x T x V.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td>
                  <code><span title="torch.LongTensor">LongTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D Tensor of shape BS x T.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss_weights</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D Optional tensor of BS x T with token-wise loss weights.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reduction</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional string specifying the reduction to apply to the
output. Defaults to "mean", options are "mean", "sum", "none".</p>
              </div>
            </td>
            <td>
                  <code>&#39;mean&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_index</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The token ID to ignore in the labels. Defaults to -100.</p>
              </div>
            </td>
            <td>
                  <code>-100</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Loss tensor
OD if reduction=="mean"|"sum"
BS x T if reduction=="none"</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-373">373</a></span>
<span class="normal"><a href="#__codelineno-0-374">374</a></span>
<span class="normal"><a href="#__codelineno-0-375">375</a></span>
<span class="normal"><a href="#__codelineno-0-376">376</a></span>
<span class="normal"><a href="#__codelineno-0-377">377</a></span>
<span class="normal"><a href="#__codelineno-0-378">378</a></span>
<span class="normal"><a href="#__codelineno-0-379">379</a></span>
<span class="normal"><a href="#__codelineno-0-380">380</a></span>
<span class="normal"><a href="#__codelineno-0-381">381</a></span>
<span class="normal"><a href="#__codelineno-0-382">382</a></span>
<span class="normal"><a href="#__codelineno-0-383">383</a></span>
<span class="normal"><a href="#__codelineno-0-384">384</a></span>
<span class="normal"><a href="#__codelineno-0-385">385</a></span>
<span class="normal"><a href="#__codelineno-0-386">386</a></span>
<span class="normal"><a href="#__codelineno-0-387">387</a></span>
<span class="normal"><a href="#__codelineno-0-388">388</a></span>
<span class="normal"><a href="#__codelineno-0-389">389</a></span>
<span class="normal"><a href="#__codelineno-0-390">390</a></span>
<span class="normal"><a href="#__codelineno-0-391">391</a></span>
<span class="normal"><a href="#__codelineno-0-392">392</a></span>
<span class="normal"><a href="#__codelineno-0-393">393</a></span>
<span class="normal"><a href="#__codelineno-0-394">394</a></span>
<span class="normal"><a href="#__codelineno-0-395">395</a></span>
<span class="normal"><a href="#__codelineno-0-396">396</a></span>
<span class="normal"><a href="#__codelineno-0-397">397</a></span>
<span class="normal"><a href="#__codelineno-0-398">398</a></span>
<span class="normal"><a href="#__codelineno-0-399">399</a></span>
<span class="normal"><a href="#__codelineno-0-400">400</a></span>
<span class="normal"><a href="#__codelineno-0-401">401</a></span>
<span class="normal"><a href="#__codelineno-0-402">402</a></span>
<span class="normal"><a href="#__codelineno-0-403">403</a></span>
<span class="normal"><a href="#__codelineno-0-404">404</a></span>
<span class="normal"><a href="#__codelineno-0-405">405</a></span>
<span class="normal"><a href="#__codelineno-0-406">406</a></span>
<span class="normal"><a href="#__codelineno-0-407">407</a></span>
<span class="normal"><a href="#__codelineno-0-408">408</a></span>
<span class="normal"><a href="#__codelineno-0-409">409</a></span>
<span class="normal"><a href="#__codelineno-0-410">410</a></span>
<span class="normal"><a href="#__codelineno-0-411">411</a></span>
<span class="normal"><a href="#__codelineno-0-412">412</a></span>
<span class="normal"><a href="#__codelineno-0-413">413</a></span>
<span class="normal"><a href="#__codelineno-0-414">414</a></span>
<span class="normal"><a href="#__codelineno-0-415">415</a></span>
<span class="normal"><a href="#__codelineno-0-416">416</a></span>
<span class="normal"><a href="#__codelineno-0-417">417</a></span>
<span class="normal"><a href="#__codelineno-0-418">418</a></span>
<span class="normal"><a href="#__codelineno-0-419">419</a></span>
<span class="normal"><a href="#__codelineno-0-420">420</a></span>
<span class="normal"><a href="#__codelineno-0-421">421</a></span>
<span class="normal"><a href="#__codelineno-0-422">422</a></span>
<span class="normal"><a href="#__codelineno-0-423">423</a></span>
<span class="normal"><a href="#__codelineno-0-424">424</a></span>
<span class="normal"><a href="#__codelineno-0-425">425</a></span>
<span class="normal"><a href="#__codelineno-0-426">426</a></span>
<span class="normal"><a href="#__codelineno-0-427">427</a></span>
<span class="normal"><a href="#__codelineno-0-428">428</a></span>
<span class="normal"><a href="#__codelineno-0-429">429</a></span>
<span class="normal"><a href="#__codelineno-0-430">430</a></span>
<span class="normal"><a href="#__codelineno-0-431">431</a></span>
<span class="normal"><a href="#__codelineno-0-432">432</a></span>
<span class="normal"><a href="#__codelineno-0-433">433</a></span>
<span class="normal"><a href="#__codelineno-0-434">434</a></span>
<span class="normal"><a href="#__codelineno-0-435">435</a></span>
<span class="normal"><a href="#__codelineno-0-436">436</a></span>
<span class="normal"><a href="#__codelineno-0-437">437</a></span>
<span class="normal"><a href="#__codelineno-0-438">438</a></span>
<span class="normal"><a href="#__codelineno-0-439">439</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-373" name="__codelineno-0-373"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-374" name="__codelineno-0-374"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-375" name="__codelineno-0-375"></a>    <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-376" name="__codelineno-0-376"></a>    <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-377" name="__codelineno-0-377"></a>    <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-378" name="__codelineno-0-378"></a>    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-379" name="__codelineno-0-379"></a>    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-380" name="__codelineno-0-380"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-381" name="__codelineno-0-381"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-382" name="__codelineno-0-382"></a><span class="sd">    Computes the NTL based on the dot product between token values and their probs.</span>
<a id="__codelineno-0-383" name="__codelineno-0-383"></a>
<a id="__codelineno-0-384" name="__codelineno-0-384"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-385" name="__codelineno-0-385"></a><span class="sd">        logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-386" name="__codelineno-0-386"></a><span class="sd">        labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-387" name="__codelineno-0-387"></a><span class="sd">        loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.</span>
<a id="__codelineno-0-388" name="__codelineno-0-388"></a><span class="sd">        reduction: Optional string specifying the reduction to apply to the</span>
<a id="__codelineno-0-389" name="__codelineno-0-389"></a><span class="sd">            output. Defaults to &quot;mean&quot;, options are &quot;mean&quot;, &quot;sum&quot;, &quot;none&quot;.</span>
<a id="__codelineno-0-390" name="__codelineno-0-390"></a><span class="sd">        ignore_index: The token ID to ignore in the labels. Defaults to -100.</span>
<a id="__codelineno-0-391" name="__codelineno-0-391"></a>
<a id="__codelineno-0-392" name="__codelineno-0-392"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-393" name="__codelineno-0-393"></a><span class="sd">        Loss tensor</span>
<a id="__codelineno-0-394" name="__codelineno-0-394"></a><span class="sd">            OD if reduction==&quot;mean&quot;|&quot;sum&quot;</span>
<a id="__codelineno-0-395" name="__codelineno-0-395"></a><span class="sd">            BS x T if reduction==&quot;none&quot;</span>
<a id="__codelineno-0-396" name="__codelineno-0-396"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-397" name="__codelineno-0-397"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
<a id="__codelineno-0-398" name="__codelineno-0-398"></a>
<a id="__codelineno-0-399" name="__codelineno-0-399"></a>    <span class="n">y</span><span class="p">,</span> <span class="n">loss_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_number_token_targets</span><span class="p">(</span>
<a id="__codelineno-0-400" name="__codelineno-0-400"></a>        <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">ignore_index</span>
<a id="__codelineno-0-401" name="__codelineno-0-401"></a>    <span class="p">)</span>
<a id="__codelineno-0-402" name="__codelineno-0-402"></a>    <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-403" name="__codelineno-0-403"></a>    <span class="n">number_token_positions</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<a id="__codelineno-0-404" name="__codelineno-0-404"></a>
<a id="__codelineno-0-405" name="__codelineno-0-405"></a>    <span class="c1"># If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations</span>
<a id="__codelineno-0-406" name="__codelineno-0-406"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">number_token_positions</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
<a id="__codelineno-0-407" name="__codelineno-0-407"></a>        <span class="k">if</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">):</span>
<a id="__codelineno-0-408" name="__codelineno-0-408"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-409" name="__codelineno-0-409"></a>        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-410" name="__codelineno-0-410"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
<a id="__codelineno-0-411" name="__codelineno-0-411"></a>                <span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span>
<a id="__codelineno-0-412" name="__codelineno-0-412"></a>            <span class="p">)</span>
<a id="__codelineno-0-413" name="__codelineno-0-413"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-414" name="__codelineno-0-414"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-415" name="__codelineno-0-415"></a>
<a id="__codelineno-0-416" name="__codelineno-0-416"></a>        <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-417" name="__codelineno-0-417"></a>
<a id="__codelineno-0-418" name="__codelineno-0-418"></a>    <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dot_product</span><span class="p">(</span>
<a id="__codelineno-0-419" name="__codelineno-0-419"></a>        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-420" name="__codelineno-0-420"></a>    <span class="p">)</span>
<a id="__codelineno-0-421" name="__codelineno-0-421"></a>
<a id="__codelineno-0-422" name="__codelineno-0-422"></a>    <span class="c1"># Apply specified loss function to y and yhat</span>
<a id="__codelineno-0-423" name="__codelineno-0-423"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">],</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
<a id="__codelineno-0-424" name="__codelineno-0-424"></a>
<a id="__codelineno-0-425" name="__codelineno-0-425"></a>    <span class="c1"># If reweigh: compute weights for NTL based on logits</span>
<a id="__codelineno-0-426" name="__codelineno-0-426"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span><span class="p">:</span>
<a id="__codelineno-0-427" name="__codelineno-0-427"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-428" name="__codelineno-0-428"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-429" name="__codelineno-0-429"></a>        <span class="p">)</span>
<a id="__codelineno-0-430" name="__codelineno-0-430"></a>
<a id="__codelineno-0-431" name="__codelineno-0-431"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-432" name="__codelineno-0-432"></a>        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-433" name="__codelineno-0-433"></a>        <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
<a id="__codelineno-0-434" name="__codelineno-0-434"></a>        <span class="n">loss_weights</span><span class="o">=</span><span class="n">loss_weights</span><span class="p">,</span>
<a id="__codelineno-0-435" name="__codelineno-0-435"></a>        <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span><span class="p">,</span>
<a id="__codelineno-0-436" name="__codelineno-0-436"></a>        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
<a id="__codelineno-0-437" name="__codelineno-0-437"></a>    <span class="p">)</span>
<a id="__codelineno-0-438" name="__codelineno-0-438"></a>
<a id="__codelineno-0-439" name="__codelineno-0-439"></a>    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="ntloss.core.NTLoss" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>NTLoss</code>


<a href="#ntloss.core.NTLoss" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;AbstractNTLoss&lt;/code&gt; (&lt;code&gt;ntloss.core.AbstractNTLoss&lt;/code&gt;)" href="../#ntloss.core.AbstractNTLoss">AbstractNTLoss</a></code></p>


        <p>Class for Wasserstein-based NTLoss. This is the default in the ICML paper.</p>







              <details class="quote">
                <summary>Source code in <code>ntloss/core.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-442">442</a></span>
<span class="normal"><a href="#__codelineno-0-443">443</a></span>
<span class="normal"><a href="#__codelineno-0-444">444</a></span>
<span class="normal"><a href="#__codelineno-0-445">445</a></span>
<span class="normal"><a href="#__codelineno-0-446">446</a></span>
<span class="normal"><a href="#__codelineno-0-447">447</a></span>
<span class="normal"><a href="#__codelineno-0-448">448</a></span>
<span class="normal"><a href="#__codelineno-0-449">449</a></span>
<span class="normal"><a href="#__codelineno-0-450">450</a></span>
<span class="normal"><a href="#__codelineno-0-451">451</a></span>
<span class="normal"><a href="#__codelineno-0-452">452</a></span>
<span class="normal"><a href="#__codelineno-0-453">453</a></span>
<span class="normal"><a href="#__codelineno-0-454">454</a></span>
<span class="normal"><a href="#__codelineno-0-455">455</a></span>
<span class="normal"><a href="#__codelineno-0-456">456</a></span>
<span class="normal"><a href="#__codelineno-0-457">457</a></span>
<span class="normal"><a href="#__codelineno-0-458">458</a></span>
<span class="normal"><a href="#__codelineno-0-459">459</a></span>
<span class="normal"><a href="#__codelineno-0-460">460</a></span>
<span class="normal"><a href="#__codelineno-0-461">461</a></span>
<span class="normal"><a href="#__codelineno-0-462">462</a></span>
<span class="normal"><a href="#__codelineno-0-463">463</a></span>
<span class="normal"><a href="#__codelineno-0-464">464</a></span>
<span class="normal"><a href="#__codelineno-0-465">465</a></span>
<span class="normal"><a href="#__codelineno-0-466">466</a></span>
<span class="normal"><a href="#__codelineno-0-467">467</a></span>
<span class="normal"><a href="#__codelineno-0-468">468</a></span>
<span class="normal"><a href="#__codelineno-0-469">469</a></span>
<span class="normal"><a href="#__codelineno-0-470">470</a></span>
<span class="normal"><a href="#__codelineno-0-471">471</a></span>
<span class="normal"><a href="#__codelineno-0-472">472</a></span>
<span class="normal"><a href="#__codelineno-0-473">473</a></span>
<span class="normal"><a href="#__codelineno-0-474">474</a></span>
<span class="normal"><a href="#__codelineno-0-475">475</a></span>
<span class="normal"><a href="#__codelineno-0-476">476</a></span>
<span class="normal"><a href="#__codelineno-0-477">477</a></span>
<span class="normal"><a href="#__codelineno-0-478">478</a></span>
<span class="normal"><a href="#__codelineno-0-479">479</a></span>
<span class="normal"><a href="#__codelineno-0-480">480</a></span>
<span class="normal"><a href="#__codelineno-0-481">481</a></span>
<span class="normal"><a href="#__codelineno-0-482">482</a></span>
<span class="normal"><a href="#__codelineno-0-483">483</a></span>
<span class="normal"><a href="#__codelineno-0-484">484</a></span>
<span class="normal"><a href="#__codelineno-0-485">485</a></span>
<span class="normal"><a href="#__codelineno-0-486">486</a></span>
<span class="normal"><a href="#__codelineno-0-487">487</a></span>
<span class="normal"><a href="#__codelineno-0-488">488</a></span>
<span class="normal"><a href="#__codelineno-0-489">489</a></span>
<span class="normal"><a href="#__codelineno-0-490">490</a></span>
<span class="normal"><a href="#__codelineno-0-491">491</a></span>
<span class="normal"><a href="#__codelineno-0-492">492</a></span>
<span class="normal"><a href="#__codelineno-0-493">493</a></span>
<span class="normal"><a href="#__codelineno-0-494">494</a></span>
<span class="normal"><a href="#__codelineno-0-495">495</a></span>
<span class="normal"><a href="#__codelineno-0-496">496</a></span>
<span class="normal"><a href="#__codelineno-0-497">497</a></span>
<span class="normal"><a href="#__codelineno-0-498">498</a></span>
<span class="normal"><a href="#__codelineno-0-499">499</a></span>
<span class="normal"><a href="#__codelineno-0-500">500</a></span>
<span class="normal"><a href="#__codelineno-0-501">501</a></span>
<span class="normal"><a href="#__codelineno-0-502">502</a></span>
<span class="normal"><a href="#__codelineno-0-503">503</a></span>
<span class="normal"><a href="#__codelineno-0-504">504</a></span>
<span class="normal"><a href="#__codelineno-0-505">505</a></span>
<span class="normal"><a href="#__codelineno-0-506">506</a></span>
<span class="normal"><a href="#__codelineno-0-507">507</a></span>
<span class="normal"><a href="#__codelineno-0-508">508</a></span>
<span class="normal"><a href="#__codelineno-0-509">509</a></span>
<span class="normal"><a href="#__codelineno-0-510">510</a></span>
<span class="normal"><a href="#__codelineno-0-511">511</a></span>
<span class="normal"><a href="#__codelineno-0-512">512</a></span>
<span class="normal"><a href="#__codelineno-0-513">513</a></span>
<span class="normal"><a href="#__codelineno-0-514">514</a></span>
<span class="normal"><a href="#__codelineno-0-515">515</a></span>
<span class="normal"><a href="#__codelineno-0-516">516</a></span>
<span class="normal"><a href="#__codelineno-0-517">517</a></span>
<span class="normal"><a href="#__codelineno-0-518">518</a></span>
<span class="normal"><a href="#__codelineno-0-519">519</a></span>
<span class="normal"><a href="#__codelineno-0-520">520</a></span>
<span class="normal"><a href="#__codelineno-0-521">521</a></span>
<span class="normal"><a href="#__codelineno-0-522">522</a></span>
<span class="normal"><a href="#__codelineno-0-523">523</a></span>
<span class="normal"><a href="#__codelineno-0-524">524</a></span>
<span class="normal"><a href="#__codelineno-0-525">525</a></span>
<span class="normal"><a href="#__codelineno-0-526">526</a></span>
<span class="normal"><a href="#__codelineno-0-527">527</a></span>
<span class="normal"><a href="#__codelineno-0-528">528</a></span>
<span class="normal"><a href="#__codelineno-0-529">529</a></span>
<span class="normal"><a href="#__codelineno-0-530">530</a></span>
<span class="normal"><a href="#__codelineno-0-531">531</a></span>
<span class="normal"><a href="#__codelineno-0-532">532</a></span>
<span class="normal"><a href="#__codelineno-0-533">533</a></span>
<span class="normal"><a href="#__codelineno-0-534">534</a></span>
<span class="normal"><a href="#__codelineno-0-535">535</a></span>
<span class="normal"><a href="#__codelineno-0-536">536</a></span>
<span class="normal"><a href="#__codelineno-0-537">537</a></span>
<span class="normal"><a href="#__codelineno-0-538">538</a></span>
<span class="normal"><a href="#__codelineno-0-539">539</a></span>
<span class="normal"><a href="#__codelineno-0-540">540</a></span>
<span class="normal"><a href="#__codelineno-0-541">541</a></span>
<span class="normal"><a href="#__codelineno-0-542">542</a></span>
<span class="normal"><a href="#__codelineno-0-543">543</a></span>
<span class="normal"><a href="#__codelineno-0-544">544</a></span>
<span class="normal"><a href="#__codelineno-0-545">545</a></span>
<span class="normal"><a href="#__codelineno-0-546">546</a></span>
<span class="normal"><a href="#__codelineno-0-547">547</a></span>
<span class="normal"><a href="#__codelineno-0-548">548</a></span>
<span class="normal"><a href="#__codelineno-0-549">549</a></span>
<span class="normal"><a href="#__codelineno-0-550">550</a></span>
<span class="normal"><a href="#__codelineno-0-551">551</a></span>
<span class="normal"><a href="#__codelineno-0-552">552</a></span>
<span class="normal"><a href="#__codelineno-0-553">553</a></span>
<span class="normal"><a href="#__codelineno-0-554">554</a></span>
<span class="normal"><a href="#__codelineno-0-555">555</a></span>
<span class="normal"><a href="#__codelineno-0-556">556</a></span>
<span class="normal"><a href="#__codelineno-0-557">557</a></span>
<span class="normal"><a href="#__codelineno-0-558">558</a></span>
<span class="normal"><a href="#__codelineno-0-559">559</a></span>
<span class="normal"><a href="#__codelineno-0-560">560</a></span>
<span class="normal"><a href="#__codelineno-0-561">561</a></span>
<span class="normal"><a href="#__codelineno-0-562">562</a></span>
<span class="normal"><a href="#__codelineno-0-563">563</a></span>
<span class="normal"><a href="#__codelineno-0-564">564</a></span>
<span class="normal"><a href="#__codelineno-0-565">565</a></span>
<span class="normal"><a href="#__codelineno-0-566">566</a></span>
<span class="normal"><a href="#__codelineno-0-567">567</a></span>
<span class="normal"><a href="#__codelineno-0-568">568</a></span>
<span class="normal"><a href="#__codelineno-0-569">569</a></span>
<span class="normal"><a href="#__codelineno-0-570">570</a></span>
<span class="normal"><a href="#__codelineno-0-571">571</a></span>
<span class="normal"><a href="#__codelineno-0-572">572</a></span>
<span class="normal"><a href="#__codelineno-0-573">573</a></span>
<span class="normal"><a href="#__codelineno-0-574">574</a></span>
<span class="normal"><a href="#__codelineno-0-575">575</a></span>
<span class="normal"><a href="#__codelineno-0-576">576</a></span>
<span class="normal"><a href="#__codelineno-0-577">577</a></span>
<span class="normal"><a href="#__codelineno-0-578">578</a></span>
<span class="normal"><a href="#__codelineno-0-579">579</a></span>
<span class="normal"><a href="#__codelineno-0-580">580</a></span>
<span class="normal"><a href="#__codelineno-0-581">581</a></span>
<span class="normal"><a href="#__codelineno-0-582">582</a></span>
<span class="normal"><a href="#__codelineno-0-583">583</a></span>
<span class="normal"><a href="#__codelineno-0-584">584</a></span>
<span class="normal"><a href="#__codelineno-0-585">585</a></span>
<span class="normal"><a href="#__codelineno-0-586">586</a></span>
<span class="normal"><a href="#__codelineno-0-587">587</a></span>
<span class="normal"><a href="#__codelineno-0-588">588</a></span>
<span class="normal"><a href="#__codelineno-0-589">589</a></span>
<span class="normal"><a href="#__codelineno-0-590">590</a></span>
<span class="normal"><a href="#__codelineno-0-591">591</a></span>
<span class="normal"><a href="#__codelineno-0-592">592</a></span>
<span class="normal"><a href="#__codelineno-0-593">593</a></span>
<span class="normal"><a href="#__codelineno-0-594">594</a></span>
<span class="normal"><a href="#__codelineno-0-595">595</a></span>
<span class="normal"><a href="#__codelineno-0-596">596</a></span>
<span class="normal"><a href="#__codelineno-0-597">597</a></span>
<span class="normal"><a href="#__codelineno-0-598">598</a></span>
<span class="normal"><a href="#__codelineno-0-599">599</a></span>
<span class="normal"><a href="#__codelineno-0-600">600</a></span>
<span class="normal"><a href="#__codelineno-0-601">601</a></span>
<span class="normal"><a href="#__codelineno-0-602">602</a></span>
<span class="normal"><a href="#__codelineno-0-603">603</a></span>
<span class="normal"><a href="#__codelineno-0-604">604</a></span>
<span class="normal"><a href="#__codelineno-0-605">605</a></span>
<span class="normal"><a href="#__codelineno-0-606">606</a></span>
<span class="normal"><a href="#__codelineno-0-607">607</a></span>
<span class="normal"><a href="#__codelineno-0-608">608</a></span>
<span class="normal"><a href="#__codelineno-0-609">609</a></span>
<span class="normal"><a href="#__codelineno-0-610">610</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-442" name="__codelineno-0-442"></a><span class="k">class</span><span class="w"> </span><span class="nc">NTLoss</span><span class="p">(</span><span class="n">AbstractNTLoss</span><span class="p">):</span>
<a id="__codelineno-0-443" name="__codelineno-0-443"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for Wasserstein-based NTLoss. This is the default in the ICML paper.&quot;&quot;&quot;</span>
<a id="__codelineno-0-444" name="__codelineno-0-444"></a>
<a id="__codelineno-0-445" name="__codelineno-0-445"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-446" name="__codelineno-0-446"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-447" name="__codelineno-0-447"></a>        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-448" name="__codelineno-0-448"></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-449" name="__codelineno-0-449"></a>        <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-450" name="__codelineno-0-450"></a>        <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-451" name="__codelineno-0-451"></a>        <span class="n">squash_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-452" name="__codelineno-0-452"></a>    <span class="p">):</span>
<a id="__codelineno-0-453" name="__codelineno-0-453"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-454" name="__codelineno-0-454"></a><span class="sd">        NTL constructor for the Wasserstein-based NTLoss.</span>
<a id="__codelineno-0-455" name="__codelineno-0-455"></a>
<a id="__codelineno-0-456" name="__codelineno-0-456"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-457" name="__codelineno-0-457"></a><span class="sd">            tokenizer: Any HuggingFace tokenizer.</span>
<a id="__codelineno-0-458" name="__codelineno-0-458"></a><span class="sd">            vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-459" name="__codelineno-0-459"></a><span class="sd">                tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-460" name="__codelineno-0-460"></a><span class="sd">            digit_level: Whether to ensure only digits are considered number tokens,</span>
<a id="__codelineno-0-461" name="__codelineno-0-461"></a><span class="sd">                stabilizing training with NTL. Defaults to True. Used for most</span>
<a id="__codelineno-0-462" name="__codelineno-0-462"></a><span class="sd">                experiments in the ICML paper.</span>
<a id="__codelineno-0-463" name="__codelineno-0-463"></a><span class="sd">            reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-464" name="__codelineno-0-464"></a><span class="sd">                number tokens. Defaults to True.</span>
<a id="__codelineno-0-465" name="__codelineno-0-465"></a><span class="sd">                NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-466" name="__codelineno-0-466"></a><span class="sd">                incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-467" name="__codelineno-0-467"></a><span class="sd">            squash_factor: The optional squashing factor for the NTL. If provided,</span>
<a id="__codelineno-0-468" name="__codelineno-0-468"></a><span class="sd">                this number denotes the factor by which predicting the largest number</span>
<a id="__codelineno-0-469" name="__codelineno-0-469"></a><span class="sd">                token is worse than predicting the closest incorrect number token.</span>
<a id="__codelineno-0-470" name="__codelineno-0-470"></a><span class="sd">                E.g., with digit-level tokenization this factor is 9. Setting this</span>
<a id="__codelineno-0-471" name="__codelineno-0-471"></a><span class="sd">                to 1 will recover cross entropy. This argument is intended to handle</span>
<a id="__codelineno-0-472" name="__codelineno-0-472"></a><span class="sd">                irregular vocabs with large numerical token values.</span>
<a id="__codelineno-0-473" name="__codelineno-0-473"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-474" name="__codelineno-0-474"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-475" name="__codelineno-0-475"></a>            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<a id="__codelineno-0-476" name="__codelineno-0-476"></a>            <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-0-477" name="__codelineno-0-477"></a>            <span class="n">digit_level</span><span class="o">=</span><span class="n">digit_level</span><span class="p">,</span>
<a id="__codelineno-0-478" name="__codelineno-0-478"></a>            <span class="n">reweigh</span><span class="o">=</span><span class="n">reweigh</span><span class="p">,</span>
<a id="__codelineno-0-479" name="__codelineno-0-479"></a>        <span class="p">)</span>
<a id="__codelineno-0-480" name="__codelineno-0-480"></a>
<a id="__codelineno-0-481" name="__codelineno-0-481"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">squash_factor</span> <span class="o">=</span> <span class="n">squash_factor</span>
<a id="__codelineno-0-482" name="__codelineno-0-482"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">setup_distance_lookup</span><span class="p">(</span><span class="n">squash_factor</span><span class="p">)</span>
<a id="__codelineno-0-483" name="__codelineno-0-483"></a>
<a id="__codelineno-0-484" name="__codelineno-0-484"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_distance_lookup</span><span class="p">(</span>
<a id="__codelineno-0-485" name="__codelineno-0-485"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-486" name="__codelineno-0-486"></a>        <span class="n">squash_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-487" name="__codelineno-0-487"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-488" name="__codelineno-0-488"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-489" name="__codelineno-0-489"></a><span class="sd">        Set up a lookup table for the distances between the number tokens.</span>
<a id="__codelineno-0-490" name="__codelineno-0-490"></a><span class="sd">        Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.</span>
<a id="__codelineno-0-491" name="__codelineno-0-491"></a><span class="sd">        If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.</span>
<a id="__codelineno-0-492" name="__codelineno-0-492"></a><span class="sd">        NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</span>
<a id="__codelineno-0-493" name="__codelineno-0-493"></a>
<a id="__codelineno-0-494" name="__codelineno-0-494"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-495" name="__codelineno-0-495"></a><span class="sd">            squash_factor: The optional squashing factor used.</span>
<a id="__codelineno-0-496" name="__codelineno-0-496"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-497" name="__codelineno-0-497"></a>
<a id="__codelineno-0-498" name="__codelineno-0-498"></a>        <span class="c1"># Get token ids for number tokens</span>
<a id="__codelineno-0-499" name="__codelineno-0-499"></a>        <span class="n">num_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">,</span> <span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-500" name="__codelineno-0-500"></a>        <span class="c1"># Create mapping from number token ids to their index in order of appearance in vocab:</span>
<a id="__codelineno-0-501" name="__codelineno-0-501"></a>        <span class="c1"># e.g. token &quot;3&quot; -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1</span>
<a id="__codelineno-0-502" name="__codelineno-0-502"></a>        <span class="n">final_vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-503" name="__codelineno-0-503"></a>        <span class="n">vocab_to_dist_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">final_vocab_size</span><span class="p">,),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<a id="__codelineno-0-504" name="__codelineno-0-504"></a>        <span class="c1"># Use arange to ensure order of appearance</span>
<a id="__codelineno-0-505" name="__codelineno-0-505"></a>        <span class="n">vocab_to_dist_idx</span><span class="p">[</span><span class="n">num_ids</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<a id="__codelineno-0-506" name="__codelineno-0-506"></a>
<a id="__codelineno-0-507" name="__codelineno-0-507"></a>        <span class="c1"># Build NxN abs-diff matrix</span>
<a id="__codelineno-0-508" name="__codelineno-0-508"></a>        <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1 x N)</span>
<a id="__codelineno-0-509" name="__codelineno-0-509"></a>        <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vals</span> <span class="o">-</span> <span class="n">vals</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>  <span class="c1"># (N x N)</span>
<a id="__codelineno-0-510" name="__codelineno-0-510"></a>
<a id="__codelineno-0-511" name="__codelineno-0-511"></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">squash_factor</span><span class="p">,</span> <span class="n">Number</span><span class="p">):</span>
<a id="__codelineno-0-512" name="__codelineno-0-512"></a>            <span class="k">assert</span> <span class="n">squash_factor</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
<a id="__codelineno-0-513" name="__codelineno-0-513"></a>                <span class="sa">f</span><span class="s2">&quot;The squash factor can&#39;t be equal to or smaller than 1, please use a different squashing factor than </span><span class="si">{</span><span class="n">squash_factor</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-514" name="__codelineno-0-514"></a>            <span class="p">)</span>
<a id="__codelineno-0-515" name="__codelineno-0-515"></a>
<a id="__codelineno-0-516" name="__codelineno-0-516"></a>            <span class="c1"># Mask out zeros to find the smallest nonzero diff</span>
<a id="__codelineno-0-517" name="__codelineno-0-517"></a>            <span class="n">inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
<a id="__codelineno-0-518" name="__codelineno-0-518"></a>            <span class="n">diff_nonzero</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">diff</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inf</span><span class="p">)</span>
<a id="__codelineno-0-519" name="__codelineno-0-519"></a>            <span class="n">global_min_nz</span> <span class="o">=</span> <span class="n">diff_nonzero</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<a id="__codelineno-0-520" name="__codelineno-0-520"></a>            <span class="c1"># Find largest diff</span>
<a id="__codelineno-0-521" name="__codelineno-0-521"></a>            <span class="n">global_max</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a id="__codelineno-0-522" name="__codelineno-0-522"></a>
<a id="__codelineno-0-523" name="__codelineno-0-523"></a>            <span class="c1"># Compute scaling factor based on indicated squash factor</span>
<a id="__codelineno-0-524" name="__codelineno-0-524"></a>            <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">squash_factor</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">global_max</span> <span class="o">-</span> <span class="n">global_min_nz</span><span class="p">)</span>
<a id="__codelineno-0-525" name="__codelineno-0-525"></a>            <span class="c1"># Scale the absolute differences using scaling factor</span>
<a id="__codelineno-0-526" name="__codelineno-0-526"></a>            <span class="n">lookup</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">diff</span> <span class="o">-</span> <span class="n">global_min_nz</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
<a id="__codelineno-0-527" name="__codelineno-0-527"></a>            <span class="n">lookup</span><span class="p">[</span><span class="n">diff</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-0-528" name="__codelineno-0-528"></a>
<a id="__codelineno-0-529" name="__codelineno-0-529"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-530" name="__codelineno-0-530"></a>            <span class="n">lookup</span> <span class="o">=</span> <span class="n">diff</span>
<a id="__codelineno-0-531" name="__codelineno-0-531"></a>
<a id="__codelineno-0-532" name="__codelineno-0-532"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_to_dist_idx</span> <span class="o">=</span> <span class="n">vocab_to_dist_idx</span>
<a id="__codelineno-0-533" name="__codelineno-0-533"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dist_lookup</span> <span class="o">=</span> <span class="n">lookup</span>
<a id="__codelineno-0-534" name="__codelineno-0-534"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">lookup</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a id="__codelineno-0-535" name="__codelineno-0-535"></a>
<a id="__codelineno-0-536" name="__codelineno-0-536"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-537" name="__codelineno-0-537"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-538" name="__codelineno-0-538"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-539" name="__codelineno-0-539"></a>        <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-540" name="__codelineno-0-540"></a>        <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-541" name="__codelineno-0-541"></a>        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-542" name="__codelineno-0-542"></a>        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-543" name="__codelineno-0-543"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-544" name="__codelineno-0-544"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-545" name="__codelineno-0-545"></a><span class="sd">        Computes the NTL.</span>
<a id="__codelineno-0-546" name="__codelineno-0-546"></a>
<a id="__codelineno-0-547" name="__codelineno-0-547"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-548" name="__codelineno-0-548"></a><span class="sd">            logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-549" name="__codelineno-0-549"></a><span class="sd">            labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-550" name="__codelineno-0-550"></a><span class="sd">            loss_weights: Optional 2D tensor of BS x T with token-specific weights.</span>
<a id="__codelineno-0-551" name="__codelineno-0-551"></a><span class="sd">            reduction: Optional string specifying the reduction to apply to the</span>
<a id="__codelineno-0-552" name="__codelineno-0-552"></a><span class="sd">                output. Defaults to &quot;mean&quot;, options are &quot;mean&quot;, &quot;sum&quot;, &quot;none&quot;.</span>
<a id="__codelineno-0-553" name="__codelineno-0-553"></a><span class="sd">            ignore_index: The token ID to ignore in the labels. Defaults to -100.</span>
<a id="__codelineno-0-554" name="__codelineno-0-554"></a>
<a id="__codelineno-0-555" name="__codelineno-0-555"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-556" name="__codelineno-0-556"></a><span class="sd">            Loss tensor</span>
<a id="__codelineno-0-557" name="__codelineno-0-557"></a><span class="sd">                OD if reduction==&quot;mean&quot;|&quot;sum&quot;</span>
<a id="__codelineno-0-558" name="__codelineno-0-558"></a><span class="sd">                BS x T if reduction==&quot;none&quot;</span>
<a id="__codelineno-0-559" name="__codelineno-0-559"></a>
<a id="__codelineno-0-560" name="__codelineno-0-560"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-561" name="__codelineno-0-561"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
<a id="__codelineno-0-562" name="__codelineno-0-562"></a>
<a id="__codelineno-0-563" name="__codelineno-0-563"></a>        <span class="n">y</span><span class="p">,</span> <span class="n">loss_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_number_token_targets</span><span class="p">(</span>
<a id="__codelineno-0-564" name="__codelineno-0-564"></a>            <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">ignore_index</span>
<a id="__codelineno-0-565" name="__codelineno-0-565"></a>        <span class="p">)</span>
<a id="__codelineno-0-566" name="__codelineno-0-566"></a>        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-567" name="__codelineno-0-567"></a>        <span class="n">number_token_positions</span> <span class="o">=</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-568" name="__codelineno-0-568"></a>
<a id="__codelineno-0-569" name="__codelineno-0-569"></a>        <span class="c1"># If no digit tokens in batch, or total of the relevant loss_weights is zero, no need for upcoming calculations</span>
<a id="__codelineno-0-570" name="__codelineno-0-570"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">number_token_positions</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
<a id="__codelineno-0-571" name="__codelineno-0-571"></a>            <span class="k">if</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">):</span>
<a id="__codelineno-0-572" name="__codelineno-0-572"></a>                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-573" name="__codelineno-0-573"></a>            <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-574" name="__codelineno-0-574"></a>                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
<a id="__codelineno-0-575" name="__codelineno-0-575"></a>                    <span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span>
<a id="__codelineno-0-576" name="__codelineno-0-576"></a>                <span class="p">)</span>
<a id="__codelineno-0-577" name="__codelineno-0-577"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-578" name="__codelineno-0-578"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-579" name="__codelineno-0-579"></a>
<a id="__codelineno-0-580" name="__codelineno-0-580"></a>            <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-581" name="__codelineno-0-581"></a>
<a id="__codelineno-0-582" name="__codelineno-0-582"></a>        <span class="c1"># apply softmax and get number labels</span>
<a id="__codelineno-0-583" name="__codelineno-0-583"></a>        <span class="n">nt_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-584" name="__codelineno-0-584"></a>        <span class="n">softmax_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nt_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-585" name="__codelineno-0-585"></a>
<a id="__codelineno-0-586" name="__codelineno-0-586"></a>        <span class="c1"># get distance between the true numbers and all possible number values from lookup table</span>
<a id="__codelineno-0-587" name="__codelineno-0-587"></a>        <span class="n">abs_diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_lookup</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span>
<a id="__codelineno-0-588" name="__codelineno-0-588"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">vocab_to_dist_idx</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span>
<a id="__codelineno-0-589" name="__codelineno-0-589"></a>                <span class="n">labels</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-590" name="__codelineno-0-590"></a>            <span class="p">]</span>
<a id="__codelineno-0-591" name="__codelineno-0-591"></a>        <span class="p">]</span>
<a id="__codelineno-0-592" name="__codelineno-0-592"></a>
<a id="__codelineno-0-593" name="__codelineno-0-593"></a>        <span class="c1"># loss is the absolute difference weighted by the softmax probs</span>
<a id="__codelineno-0-594" name="__codelineno-0-594"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">abs_diff</span> <span class="o">*</span> <span class="n">softmax_probs</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-595" name="__codelineno-0-595"></a>
<a id="__codelineno-0-596" name="__codelineno-0-596"></a>        <span class="c1"># If reweigh: compute weights for NTL based on logits</span>
<a id="__codelineno-0-597" name="__codelineno-0-597"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span><span class="p">:</span>
<a id="__codelineno-0-598" name="__codelineno-0-598"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-599" name="__codelineno-0-599"></a>                <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-600" name="__codelineno-0-600"></a>            <span class="p">)</span>
<a id="__codelineno-0-601" name="__codelineno-0-601"></a>
<a id="__codelineno-0-602" name="__codelineno-0-602"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-603" name="__codelineno-0-603"></a>            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-604" name="__codelineno-0-604"></a>            <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
<a id="__codelineno-0-605" name="__codelineno-0-605"></a>            <span class="n">loss_weights</span><span class="o">=</span><span class="n">loss_weights</span><span class="p">,</span>
<a id="__codelineno-0-606" name="__codelineno-0-606"></a>            <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span><span class="p">,</span>
<a id="__codelineno-0-607" name="__codelineno-0-607"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
<a id="__codelineno-0-608" name="__codelineno-0-608"></a>        <span class="p">)</span>
<a id="__codelineno-0-609" name="__codelineno-0-609"></a>
<a id="__codelineno-0-610" name="__codelineno-0-610"></a>        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLoss.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">squash_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span></code>

<a href="#ntloss.core.NTLoss.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>NTL constructor for the Wasserstein-based NTLoss.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td>
                  <code><span title="transformers.PreTrainedTokenizer">PreTrainedTokenizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Any HuggingFace tokenizer.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional user-provided vocab size. If not provided, the
tokenizer's vocab size is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>digit_level</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to ensure only digits are considered number tokens,
stabilizing training with NTL. Defaults to True. Used for most
experiments in the ICML paper.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reweigh</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to scale the NTL using the logit weight on
number tokens. Defaults to True.
NOTE: The ICML paper does <em>not</em> use this option which can lead to
incorrect loss if most mass is placed outside of the number tokens.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>squash_factor</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The optional squashing factor for the NTL. If provided,
this number denotes the factor by which predicting the largest number
token is worse than predicting the closest incorrect number token.
E.g., with digit-level tokenization this factor is 9. Setting this
to 1 will recover cross entropy. This argument is intended to handle
irregular vocabs with large numerical token values.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-445">445</a></span>
<span class="normal"><a href="#__codelineno-0-446">446</a></span>
<span class="normal"><a href="#__codelineno-0-447">447</a></span>
<span class="normal"><a href="#__codelineno-0-448">448</a></span>
<span class="normal"><a href="#__codelineno-0-449">449</a></span>
<span class="normal"><a href="#__codelineno-0-450">450</a></span>
<span class="normal"><a href="#__codelineno-0-451">451</a></span>
<span class="normal"><a href="#__codelineno-0-452">452</a></span>
<span class="normal"><a href="#__codelineno-0-453">453</a></span>
<span class="normal"><a href="#__codelineno-0-454">454</a></span>
<span class="normal"><a href="#__codelineno-0-455">455</a></span>
<span class="normal"><a href="#__codelineno-0-456">456</a></span>
<span class="normal"><a href="#__codelineno-0-457">457</a></span>
<span class="normal"><a href="#__codelineno-0-458">458</a></span>
<span class="normal"><a href="#__codelineno-0-459">459</a></span>
<span class="normal"><a href="#__codelineno-0-460">460</a></span>
<span class="normal"><a href="#__codelineno-0-461">461</a></span>
<span class="normal"><a href="#__codelineno-0-462">462</a></span>
<span class="normal"><a href="#__codelineno-0-463">463</a></span>
<span class="normal"><a href="#__codelineno-0-464">464</a></span>
<span class="normal"><a href="#__codelineno-0-465">465</a></span>
<span class="normal"><a href="#__codelineno-0-466">466</a></span>
<span class="normal"><a href="#__codelineno-0-467">467</a></span>
<span class="normal"><a href="#__codelineno-0-468">468</a></span>
<span class="normal"><a href="#__codelineno-0-469">469</a></span>
<span class="normal"><a href="#__codelineno-0-470">470</a></span>
<span class="normal"><a href="#__codelineno-0-471">471</a></span>
<span class="normal"><a href="#__codelineno-0-472">472</a></span>
<span class="normal"><a href="#__codelineno-0-473">473</a></span>
<span class="normal"><a href="#__codelineno-0-474">474</a></span>
<span class="normal"><a href="#__codelineno-0-475">475</a></span>
<span class="normal"><a href="#__codelineno-0-476">476</a></span>
<span class="normal"><a href="#__codelineno-0-477">477</a></span>
<span class="normal"><a href="#__codelineno-0-478">478</a></span>
<span class="normal"><a href="#__codelineno-0-479">479</a></span>
<span class="normal"><a href="#__codelineno-0-480">480</a></span>
<span class="normal"><a href="#__codelineno-0-481">481</a></span>
<span class="normal"><a href="#__codelineno-0-482">482</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-445" name="__codelineno-0-445"></a><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-446" name="__codelineno-0-446"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-447" name="__codelineno-0-447"></a>    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-448" name="__codelineno-0-448"></a>    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-449" name="__codelineno-0-449"></a>    <span class="n">digit_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-450" name="__codelineno-0-450"></a>    <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-451" name="__codelineno-0-451"></a>    <span class="n">squash_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-452" name="__codelineno-0-452"></a><span class="p">):</span>
<a id="__codelineno-0-453" name="__codelineno-0-453"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-454" name="__codelineno-0-454"></a><span class="sd">    NTL constructor for the Wasserstein-based NTLoss.</span>
<a id="__codelineno-0-455" name="__codelineno-0-455"></a>
<a id="__codelineno-0-456" name="__codelineno-0-456"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-457" name="__codelineno-0-457"></a><span class="sd">        tokenizer: Any HuggingFace tokenizer.</span>
<a id="__codelineno-0-458" name="__codelineno-0-458"></a><span class="sd">        vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-459" name="__codelineno-0-459"></a><span class="sd">            tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-460" name="__codelineno-0-460"></a><span class="sd">        digit_level: Whether to ensure only digits are considered number tokens,</span>
<a id="__codelineno-0-461" name="__codelineno-0-461"></a><span class="sd">            stabilizing training with NTL. Defaults to True. Used for most</span>
<a id="__codelineno-0-462" name="__codelineno-0-462"></a><span class="sd">            experiments in the ICML paper.</span>
<a id="__codelineno-0-463" name="__codelineno-0-463"></a><span class="sd">        reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-464" name="__codelineno-0-464"></a><span class="sd">            number tokens. Defaults to True.</span>
<a id="__codelineno-0-465" name="__codelineno-0-465"></a><span class="sd">            NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-466" name="__codelineno-0-466"></a><span class="sd">            incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-467" name="__codelineno-0-467"></a><span class="sd">        squash_factor: The optional squashing factor for the NTL. If provided,</span>
<a id="__codelineno-0-468" name="__codelineno-0-468"></a><span class="sd">            this number denotes the factor by which predicting the largest number</span>
<a id="__codelineno-0-469" name="__codelineno-0-469"></a><span class="sd">            token is worse than predicting the closest incorrect number token.</span>
<a id="__codelineno-0-470" name="__codelineno-0-470"></a><span class="sd">            E.g., with digit-level tokenization this factor is 9. Setting this</span>
<a id="__codelineno-0-471" name="__codelineno-0-471"></a><span class="sd">            to 1 will recover cross entropy. This argument is intended to handle</span>
<a id="__codelineno-0-472" name="__codelineno-0-472"></a><span class="sd">            irregular vocabs with large numerical token values.</span>
<a id="__codelineno-0-473" name="__codelineno-0-473"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-474" name="__codelineno-0-474"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-475" name="__codelineno-0-475"></a>        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<a id="__codelineno-0-476" name="__codelineno-0-476"></a>        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-0-477" name="__codelineno-0-477"></a>        <span class="n">digit_level</span><span class="o">=</span><span class="n">digit_level</span><span class="p">,</span>
<a id="__codelineno-0-478" name="__codelineno-0-478"></a>        <span class="n">reweigh</span><span class="o">=</span><span class="n">reweigh</span><span class="p">,</span>
<a id="__codelineno-0-479" name="__codelineno-0-479"></a>    <span class="p">)</span>
<a id="__codelineno-0-480" name="__codelineno-0-480"></a>
<a id="__codelineno-0-481" name="__codelineno-0-481"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">squash_factor</span> <span class="o">=</span> <span class="n">squash_factor</span>
<a id="__codelineno-0-482" name="__codelineno-0-482"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">setup_distance_lookup</span><span class="p">(</span><span class="n">squash_factor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLoss.setup_distance_lookup" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">setup_distance_lookup</span><span class="p">(</span><span class="n">squash_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

<a href="#ntloss.core.NTLoss.setup_distance_lookup" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set up a lookup table for the distances between the number tokens.
Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.
If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.
NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>squash_factor</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The optional squashing factor used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-484">484</a></span>
<span class="normal"><a href="#__codelineno-0-485">485</a></span>
<span class="normal"><a href="#__codelineno-0-486">486</a></span>
<span class="normal"><a href="#__codelineno-0-487">487</a></span>
<span class="normal"><a href="#__codelineno-0-488">488</a></span>
<span class="normal"><a href="#__codelineno-0-489">489</a></span>
<span class="normal"><a href="#__codelineno-0-490">490</a></span>
<span class="normal"><a href="#__codelineno-0-491">491</a></span>
<span class="normal"><a href="#__codelineno-0-492">492</a></span>
<span class="normal"><a href="#__codelineno-0-493">493</a></span>
<span class="normal"><a href="#__codelineno-0-494">494</a></span>
<span class="normal"><a href="#__codelineno-0-495">495</a></span>
<span class="normal"><a href="#__codelineno-0-496">496</a></span>
<span class="normal"><a href="#__codelineno-0-497">497</a></span>
<span class="normal"><a href="#__codelineno-0-498">498</a></span>
<span class="normal"><a href="#__codelineno-0-499">499</a></span>
<span class="normal"><a href="#__codelineno-0-500">500</a></span>
<span class="normal"><a href="#__codelineno-0-501">501</a></span>
<span class="normal"><a href="#__codelineno-0-502">502</a></span>
<span class="normal"><a href="#__codelineno-0-503">503</a></span>
<span class="normal"><a href="#__codelineno-0-504">504</a></span>
<span class="normal"><a href="#__codelineno-0-505">505</a></span>
<span class="normal"><a href="#__codelineno-0-506">506</a></span>
<span class="normal"><a href="#__codelineno-0-507">507</a></span>
<span class="normal"><a href="#__codelineno-0-508">508</a></span>
<span class="normal"><a href="#__codelineno-0-509">509</a></span>
<span class="normal"><a href="#__codelineno-0-510">510</a></span>
<span class="normal"><a href="#__codelineno-0-511">511</a></span>
<span class="normal"><a href="#__codelineno-0-512">512</a></span>
<span class="normal"><a href="#__codelineno-0-513">513</a></span>
<span class="normal"><a href="#__codelineno-0-514">514</a></span>
<span class="normal"><a href="#__codelineno-0-515">515</a></span>
<span class="normal"><a href="#__codelineno-0-516">516</a></span>
<span class="normal"><a href="#__codelineno-0-517">517</a></span>
<span class="normal"><a href="#__codelineno-0-518">518</a></span>
<span class="normal"><a href="#__codelineno-0-519">519</a></span>
<span class="normal"><a href="#__codelineno-0-520">520</a></span>
<span class="normal"><a href="#__codelineno-0-521">521</a></span>
<span class="normal"><a href="#__codelineno-0-522">522</a></span>
<span class="normal"><a href="#__codelineno-0-523">523</a></span>
<span class="normal"><a href="#__codelineno-0-524">524</a></span>
<span class="normal"><a href="#__codelineno-0-525">525</a></span>
<span class="normal"><a href="#__codelineno-0-526">526</a></span>
<span class="normal"><a href="#__codelineno-0-527">527</a></span>
<span class="normal"><a href="#__codelineno-0-528">528</a></span>
<span class="normal"><a href="#__codelineno-0-529">529</a></span>
<span class="normal"><a href="#__codelineno-0-530">530</a></span>
<span class="normal"><a href="#__codelineno-0-531">531</a></span>
<span class="normal"><a href="#__codelineno-0-532">532</a></span>
<span class="normal"><a href="#__codelineno-0-533">533</a></span>
<span class="normal"><a href="#__codelineno-0-534">534</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-484" name="__codelineno-0-484"></a><span class="k">def</span><span class="w"> </span><span class="nf">setup_distance_lookup</span><span class="p">(</span>
<a id="__codelineno-0-485" name="__codelineno-0-485"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-486" name="__codelineno-0-486"></a>    <span class="n">squash_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-487" name="__codelineno-0-487"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-488" name="__codelineno-0-488"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-489" name="__codelineno-0-489"></a><span class="sd">    Set up a lookup table for the distances between the number tokens.</span>
<a id="__codelineno-0-490" name="__codelineno-0-490"></a><span class="sd">    Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.</span>
<a id="__codelineno-0-491" name="__codelineno-0-491"></a><span class="sd">    If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.</span>
<a id="__codelineno-0-492" name="__codelineno-0-492"></a><span class="sd">    NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</span>
<a id="__codelineno-0-493" name="__codelineno-0-493"></a>
<a id="__codelineno-0-494" name="__codelineno-0-494"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-495" name="__codelineno-0-495"></a><span class="sd">        squash_factor: The optional squashing factor used.</span>
<a id="__codelineno-0-496" name="__codelineno-0-496"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-497" name="__codelineno-0-497"></a>
<a id="__codelineno-0-498" name="__codelineno-0-498"></a>    <span class="c1"># Get token ids for number tokens</span>
<a id="__codelineno-0-499" name="__codelineno-0-499"></a>    <span class="n">num_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">,</span> <span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-500" name="__codelineno-0-500"></a>    <span class="c1"># Create mapping from number token ids to their index in order of appearance in vocab:</span>
<a id="__codelineno-0-501" name="__codelineno-0-501"></a>    <span class="c1"># e.g. token &quot;3&quot; -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1</span>
<a id="__codelineno-0-502" name="__codelineno-0-502"></a>    <span class="n">final_vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-503" name="__codelineno-0-503"></a>    <span class="n">vocab_to_dist_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">final_vocab_size</span><span class="p">,),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<a id="__codelineno-0-504" name="__codelineno-0-504"></a>    <span class="c1"># Use arange to ensure order of appearance</span>
<a id="__codelineno-0-505" name="__codelineno-0-505"></a>    <span class="n">vocab_to_dist_idx</span><span class="p">[</span><span class="n">num_ids</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<a id="__codelineno-0-506" name="__codelineno-0-506"></a>
<a id="__codelineno-0-507" name="__codelineno-0-507"></a>    <span class="c1"># Build NxN abs-diff matrix</span>
<a id="__codelineno-0-508" name="__codelineno-0-508"></a>    <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_values_dense</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1 x N)</span>
<a id="__codelineno-0-509" name="__codelineno-0-509"></a>    <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vals</span> <span class="o">-</span> <span class="n">vals</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>  <span class="c1"># (N x N)</span>
<a id="__codelineno-0-510" name="__codelineno-0-510"></a>
<a id="__codelineno-0-511" name="__codelineno-0-511"></a>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">squash_factor</span><span class="p">,</span> <span class="n">Number</span><span class="p">):</span>
<a id="__codelineno-0-512" name="__codelineno-0-512"></a>        <span class="k">assert</span> <span class="n">squash_factor</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
<a id="__codelineno-0-513" name="__codelineno-0-513"></a>            <span class="sa">f</span><span class="s2">&quot;The squash factor can&#39;t be equal to or smaller than 1, please use a different squashing factor than </span><span class="si">{</span><span class="n">squash_factor</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-514" name="__codelineno-0-514"></a>        <span class="p">)</span>
<a id="__codelineno-0-515" name="__codelineno-0-515"></a>
<a id="__codelineno-0-516" name="__codelineno-0-516"></a>        <span class="c1"># Mask out zeros to find the smallest nonzero diff</span>
<a id="__codelineno-0-517" name="__codelineno-0-517"></a>        <span class="n">inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
<a id="__codelineno-0-518" name="__codelineno-0-518"></a>        <span class="n">diff_nonzero</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">diff</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inf</span><span class="p">)</span>
<a id="__codelineno-0-519" name="__codelineno-0-519"></a>        <span class="n">global_min_nz</span> <span class="o">=</span> <span class="n">diff_nonzero</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<a id="__codelineno-0-520" name="__codelineno-0-520"></a>        <span class="c1"># Find largest diff</span>
<a id="__codelineno-0-521" name="__codelineno-0-521"></a>        <span class="n">global_max</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a id="__codelineno-0-522" name="__codelineno-0-522"></a>
<a id="__codelineno-0-523" name="__codelineno-0-523"></a>        <span class="c1"># Compute scaling factor based on indicated squash factor</span>
<a id="__codelineno-0-524" name="__codelineno-0-524"></a>        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">squash_factor</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">global_max</span> <span class="o">-</span> <span class="n">global_min_nz</span><span class="p">)</span>
<a id="__codelineno-0-525" name="__codelineno-0-525"></a>        <span class="c1"># Scale the absolute differences using scaling factor</span>
<a id="__codelineno-0-526" name="__codelineno-0-526"></a>        <span class="n">lookup</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">diff</span> <span class="o">-</span> <span class="n">global_min_nz</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
<a id="__codelineno-0-527" name="__codelineno-0-527"></a>        <span class="n">lookup</span><span class="p">[</span><span class="n">diff</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-0-528" name="__codelineno-0-528"></a>
<a id="__codelineno-0-529" name="__codelineno-0-529"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-530" name="__codelineno-0-530"></a>        <span class="n">lookup</span> <span class="o">=</span> <span class="n">diff</span>
<a id="__codelineno-0-531" name="__codelineno-0-531"></a>
<a id="__codelineno-0-532" name="__codelineno-0-532"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_to_dist_idx</span> <span class="o">=</span> <span class="n">vocab_to_dist_idx</span>
<a id="__codelineno-0-533" name="__codelineno-0-533"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">dist_lookup</span> <span class="o">=</span> <span class="n">lookup</span>
<a id="__codelineno-0-534" name="__codelineno-0-534"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">lookup</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NTLoss.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span></code>

<a href="#ntloss.core.NTLoss.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Computes the NTL.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>3D Tensor of shape BS x T x V.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td>
                  <code><span title="torch.LongTensor">LongTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D Tensor of shape BS x T.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss_weights</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional 2D tensor of BS x T with token-specific weights.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reduction</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional string specifying the reduction to apply to the
output. Defaults to "mean", options are "mean", "sum", "none".</p>
              </div>
            </td>
            <td>
                  <code>&#39;mean&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_index</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The token ID to ignore in the labels. Defaults to -100.</p>
              </div>
            </td>
            <td>
                  <code>-100</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Loss tensor
OD if reduction=="mean"|"sum"
BS x T if reduction=="none"</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-536">536</a></span>
<span class="normal"><a href="#__codelineno-0-537">537</a></span>
<span class="normal"><a href="#__codelineno-0-538">538</a></span>
<span class="normal"><a href="#__codelineno-0-539">539</a></span>
<span class="normal"><a href="#__codelineno-0-540">540</a></span>
<span class="normal"><a href="#__codelineno-0-541">541</a></span>
<span class="normal"><a href="#__codelineno-0-542">542</a></span>
<span class="normal"><a href="#__codelineno-0-543">543</a></span>
<span class="normal"><a href="#__codelineno-0-544">544</a></span>
<span class="normal"><a href="#__codelineno-0-545">545</a></span>
<span class="normal"><a href="#__codelineno-0-546">546</a></span>
<span class="normal"><a href="#__codelineno-0-547">547</a></span>
<span class="normal"><a href="#__codelineno-0-548">548</a></span>
<span class="normal"><a href="#__codelineno-0-549">549</a></span>
<span class="normal"><a href="#__codelineno-0-550">550</a></span>
<span class="normal"><a href="#__codelineno-0-551">551</a></span>
<span class="normal"><a href="#__codelineno-0-552">552</a></span>
<span class="normal"><a href="#__codelineno-0-553">553</a></span>
<span class="normal"><a href="#__codelineno-0-554">554</a></span>
<span class="normal"><a href="#__codelineno-0-555">555</a></span>
<span class="normal"><a href="#__codelineno-0-556">556</a></span>
<span class="normal"><a href="#__codelineno-0-557">557</a></span>
<span class="normal"><a href="#__codelineno-0-558">558</a></span>
<span class="normal"><a href="#__codelineno-0-559">559</a></span>
<span class="normal"><a href="#__codelineno-0-560">560</a></span>
<span class="normal"><a href="#__codelineno-0-561">561</a></span>
<span class="normal"><a href="#__codelineno-0-562">562</a></span>
<span class="normal"><a href="#__codelineno-0-563">563</a></span>
<span class="normal"><a href="#__codelineno-0-564">564</a></span>
<span class="normal"><a href="#__codelineno-0-565">565</a></span>
<span class="normal"><a href="#__codelineno-0-566">566</a></span>
<span class="normal"><a href="#__codelineno-0-567">567</a></span>
<span class="normal"><a href="#__codelineno-0-568">568</a></span>
<span class="normal"><a href="#__codelineno-0-569">569</a></span>
<span class="normal"><a href="#__codelineno-0-570">570</a></span>
<span class="normal"><a href="#__codelineno-0-571">571</a></span>
<span class="normal"><a href="#__codelineno-0-572">572</a></span>
<span class="normal"><a href="#__codelineno-0-573">573</a></span>
<span class="normal"><a href="#__codelineno-0-574">574</a></span>
<span class="normal"><a href="#__codelineno-0-575">575</a></span>
<span class="normal"><a href="#__codelineno-0-576">576</a></span>
<span class="normal"><a href="#__codelineno-0-577">577</a></span>
<span class="normal"><a href="#__codelineno-0-578">578</a></span>
<span class="normal"><a href="#__codelineno-0-579">579</a></span>
<span class="normal"><a href="#__codelineno-0-580">580</a></span>
<span class="normal"><a href="#__codelineno-0-581">581</a></span>
<span class="normal"><a href="#__codelineno-0-582">582</a></span>
<span class="normal"><a href="#__codelineno-0-583">583</a></span>
<span class="normal"><a href="#__codelineno-0-584">584</a></span>
<span class="normal"><a href="#__codelineno-0-585">585</a></span>
<span class="normal"><a href="#__codelineno-0-586">586</a></span>
<span class="normal"><a href="#__codelineno-0-587">587</a></span>
<span class="normal"><a href="#__codelineno-0-588">588</a></span>
<span class="normal"><a href="#__codelineno-0-589">589</a></span>
<span class="normal"><a href="#__codelineno-0-590">590</a></span>
<span class="normal"><a href="#__codelineno-0-591">591</a></span>
<span class="normal"><a href="#__codelineno-0-592">592</a></span>
<span class="normal"><a href="#__codelineno-0-593">593</a></span>
<span class="normal"><a href="#__codelineno-0-594">594</a></span>
<span class="normal"><a href="#__codelineno-0-595">595</a></span>
<span class="normal"><a href="#__codelineno-0-596">596</a></span>
<span class="normal"><a href="#__codelineno-0-597">597</a></span>
<span class="normal"><a href="#__codelineno-0-598">598</a></span>
<span class="normal"><a href="#__codelineno-0-599">599</a></span>
<span class="normal"><a href="#__codelineno-0-600">600</a></span>
<span class="normal"><a href="#__codelineno-0-601">601</a></span>
<span class="normal"><a href="#__codelineno-0-602">602</a></span>
<span class="normal"><a href="#__codelineno-0-603">603</a></span>
<span class="normal"><a href="#__codelineno-0-604">604</a></span>
<span class="normal"><a href="#__codelineno-0-605">605</a></span>
<span class="normal"><a href="#__codelineno-0-606">606</a></span>
<span class="normal"><a href="#__codelineno-0-607">607</a></span>
<span class="normal"><a href="#__codelineno-0-608">608</a></span>
<span class="normal"><a href="#__codelineno-0-609">609</a></span>
<span class="normal"><a href="#__codelineno-0-610">610</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-536" name="__codelineno-0-536"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-537" name="__codelineno-0-537"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-538" name="__codelineno-0-538"></a>    <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-539" name="__codelineno-0-539"></a>    <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-540" name="__codelineno-0-540"></a>    <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-541" name="__codelineno-0-541"></a>    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-542" name="__codelineno-0-542"></a>    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-543" name="__codelineno-0-543"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-544" name="__codelineno-0-544"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-545" name="__codelineno-0-545"></a><span class="sd">    Computes the NTL.</span>
<a id="__codelineno-0-546" name="__codelineno-0-546"></a>
<a id="__codelineno-0-547" name="__codelineno-0-547"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-548" name="__codelineno-0-548"></a><span class="sd">        logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-549" name="__codelineno-0-549"></a><span class="sd">        labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-550" name="__codelineno-0-550"></a><span class="sd">        loss_weights: Optional 2D tensor of BS x T with token-specific weights.</span>
<a id="__codelineno-0-551" name="__codelineno-0-551"></a><span class="sd">        reduction: Optional string specifying the reduction to apply to the</span>
<a id="__codelineno-0-552" name="__codelineno-0-552"></a><span class="sd">            output. Defaults to &quot;mean&quot;, options are &quot;mean&quot;, &quot;sum&quot;, &quot;none&quot;.</span>
<a id="__codelineno-0-553" name="__codelineno-0-553"></a><span class="sd">        ignore_index: The token ID to ignore in the labels. Defaults to -100.</span>
<a id="__codelineno-0-554" name="__codelineno-0-554"></a>
<a id="__codelineno-0-555" name="__codelineno-0-555"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-556" name="__codelineno-0-556"></a><span class="sd">        Loss tensor</span>
<a id="__codelineno-0-557" name="__codelineno-0-557"></a><span class="sd">            OD if reduction==&quot;mean&quot;|&quot;sum&quot;</span>
<a id="__codelineno-0-558" name="__codelineno-0-558"></a><span class="sd">            BS x T if reduction==&quot;none&quot;</span>
<a id="__codelineno-0-559" name="__codelineno-0-559"></a>
<a id="__codelineno-0-560" name="__codelineno-0-560"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-561" name="__codelineno-0-561"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
<a id="__codelineno-0-562" name="__codelineno-0-562"></a>
<a id="__codelineno-0-563" name="__codelineno-0-563"></a>    <span class="n">y</span><span class="p">,</span> <span class="n">loss_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_number_token_targets</span><span class="p">(</span>
<a id="__codelineno-0-564" name="__codelineno-0-564"></a>        <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">ignore_index</span>
<a id="__codelineno-0-565" name="__codelineno-0-565"></a>    <span class="p">)</span>
<a id="__codelineno-0-566" name="__codelineno-0-566"></a>    <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-567" name="__codelineno-0-567"></a>    <span class="n">number_token_positions</span> <span class="o">=</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-568" name="__codelineno-0-568"></a>
<a id="__codelineno-0-569" name="__codelineno-0-569"></a>    <span class="c1"># If no digit tokens in batch, or total of the relevant loss_weights is zero, no need for upcoming calculations</span>
<a id="__codelineno-0-570" name="__codelineno-0-570"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">number_token_positions</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
<a id="__codelineno-0-571" name="__codelineno-0-571"></a>        <span class="k">if</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">):</span>
<a id="__codelineno-0-572" name="__codelineno-0-572"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-573" name="__codelineno-0-573"></a>        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-574" name="__codelineno-0-574"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
<a id="__codelineno-0-575" name="__codelineno-0-575"></a>                <span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span>
<a id="__codelineno-0-576" name="__codelineno-0-576"></a>            <span class="p">)</span>
<a id="__codelineno-0-577" name="__codelineno-0-577"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-578" name="__codelineno-0-578"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-579" name="__codelineno-0-579"></a>
<a id="__codelineno-0-580" name="__codelineno-0-580"></a>        <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-581" name="__codelineno-0-581"></a>
<a id="__codelineno-0-582" name="__codelineno-0-582"></a>    <span class="c1"># apply softmax and get number labels</span>
<a id="__codelineno-0-583" name="__codelineno-0-583"></a>    <span class="n">nt_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_number_token</span><span class="p">]</span>
<a id="__codelineno-0-584" name="__codelineno-0-584"></a>    <span class="n">softmax_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nt_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-585" name="__codelineno-0-585"></a>
<a id="__codelineno-0-586" name="__codelineno-0-586"></a>    <span class="c1"># get distance between the true numbers and all possible number values from lookup table</span>
<a id="__codelineno-0-587" name="__codelineno-0-587"></a>    <span class="n">abs_diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_lookup</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span>
<a id="__codelineno-0-588" name="__codelineno-0-588"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_to_dist_idx</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span>
<a id="__codelineno-0-589" name="__codelineno-0-589"></a>            <span class="n">labels</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-590" name="__codelineno-0-590"></a>        <span class="p">]</span>
<a id="__codelineno-0-591" name="__codelineno-0-591"></a>    <span class="p">]</span>
<a id="__codelineno-0-592" name="__codelineno-0-592"></a>
<a id="__codelineno-0-593" name="__codelineno-0-593"></a>    <span class="c1"># loss is the absolute difference weighted by the softmax probs</span>
<a id="__codelineno-0-594" name="__codelineno-0-594"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">abs_diff</span> <span class="o">*</span> <span class="n">softmax_probs</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-595" name="__codelineno-0-595"></a>
<a id="__codelineno-0-596" name="__codelineno-0-596"></a>    <span class="c1"># If reweigh: compute weights for NTL based on logits</span>
<a id="__codelineno-0-597" name="__codelineno-0-597"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span><span class="p">:</span>
<a id="__codelineno-0-598" name="__codelineno-0-598"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-599" name="__codelineno-0-599"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-600" name="__codelineno-0-600"></a>        <span class="p">)</span>
<a id="__codelineno-0-601" name="__codelineno-0-601"></a>
<a id="__codelineno-0-602" name="__codelineno-0-602"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-603" name="__codelineno-0-603"></a>        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-604" name="__codelineno-0-604"></a>        <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
<a id="__codelineno-0-605" name="__codelineno-0-605"></a>        <span class="n">loss_weights</span><span class="o">=</span><span class="n">loss_weights</span><span class="p">,</span>
<a id="__codelineno-0-606" name="__codelineno-0-606"></a>        <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span><span class="p">,</span>
<a id="__codelineno-0-607" name="__codelineno-0-607"></a>        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
<a id="__codelineno-0-608" name="__codelineno-0-608"></a>    <span class="p">)</span>
<a id="__codelineno-0-609" name="__codelineno-0-609"></a>
<a id="__codelineno-0-610" name="__codelineno-0-610"></a>    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="ntloss.core.NumberLevelLoss" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>NumberLevelLoss</code>


<a href="#ntloss.core.NumberLevelLoss" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;NTLossDotProduct&lt;/code&gt; (&lt;code&gt;ntloss.core.NTLossDotProduct&lt;/code&gt;)" href="../#ntloss.core.NTLossDotProduct">NTLossDotProduct</a></code></p>


        <p>Class to calculate NTL on a per-number (rather than per-token) basis.</p>







              <details class="quote">
                <summary>Source code in <code>ntloss/core.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-613">613</a></span>
<span class="normal"><a href="#__codelineno-0-614">614</a></span>
<span class="normal"><a href="#__codelineno-0-615">615</a></span>
<span class="normal"><a href="#__codelineno-0-616">616</a></span>
<span class="normal"><a href="#__codelineno-0-617">617</a></span>
<span class="normal"><a href="#__codelineno-0-618">618</a></span>
<span class="normal"><a href="#__codelineno-0-619">619</a></span>
<span class="normal"><a href="#__codelineno-0-620">620</a></span>
<span class="normal"><a href="#__codelineno-0-621">621</a></span>
<span class="normal"><a href="#__codelineno-0-622">622</a></span>
<span class="normal"><a href="#__codelineno-0-623">623</a></span>
<span class="normal"><a href="#__codelineno-0-624">624</a></span>
<span class="normal"><a href="#__codelineno-0-625">625</a></span>
<span class="normal"><a href="#__codelineno-0-626">626</a></span>
<span class="normal"><a href="#__codelineno-0-627">627</a></span>
<span class="normal"><a href="#__codelineno-0-628">628</a></span>
<span class="normal"><a href="#__codelineno-0-629">629</a></span>
<span class="normal"><a href="#__codelineno-0-630">630</a></span>
<span class="normal"><a href="#__codelineno-0-631">631</a></span>
<span class="normal"><a href="#__codelineno-0-632">632</a></span>
<span class="normal"><a href="#__codelineno-0-633">633</a></span>
<span class="normal"><a href="#__codelineno-0-634">634</a></span>
<span class="normal"><a href="#__codelineno-0-635">635</a></span>
<span class="normal"><a href="#__codelineno-0-636">636</a></span>
<span class="normal"><a href="#__codelineno-0-637">637</a></span>
<span class="normal"><a href="#__codelineno-0-638">638</a></span>
<span class="normal"><a href="#__codelineno-0-639">639</a></span>
<span class="normal"><a href="#__codelineno-0-640">640</a></span>
<span class="normal"><a href="#__codelineno-0-641">641</a></span>
<span class="normal"><a href="#__codelineno-0-642">642</a></span>
<span class="normal"><a href="#__codelineno-0-643">643</a></span>
<span class="normal"><a href="#__codelineno-0-644">644</a></span>
<span class="normal"><a href="#__codelineno-0-645">645</a></span>
<span class="normal"><a href="#__codelineno-0-646">646</a></span>
<span class="normal"><a href="#__codelineno-0-647">647</a></span>
<span class="normal"><a href="#__codelineno-0-648">648</a></span>
<span class="normal"><a href="#__codelineno-0-649">649</a></span>
<span class="normal"><a href="#__codelineno-0-650">650</a></span>
<span class="normal"><a href="#__codelineno-0-651">651</a></span>
<span class="normal"><a href="#__codelineno-0-652">652</a></span>
<span class="normal"><a href="#__codelineno-0-653">653</a></span>
<span class="normal"><a href="#__codelineno-0-654">654</a></span>
<span class="normal"><a href="#__codelineno-0-655">655</a></span>
<span class="normal"><a href="#__codelineno-0-656">656</a></span>
<span class="normal"><a href="#__codelineno-0-657">657</a></span>
<span class="normal"><a href="#__codelineno-0-658">658</a></span>
<span class="normal"><a href="#__codelineno-0-659">659</a></span>
<span class="normal"><a href="#__codelineno-0-660">660</a></span>
<span class="normal"><a href="#__codelineno-0-661">661</a></span>
<span class="normal"><a href="#__codelineno-0-662">662</a></span>
<span class="normal"><a href="#__codelineno-0-663">663</a></span>
<span class="normal"><a href="#__codelineno-0-664">664</a></span>
<span class="normal"><a href="#__codelineno-0-665">665</a></span>
<span class="normal"><a href="#__codelineno-0-666">666</a></span>
<span class="normal"><a href="#__codelineno-0-667">667</a></span>
<span class="normal"><a href="#__codelineno-0-668">668</a></span>
<span class="normal"><a href="#__codelineno-0-669">669</a></span>
<span class="normal"><a href="#__codelineno-0-670">670</a></span>
<span class="normal"><a href="#__codelineno-0-671">671</a></span>
<span class="normal"><a href="#__codelineno-0-672">672</a></span>
<span class="normal"><a href="#__codelineno-0-673">673</a></span>
<span class="normal"><a href="#__codelineno-0-674">674</a></span>
<span class="normal"><a href="#__codelineno-0-675">675</a></span>
<span class="normal"><a href="#__codelineno-0-676">676</a></span>
<span class="normal"><a href="#__codelineno-0-677">677</a></span>
<span class="normal"><a href="#__codelineno-0-678">678</a></span>
<span class="normal"><a href="#__codelineno-0-679">679</a></span>
<span class="normal"><a href="#__codelineno-0-680">680</a></span>
<span class="normal"><a href="#__codelineno-0-681">681</a></span>
<span class="normal"><a href="#__codelineno-0-682">682</a></span>
<span class="normal"><a href="#__codelineno-0-683">683</a></span>
<span class="normal"><a href="#__codelineno-0-684">684</a></span>
<span class="normal"><a href="#__codelineno-0-685">685</a></span>
<span class="normal"><a href="#__codelineno-0-686">686</a></span>
<span class="normal"><a href="#__codelineno-0-687">687</a></span>
<span class="normal"><a href="#__codelineno-0-688">688</a></span>
<span class="normal"><a href="#__codelineno-0-689">689</a></span>
<span class="normal"><a href="#__codelineno-0-690">690</a></span>
<span class="normal"><a href="#__codelineno-0-691">691</a></span>
<span class="normal"><a href="#__codelineno-0-692">692</a></span>
<span class="normal"><a href="#__codelineno-0-693">693</a></span>
<span class="normal"><a href="#__codelineno-0-694">694</a></span>
<span class="normal"><a href="#__codelineno-0-695">695</a></span>
<span class="normal"><a href="#__codelineno-0-696">696</a></span>
<span class="normal"><a href="#__codelineno-0-697">697</a></span>
<span class="normal"><a href="#__codelineno-0-698">698</a></span>
<span class="normal"><a href="#__codelineno-0-699">699</a></span>
<span class="normal"><a href="#__codelineno-0-700">700</a></span>
<span class="normal"><a href="#__codelineno-0-701">701</a></span>
<span class="normal"><a href="#__codelineno-0-702">702</a></span>
<span class="normal"><a href="#__codelineno-0-703">703</a></span>
<span class="normal"><a href="#__codelineno-0-704">704</a></span>
<span class="normal"><a href="#__codelineno-0-705">705</a></span>
<span class="normal"><a href="#__codelineno-0-706">706</a></span>
<span class="normal"><a href="#__codelineno-0-707">707</a></span>
<span class="normal"><a href="#__codelineno-0-708">708</a></span>
<span class="normal"><a href="#__codelineno-0-709">709</a></span>
<span class="normal"><a href="#__codelineno-0-710">710</a></span>
<span class="normal"><a href="#__codelineno-0-711">711</a></span>
<span class="normal"><a href="#__codelineno-0-712">712</a></span>
<span class="normal"><a href="#__codelineno-0-713">713</a></span>
<span class="normal"><a href="#__codelineno-0-714">714</a></span>
<span class="normal"><a href="#__codelineno-0-715">715</a></span>
<span class="normal"><a href="#__codelineno-0-716">716</a></span>
<span class="normal"><a href="#__codelineno-0-717">717</a></span>
<span class="normal"><a href="#__codelineno-0-718">718</a></span>
<span class="normal"><a href="#__codelineno-0-719">719</a></span>
<span class="normal"><a href="#__codelineno-0-720">720</a></span>
<span class="normal"><a href="#__codelineno-0-721">721</a></span>
<span class="normal"><a href="#__codelineno-0-722">722</a></span>
<span class="normal"><a href="#__codelineno-0-723">723</a></span>
<span class="normal"><a href="#__codelineno-0-724">724</a></span>
<span class="normal"><a href="#__codelineno-0-725">725</a></span>
<span class="normal"><a href="#__codelineno-0-726">726</a></span>
<span class="normal"><a href="#__codelineno-0-727">727</a></span>
<span class="normal"><a href="#__codelineno-0-728">728</a></span>
<span class="normal"><a href="#__codelineno-0-729">729</a></span>
<span class="normal"><a href="#__codelineno-0-730">730</a></span>
<span class="normal"><a href="#__codelineno-0-731">731</a></span>
<span class="normal"><a href="#__codelineno-0-732">732</a></span>
<span class="normal"><a href="#__codelineno-0-733">733</a></span>
<span class="normal"><a href="#__codelineno-0-734">734</a></span>
<span class="normal"><a href="#__codelineno-0-735">735</a></span>
<span class="normal"><a href="#__codelineno-0-736">736</a></span>
<span class="normal"><a href="#__codelineno-0-737">737</a></span>
<span class="normal"><a href="#__codelineno-0-738">738</a></span>
<span class="normal"><a href="#__codelineno-0-739">739</a></span>
<span class="normal"><a href="#__codelineno-0-740">740</a></span>
<span class="normal"><a href="#__codelineno-0-741">741</a></span>
<span class="normal"><a href="#__codelineno-0-742">742</a></span>
<span class="normal"><a href="#__codelineno-0-743">743</a></span>
<span class="normal"><a href="#__codelineno-0-744">744</a></span>
<span class="normal"><a href="#__codelineno-0-745">745</a></span>
<span class="normal"><a href="#__codelineno-0-746">746</a></span>
<span class="normal"><a href="#__codelineno-0-747">747</a></span>
<span class="normal"><a href="#__codelineno-0-748">748</a></span>
<span class="normal"><a href="#__codelineno-0-749">749</a></span>
<span class="normal"><a href="#__codelineno-0-750">750</a></span>
<span class="normal"><a href="#__codelineno-0-751">751</a></span>
<span class="normal"><a href="#__codelineno-0-752">752</a></span>
<span class="normal"><a href="#__codelineno-0-753">753</a></span>
<span class="normal"><a href="#__codelineno-0-754">754</a></span>
<span class="normal"><a href="#__codelineno-0-755">755</a></span>
<span class="normal"><a href="#__codelineno-0-756">756</a></span>
<span class="normal"><a href="#__codelineno-0-757">757</a></span>
<span class="normal"><a href="#__codelineno-0-758">758</a></span>
<span class="normal"><a href="#__codelineno-0-759">759</a></span>
<span class="normal"><a href="#__codelineno-0-760">760</a></span>
<span class="normal"><a href="#__codelineno-0-761">761</a></span>
<span class="normal"><a href="#__codelineno-0-762">762</a></span>
<span class="normal"><a href="#__codelineno-0-763">763</a></span>
<span class="normal"><a href="#__codelineno-0-764">764</a></span>
<span class="normal"><a href="#__codelineno-0-765">765</a></span>
<span class="normal"><a href="#__codelineno-0-766">766</a></span>
<span class="normal"><a href="#__codelineno-0-767">767</a></span>
<span class="normal"><a href="#__codelineno-0-768">768</a></span>
<span class="normal"><a href="#__codelineno-0-769">769</a></span>
<span class="normal"><a href="#__codelineno-0-770">770</a></span>
<span class="normal"><a href="#__codelineno-0-771">771</a></span>
<span class="normal"><a href="#__codelineno-0-772">772</a></span>
<span class="normal"><a href="#__codelineno-0-773">773</a></span>
<span class="normal"><a href="#__codelineno-0-774">774</a></span>
<span class="normal"><a href="#__codelineno-0-775">775</a></span>
<span class="normal"><a href="#__codelineno-0-776">776</a></span>
<span class="normal"><a href="#__codelineno-0-777">777</a></span>
<span class="normal"><a href="#__codelineno-0-778">778</a></span>
<span class="normal"><a href="#__codelineno-0-779">779</a></span>
<span class="normal"><a href="#__codelineno-0-780">780</a></span>
<span class="normal"><a href="#__codelineno-0-781">781</a></span>
<span class="normal"><a href="#__codelineno-0-782">782</a></span>
<span class="normal"><a href="#__codelineno-0-783">783</a></span>
<span class="normal"><a href="#__codelineno-0-784">784</a></span>
<span class="normal"><a href="#__codelineno-0-785">785</a></span>
<span class="normal"><a href="#__codelineno-0-786">786</a></span>
<span class="normal"><a href="#__codelineno-0-787">787</a></span>
<span class="normal"><a href="#__codelineno-0-788">788</a></span>
<span class="normal"><a href="#__codelineno-0-789">789</a></span>
<span class="normal"><a href="#__codelineno-0-790">790</a></span>
<span class="normal"><a href="#__codelineno-0-791">791</a></span>
<span class="normal"><a href="#__codelineno-0-792">792</a></span>
<span class="normal"><a href="#__codelineno-0-793">793</a></span>
<span class="normal"><a href="#__codelineno-0-794">794</a></span>
<span class="normal"><a href="#__codelineno-0-795">795</a></span>
<span class="normal"><a href="#__codelineno-0-796">796</a></span>
<span class="normal"><a href="#__codelineno-0-797">797</a></span>
<span class="normal"><a href="#__codelineno-0-798">798</a></span>
<span class="normal"><a href="#__codelineno-0-799">799</a></span>
<span class="normal"><a href="#__codelineno-0-800">800</a></span>
<span class="normal"><a href="#__codelineno-0-801">801</a></span>
<span class="normal"><a href="#__codelineno-0-802">802</a></span>
<span class="normal"><a href="#__codelineno-0-803">803</a></span>
<span class="normal"><a href="#__codelineno-0-804">804</a></span>
<span class="normal"><a href="#__codelineno-0-805">805</a></span>
<span class="normal"><a href="#__codelineno-0-806">806</a></span>
<span class="normal"><a href="#__codelineno-0-807">807</a></span>
<span class="normal"><a href="#__codelineno-0-808">808</a></span>
<span class="normal"><a href="#__codelineno-0-809">809</a></span>
<span class="normal"><a href="#__codelineno-0-810">810</a></span>
<span class="normal"><a href="#__codelineno-0-811">811</a></span>
<span class="normal"><a href="#__codelineno-0-812">812</a></span>
<span class="normal"><a href="#__codelineno-0-813">813</a></span>
<span class="normal"><a href="#__codelineno-0-814">814</a></span>
<span class="normal"><a href="#__codelineno-0-815">815</a></span>
<span class="normal"><a href="#__codelineno-0-816">816</a></span>
<span class="normal"><a href="#__codelineno-0-817">817</a></span>
<span class="normal"><a href="#__codelineno-0-818">818</a></span>
<span class="normal"><a href="#__codelineno-0-819">819</a></span>
<span class="normal"><a href="#__codelineno-0-820">820</a></span>
<span class="normal"><a href="#__codelineno-0-821">821</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-613" name="__codelineno-0-613"></a><span class="k">class</span><span class="w"> </span><span class="nc">NumberLevelLoss</span><span class="p">(</span><span class="n">NTLossDotProduct</span><span class="p">):</span>
<a id="__codelineno-0-614" name="__codelineno-0-614"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Class to calculate NTL on a per-number (rather than per-token) basis.&quot;&quot;&quot;</span>
<a id="__codelineno-0-615" name="__codelineno-0-615"></a>
<a id="__codelineno-0-616" name="__codelineno-0-616"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-617" name="__codelineno-0-617"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-618" name="__codelineno-0-618"></a>        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-619" name="__codelineno-0-619"></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-620" name="__codelineno-0-620"></a>        <span class="n">float_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-621" name="__codelineno-0-621"></a>        <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-622" name="__codelineno-0-622"></a>    <span class="p">):</span>
<a id="__codelineno-0-623" name="__codelineno-0-623"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-624" name="__codelineno-0-624"></a><span class="sd">        NTL constructor for the number-level NTLoss.</span>
<a id="__codelineno-0-625" name="__codelineno-0-625"></a>
<a id="__codelineno-0-626" name="__codelineno-0-626"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-627" name="__codelineno-0-627"></a><span class="sd">            tokenizer: Any HuggingFace tokenizer.</span>
<a id="__codelineno-0-628" name="__codelineno-0-628"></a><span class="sd">            vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-629" name="__codelineno-0-629"></a><span class="sd">                tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-630" name="__codelineno-0-630"></a><span class="sd">            float_level: Whether to calculate the loss for every float or every</span>
<a id="__codelineno-0-631" name="__codelineno-0-631"></a><span class="sd">                integer in the sequence. For `12.34`, if float_level=False, two</span>
<a id="__codelineno-0-632" name="__codelineno-0-632"></a><span class="sd">                loss terms will be calculated, respectively for `12` and `34`.</span>
<a id="__codelineno-0-633" name="__codelineno-0-633"></a><span class="sd">                If float_level=True, a single `.` does not break the contiguity</span>
<a id="__codelineno-0-634" name="__codelineno-0-634"></a><span class="sd">                of the identified number. Defaults to False.</span>
<a id="__codelineno-0-635" name="__codelineno-0-635"></a><span class="sd">            reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-636" name="__codelineno-0-636"></a><span class="sd">                number tokens. Defaults to True.</span>
<a id="__codelineno-0-637" name="__codelineno-0-637"></a><span class="sd">                NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-638" name="__codelineno-0-638"></a><span class="sd">                incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-639" name="__codelineno-0-639"></a><span class="sd">                Using this will explode the NL-NTL in the current implementation,</span>
<a id="__codelineno-0-640" name="__codelineno-0-640"></a><span class="sd">                so reweighing for the NL-NTL needs to be refined.</span>
<a id="__codelineno-0-641" name="__codelineno-0-641"></a>
<a id="__codelineno-0-642" name="__codelineno-0-642"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-643" name="__codelineno-0-643"></a>        <span class="c1"># digit_level must be set to True.</span>
<a id="__codelineno-0-644" name="__codelineno-0-644"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-645" name="__codelineno-0-645"></a>            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<a id="__codelineno-0-646" name="__codelineno-0-646"></a>            <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-0-647" name="__codelineno-0-647"></a>            <span class="n">digit_level</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-648" name="__codelineno-0-648"></a>            <span class="n">reweigh</span><span class="o">=</span><span class="n">reweigh</span><span class="p">,</span>
<a id="__codelineno-0-649" name="__codelineno-0-649"></a>            <span class="n">loss_function</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">,</span>  <span class="c1"># unused</span>
<a id="__codelineno-0-650" name="__codelineno-0-650"></a>        <span class="p">)</span>
<a id="__codelineno-0-651" name="__codelineno-0-651"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">float_level</span> <span class="o">=</span> <span class="n">float_level</span>
<a id="__codelineno-0-652" name="__codelineno-0-652"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-653" name="__codelineno-0-653"></a>
<a id="__codelineno-0-654" name="__codelineno-0-654"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_max_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-655" name="__codelineno-0-655"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-656" name="__codelineno-0-656"></a><span class="sd">        Due to the MAPE loss calculation, the max dist is limited to 1.0</span>
<a id="__codelineno-0-657" name="__codelineno-0-657"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-658" name="__codelineno-0-658"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<a id="__codelineno-0-659" name="__codelineno-0-659"></a>
<a id="__codelineno-0-660" name="__codelineno-0-660"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">convert_digits_to_numbers</span><span class="p">(</span>
<a id="__codelineno-0-661" name="__codelineno-0-661"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-662" name="__codelineno-0-662"></a>        <span class="n">y</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-663" name="__codelineno-0-663"></a>        <span class="n">yhat</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-664" name="__codelineno-0-664"></a>        <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span>
<a id="__codelineno-0-665" name="__codelineno-0-665"></a>        <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-666" name="__codelineno-0-666"></a>    <span class="p">):</span>
<a id="__codelineno-0-667" name="__codelineno-0-667"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-668" name="__codelineno-0-668"></a><span class="sd">        Set up the order mask for the batch and convert digit-level number tokens to numerical values.</span>
<a id="__codelineno-0-669" name="__codelineno-0-669"></a>
<a id="__codelineno-0-670" name="__codelineno-0-670"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-671" name="__codelineno-0-671"></a><span class="sd">            y: 2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).</span>
<a id="__codelineno-0-672" name="__codelineno-0-672"></a><span class="sd">            yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level</span>
<a id="__codelineno-0-673" name="__codelineno-0-673"></a><span class="sd">                (includes predictions for non-number tokens).</span>
<a id="__codelineno-0-674" name="__codelineno-0-674"></a><span class="sd">            number_token_positions: 2D BoolTensor (BS x T) containing locations of number tokens at digit-level.</span>
<a id="__codelineno-0-675" name="__codelineno-0-675"></a><span class="sd">            labels: 2D LongTensor of shape BS x T with the target input IDs.</span>
<a id="__codelineno-0-676" name="__codelineno-0-676"></a>
<a id="__codelineno-0-677" name="__codelineno-0-677"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-678" name="__codelineno-0-678"></a><span class="sd">            y: 2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).</span>
<a id="__codelineno-0-679" name="__codelineno-0-679"></a><span class="sd">            yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level</span>
<a id="__codelineno-0-680" name="__codelineno-0-680"></a><span class="sd">                (includes predictions for non-number tokens).</span>
<a id="__codelineno-0-681" name="__codelineno-0-681"></a><span class="sd">            number_token_positions: 2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.</span>
<a id="__codelineno-0-682" name="__codelineno-0-682"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-683" name="__codelineno-0-683"></a>
<a id="__codelineno-0-684" name="__codelineno-0-684"></a>        <span class="c1"># Set up empty order_mask: will store power with which to scale digits</span>
<a id="__codelineno-0-685" name="__codelineno-0-685"></a>        <span class="n">order_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">yhat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-686" name="__codelineno-0-686"></a>
<a id="__codelineno-0-687" name="__codelineno-0-687"></a>        <span class="c1"># Extract numbers using number blocks</span>
<a id="__codelineno-0-688" name="__codelineno-0-688"></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<a id="__codelineno-0-689" name="__codelineno-0-689"></a>            <span class="c1"># For every item in batch: assume not starting with number block</span>
<a id="__codelineno-0-690" name="__codelineno-0-690"></a>            <span class="n">in_number_block</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-691" name="__codelineno-0-691"></a>            <span class="n">end_digit</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-692" name="__codelineno-0-692"></a>
<a id="__codelineno-0-693" name="__codelineno-0-693"></a>            <span class="c1"># Loop from end of sequence to beginning to extract numbers</span>
<a id="__codelineno-0-694" name="__codelineno-0-694"></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<a id="__codelineno-0-695" name="__codelineno-0-695"></a>                <span class="c1"># Already in number block and a digit: increase order magnitude</span>
<a id="__codelineno-0-696" name="__codelineno-0-696"></a>                <span class="k">if</span> <span class="n">in_number_block</span> <span class="ow">and</span> <span class="n">number_token_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]:</span>
<a id="__codelineno-0-697" name="__codelineno-0-697"></a>                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">float_level</span> <span class="ow">or</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span><span class="p">:</span>
<a id="__codelineno-0-698" name="__codelineno-0-698"></a>                        <span class="n">previous_order_index</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span>
<a id="__codelineno-0-699" name="__codelineno-0-699"></a>                    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-700" name="__codelineno-0-700"></a>                        <span class="n">previous_order_index</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">2</span>
<a id="__codelineno-0-701" name="__codelineno-0-701"></a>                    <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_order_index</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
<a id="__codelineno-0-702" name="__codelineno-0-702"></a>
<a id="__codelineno-0-703" name="__codelineno-0-703"></a>                <span class="c1"># Not in number block: first instance of number = end digit</span>
<a id="__codelineno-0-704" name="__codelineno-0-704"></a>                <span class="k">elif</span> <span class="n">number_token_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]:</span>
<a id="__codelineno-0-705" name="__codelineno-0-705"></a>                    <span class="n">in_number_block</span> <span class="o">=</span> <span class="kc">True</span>
<a id="__codelineno-0-706" name="__codelineno-0-706"></a>                    <span class="n">end_digit</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span>
<a id="__codelineno-0-707" name="__codelineno-0-707"></a>
<a id="__codelineno-0-708" name="__codelineno-0-708"></a>                <span class="c1"># A dot can be considered part of a number if self.float_level</span>
<a id="__codelineno-0-709" name="__codelineno-0-709"></a>                <span class="k">elif</span> <span class="p">(</span>
<a id="__codelineno-0-710" name="__codelineno-0-710"></a>                    <span class="n">in_number_block</span>
<a id="__codelineno-0-711" name="__codelineno-0-711"></a>                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">float_level</span>
<a id="__codelineno-0-712" name="__codelineno-0-712"></a>                    <span class="ow">and</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span>
<a id="__codelineno-0-713" name="__codelineno-0-713"></a>                    <span class="ow">and</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span>
<a id="__codelineno-0-714" name="__codelineno-0-714"></a>                <span class="p">):</span>
<a id="__codelineno-0-715" name="__codelineno-0-715"></a>                    <span class="c1"># exp(-inf) = 0, thus, the dot does not contribute to the GT number calculation</span>
<a id="__codelineno-0-716" name="__codelineno-0-716"></a>                    <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
<a id="__codelineno-0-717" name="__codelineno-0-717"></a>                    <span class="c1"># Necessary to avoid having NaN when summing</span>
<a id="__codelineno-0-718" name="__codelineno-0-718"></a>                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-719" name="__codelineno-0-719"></a>                    <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-720" name="__codelineno-0-720"></a>
<a id="__codelineno-0-721" name="__codelineno-0-721"></a>                <span class="c1"># In number block, but not a digit: end of number_block</span>
<a id="__codelineno-0-722" name="__codelineno-0-722"></a>                <span class="k">elif</span> <span class="n">in_number_block</span><span class="p">:</span>
<a id="__codelineno-0-723" name="__codelineno-0-723"></a>                    <span class="n">in_number_block</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-724" name="__codelineno-0-724"></a>
<a id="__codelineno-0-725" name="__codelineno-0-725"></a>                    <span class="c1"># Reuse y and yhat tensors to store full numbers</span>
<a id="__codelineno-0-726" name="__codelineno-0-726"></a>                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-727" name="__codelineno-0-727"></a>                        <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">]</span>
<a id="__codelineno-0-728" name="__codelineno-0-728"></a>                        <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">])</span>
<a id="__codelineno-0-729" name="__codelineno-0-729"></a>                    <span class="p">)</span>
<a id="__codelineno-0-730" name="__codelineno-0-730"></a>                    <span class="c1"># Make sure non-relevant numerical values are turned into NaN</span>
<a id="__codelineno-0-731" name="__codelineno-0-731"></a>                    <span class="c1"># This indicates non-number tokens</span>
<a id="__codelineno-0-732" name="__codelineno-0-732"></a>                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">2</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
<a id="__codelineno-0-733" name="__codelineno-0-733"></a>                    <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-734" name="__codelineno-0-734"></a>                        <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">]</span>
<a id="__codelineno-0-735" name="__codelineno-0-735"></a>                        <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">])</span>
<a id="__codelineno-0-736" name="__codelineno-0-736"></a>                    <span class="p">)</span>
<a id="__codelineno-0-737" name="__codelineno-0-737"></a>
<a id="__codelineno-0-738" name="__codelineno-0-738"></a>        <span class="c1"># Update mask with locations of number tokens</span>
<a id="__codelineno-0-739" name="__codelineno-0-739"></a>        <span class="n">number_token_positions</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<a id="__codelineno-0-740" name="__codelineno-0-740"></a>
<a id="__codelineno-0-741" name="__codelineno-0-741"></a>        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">number_token_positions</span>
<a id="__codelineno-0-742" name="__codelineno-0-742"></a>
<a id="__codelineno-0-743" name="__codelineno-0-743"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-744" name="__codelineno-0-744"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-745" name="__codelineno-0-745"></a>        <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-746" name="__codelineno-0-746"></a>        <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-747" name="__codelineno-0-747"></a>        <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-748" name="__codelineno-0-748"></a>        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-749" name="__codelineno-0-749"></a>        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-750" name="__codelineno-0-750"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-751" name="__codelineno-0-751"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-752" name="__codelineno-0-752"></a><span class="sd">        Computes the NTL based on the dot product between token values and their probs.</span>
<a id="__codelineno-0-753" name="__codelineno-0-753"></a>
<a id="__codelineno-0-754" name="__codelineno-0-754"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-755" name="__codelineno-0-755"></a><span class="sd">            logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-756" name="__codelineno-0-756"></a><span class="sd">            labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-757" name="__codelineno-0-757"></a><span class="sd">            loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.</span>
<a id="__codelineno-0-758" name="__codelineno-0-758"></a><span class="sd">            reduction: Optional string specifying the reduction to apply to the</span>
<a id="__codelineno-0-759" name="__codelineno-0-759"></a><span class="sd">                output. Defaults to &quot;mean&quot;, options are &quot;mean&quot;, &quot;sum&quot;, &quot;none&quot;.</span>
<a id="__codelineno-0-760" name="__codelineno-0-760"></a><span class="sd">            ignore_index: The token ID to ignore in the labels. Defaults to -100.</span>
<a id="__codelineno-0-761" name="__codelineno-0-761"></a>
<a id="__codelineno-0-762" name="__codelineno-0-762"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-763" name="__codelineno-0-763"></a><span class="sd">            Loss tensor</span>
<a id="__codelineno-0-764" name="__codelineno-0-764"></a><span class="sd">                0-D if reduction==&quot;mean&quot;|&quot;sum&quot;</span>
<a id="__codelineno-0-765" name="__codelineno-0-765"></a><span class="sd">                BS x T if reduction==&quot;none&quot;</span>
<a id="__codelineno-0-766" name="__codelineno-0-766"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-767" name="__codelineno-0-767"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
<a id="__codelineno-0-768" name="__codelineno-0-768"></a>
<a id="__codelineno-0-769" name="__codelineno-0-769"></a>        <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_number_token_targets</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
<a id="__codelineno-0-770" name="__codelineno-0-770"></a>        <span class="n">number_token_positions</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<a id="__codelineno-0-771" name="__codelineno-0-771"></a>
<a id="__codelineno-0-772" name="__codelineno-0-772"></a>        <span class="c1"># If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations</span>
<a id="__codelineno-0-773" name="__codelineno-0-773"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">number_token_positions</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span>
<a id="__codelineno-0-774" name="__codelineno-0-774"></a>            <span class="n">loss_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
<a id="__codelineno-0-775" name="__codelineno-0-775"></a>        <span class="p">):</span>
<a id="__codelineno-0-776" name="__codelineno-0-776"></a>            <span class="k">if</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">):</span>
<a id="__codelineno-0-777" name="__codelineno-0-777"></a>                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-778" name="__codelineno-0-778"></a>            <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-779" name="__codelineno-0-779"></a>                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-780" name="__codelineno-0-780"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-781" name="__codelineno-0-781"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-782" name="__codelineno-0-782"></a>
<a id="__codelineno-0-783" name="__codelineno-0-783"></a>            <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-784" name="__codelineno-0-784"></a>
<a id="__codelineno-0-785" name="__codelineno-0-785"></a>        <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dot_product</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<a id="__codelineno-0-786" name="__codelineno-0-786"></a>
<a id="__codelineno-0-787" name="__codelineno-0-787"></a>        <span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">number_token_positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_digits_to_numbers</span><span class="p">(</span>
<a id="__codelineno-0-788" name="__codelineno-0-788"></a>            <span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="p">,</span> <span class="n">labels</span>
<a id="__codelineno-0-789" name="__codelineno-0-789"></a>        <span class="p">)</span>
<a id="__codelineno-0-790" name="__codelineno-0-790"></a>        <span class="k">if</span> <span class="n">loss_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-791" name="__codelineno-0-791"></a>            <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-792" name="__codelineno-0-792"></a>        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-793" name="__codelineno-0-793"></a>
<a id="__codelineno-0-794" name="__codelineno-0-794"></a>        <span class="c1"># NOTE: Alternative could be to apply specified loss function to normalized yhat</span>
<a id="__codelineno-0-795" name="__codelineno-0-795"></a>        <span class="c1"># loss = self.loss_function(torch.div(</span>
<a id="__codelineno-0-796" name="__codelineno-0-796"></a>        <span class="c1">#     yhat[number_token_positions],</span>
<a id="__codelineno-0-797" name="__codelineno-0-797"></a>        <span class="c1">#     y[number_token_positions].clamp_min(torch.finfo(y.dtype).eps),</span>
<a id="__codelineno-0-798" name="__codelineno-0-798"></a>        <span class="c1"># ), torch.ones_like(yhat), reduction=&quot;none&quot;)</span>
<a id="__codelineno-0-799" name="__codelineno-0-799"></a>
<a id="__codelineno-0-800" name="__codelineno-0-800"></a>        <span class="n">y_num</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-801" name="__codelineno-0-801"></a>        <span class="n">yh_num</span> <span class="o">=</span> <span class="n">yhat</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-802" name="__codelineno-0-802"></a>        <span class="c1"># Calculate symmetric MAPE which is bounded in [0, 1]</span>
<a id="__codelineno-0-803" name="__codelineno-0-803"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">yh_num</span> <span class="o">-</span> <span class="n">y_num</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span>
<a id="__codelineno-0-804" name="__codelineno-0-804"></a>            <span class="n">yh_num</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="n">y_num</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
<a id="__codelineno-0-805" name="__codelineno-0-805"></a>        <span class="p">)</span>
<a id="__codelineno-0-806" name="__codelineno-0-806"></a>
<a id="__codelineno-0-807" name="__codelineno-0-807"></a>        <span class="c1"># If reweigh: compute weights for NTL based on logits</span>
<a id="__codelineno-0-808" name="__codelineno-0-808"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span><span class="p">:</span>
<a id="__codelineno-0-809" name="__codelineno-0-809"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-810" name="__codelineno-0-810"></a>                <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-811" name="__codelineno-0-811"></a>            <span class="p">)</span>
<a id="__codelineno-0-812" name="__codelineno-0-812"></a>
<a id="__codelineno-0-813" name="__codelineno-0-813"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-814" name="__codelineno-0-814"></a>            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-815" name="__codelineno-0-815"></a>            <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
<a id="__codelineno-0-816" name="__codelineno-0-816"></a>            <span class="n">loss_weights</span><span class="o">=</span><span class="n">loss_weights</span><span class="p">,</span>
<a id="__codelineno-0-817" name="__codelineno-0-817"></a>            <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span><span class="p">,</span>
<a id="__codelineno-0-818" name="__codelineno-0-818"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
<a id="__codelineno-0-819" name="__codelineno-0-819"></a>        <span class="p">)</span>
<a id="__codelineno-0-820" name="__codelineno-0-820"></a>
<a id="__codelineno-0-821" name="__codelineno-0-821"></a>        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NumberLevelLoss.__init__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">float_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

<a href="#ntloss.core.NumberLevelLoss.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>NTL constructor for the number-level NTLoss.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td>
                  <code><span title="transformers.PreTrainedTokenizer">PreTrainedTokenizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Any HuggingFace tokenizer.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional user-provided vocab size. If not provided, the
tokenizer's vocab size is used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>float_level</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to calculate the loss for every float or every
integer in the sequence. For <code>12.34</code>, if float_level=False, two
loss terms will be calculated, respectively for <code>12</code> and <code>34</code>.
If float_level=True, a single <code>.</code> does not break the contiguity
of the identified number. Defaults to False.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reweigh</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to scale the NTL using the logit weight on
number tokens. Defaults to True.
NOTE: The ICML paper does <em>not</em> use this option which can lead to
incorrect loss if most mass is placed outside of the number tokens.
Using this will explode the NL-NTL in the current implementation,
so reweighing for the NL-NTL needs to be refined.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-616">616</a></span>
<span class="normal"><a href="#__codelineno-0-617">617</a></span>
<span class="normal"><a href="#__codelineno-0-618">618</a></span>
<span class="normal"><a href="#__codelineno-0-619">619</a></span>
<span class="normal"><a href="#__codelineno-0-620">620</a></span>
<span class="normal"><a href="#__codelineno-0-621">621</a></span>
<span class="normal"><a href="#__codelineno-0-622">622</a></span>
<span class="normal"><a href="#__codelineno-0-623">623</a></span>
<span class="normal"><a href="#__codelineno-0-624">624</a></span>
<span class="normal"><a href="#__codelineno-0-625">625</a></span>
<span class="normal"><a href="#__codelineno-0-626">626</a></span>
<span class="normal"><a href="#__codelineno-0-627">627</a></span>
<span class="normal"><a href="#__codelineno-0-628">628</a></span>
<span class="normal"><a href="#__codelineno-0-629">629</a></span>
<span class="normal"><a href="#__codelineno-0-630">630</a></span>
<span class="normal"><a href="#__codelineno-0-631">631</a></span>
<span class="normal"><a href="#__codelineno-0-632">632</a></span>
<span class="normal"><a href="#__codelineno-0-633">633</a></span>
<span class="normal"><a href="#__codelineno-0-634">634</a></span>
<span class="normal"><a href="#__codelineno-0-635">635</a></span>
<span class="normal"><a href="#__codelineno-0-636">636</a></span>
<span class="normal"><a href="#__codelineno-0-637">637</a></span>
<span class="normal"><a href="#__codelineno-0-638">638</a></span>
<span class="normal"><a href="#__codelineno-0-639">639</a></span>
<span class="normal"><a href="#__codelineno-0-640">640</a></span>
<span class="normal"><a href="#__codelineno-0-641">641</a></span>
<span class="normal"><a href="#__codelineno-0-642">642</a></span>
<span class="normal"><a href="#__codelineno-0-643">643</a></span>
<span class="normal"><a href="#__codelineno-0-644">644</a></span>
<span class="normal"><a href="#__codelineno-0-645">645</a></span>
<span class="normal"><a href="#__codelineno-0-646">646</a></span>
<span class="normal"><a href="#__codelineno-0-647">647</a></span>
<span class="normal"><a href="#__codelineno-0-648">648</a></span>
<span class="normal"><a href="#__codelineno-0-649">649</a></span>
<span class="normal"><a href="#__codelineno-0-650">650</a></span>
<span class="normal"><a href="#__codelineno-0-651">651</a></span>
<span class="normal"><a href="#__codelineno-0-652">652</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-616" name="__codelineno-0-616"></a><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-617" name="__codelineno-0-617"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-618" name="__codelineno-0-618"></a>    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
<a id="__codelineno-0-619" name="__codelineno-0-619"></a>    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-620" name="__codelineno-0-620"></a>    <span class="n">float_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-621" name="__codelineno-0-621"></a>    <span class="n">reweigh</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-622" name="__codelineno-0-622"></a><span class="p">):</span>
<a id="__codelineno-0-623" name="__codelineno-0-623"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-624" name="__codelineno-0-624"></a><span class="sd">    NTL constructor for the number-level NTLoss.</span>
<a id="__codelineno-0-625" name="__codelineno-0-625"></a>
<a id="__codelineno-0-626" name="__codelineno-0-626"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-627" name="__codelineno-0-627"></a><span class="sd">        tokenizer: Any HuggingFace tokenizer.</span>
<a id="__codelineno-0-628" name="__codelineno-0-628"></a><span class="sd">        vocab_size: Optional user-provided vocab size. If not provided, the</span>
<a id="__codelineno-0-629" name="__codelineno-0-629"></a><span class="sd">            tokenizer&#39;s vocab size is used.</span>
<a id="__codelineno-0-630" name="__codelineno-0-630"></a><span class="sd">        float_level: Whether to calculate the loss for every float or every</span>
<a id="__codelineno-0-631" name="__codelineno-0-631"></a><span class="sd">            integer in the sequence. For `12.34`, if float_level=False, two</span>
<a id="__codelineno-0-632" name="__codelineno-0-632"></a><span class="sd">            loss terms will be calculated, respectively for `12` and `34`.</span>
<a id="__codelineno-0-633" name="__codelineno-0-633"></a><span class="sd">            If float_level=True, a single `.` does not break the contiguity</span>
<a id="__codelineno-0-634" name="__codelineno-0-634"></a><span class="sd">            of the identified number. Defaults to False.</span>
<a id="__codelineno-0-635" name="__codelineno-0-635"></a><span class="sd">        reweigh: Whether to scale the NTL using the logit weight on</span>
<a id="__codelineno-0-636" name="__codelineno-0-636"></a><span class="sd">            number tokens. Defaults to True.</span>
<a id="__codelineno-0-637" name="__codelineno-0-637"></a><span class="sd">            NOTE: The ICML paper does *not* use this option which can lead to</span>
<a id="__codelineno-0-638" name="__codelineno-0-638"></a><span class="sd">            incorrect loss if most mass is placed outside of the number tokens.</span>
<a id="__codelineno-0-639" name="__codelineno-0-639"></a><span class="sd">            Using this will explode the NL-NTL in the current implementation,</span>
<a id="__codelineno-0-640" name="__codelineno-0-640"></a><span class="sd">            so reweighing for the NL-NTL needs to be refined.</span>
<a id="__codelineno-0-641" name="__codelineno-0-641"></a>
<a id="__codelineno-0-642" name="__codelineno-0-642"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-643" name="__codelineno-0-643"></a>    <span class="c1"># digit_level must be set to True.</span>
<a id="__codelineno-0-644" name="__codelineno-0-644"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-645" name="__codelineno-0-645"></a>        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<a id="__codelineno-0-646" name="__codelineno-0-646"></a>        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
<a id="__codelineno-0-647" name="__codelineno-0-647"></a>        <span class="n">digit_level</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-648" name="__codelineno-0-648"></a>        <span class="n">reweigh</span><span class="o">=</span><span class="n">reweigh</span><span class="p">,</span>
<a id="__codelineno-0-649" name="__codelineno-0-649"></a>        <span class="n">loss_function</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">,</span>  <span class="c1"># unused</span>
<a id="__codelineno-0-650" name="__codelineno-0-650"></a>    <span class="p">)</span>
<a id="__codelineno-0-651" name="__codelineno-0-651"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">float_level</span> <span class="o">=</span> <span class="n">float_level</span>
<a id="__codelineno-0-652" name="__codelineno-0-652"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">dot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NumberLevelLoss.setup_max_dist" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">setup_max_dist</span><span class="p">()</span></code>

<a href="#ntloss.core.NumberLevelLoss.setup_max_dist" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Due to the MAPE loss calculation, the max dist is limited to 1.0</p>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-654">654</a></span>
<span class="normal"><a href="#__codelineno-0-655">655</a></span>
<span class="normal"><a href="#__codelineno-0-656">656</a></span>
<span class="normal"><a href="#__codelineno-0-657">657</a></span>
<span class="normal"><a href="#__codelineno-0-658">658</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-654" name="__codelineno-0-654"></a><span class="k">def</span><span class="w"> </span><span class="nf">setup_max_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-655" name="__codelineno-0-655"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-656" name="__codelineno-0-656"></a><span class="sd">    Due to the MAPE loss calculation, the max dist is limited to 1.0</span>
<a id="__codelineno-0-657" name="__codelineno-0-657"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-658" name="__codelineno-0-658"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">max_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NumberLevelLoss.convert_digits_to_numbers" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">convert_digits_to_numbers</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">yhat</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">)</span></code>

<a href="#ntloss.core.NumberLevelLoss.convert_digits_to_numbers" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Set up the order mask for the batch and convert digit-level number tokens to numerical values.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>y</code>
            </td>
            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>yhat</code>
            </td>
            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level
(includes predictions for non-number tokens).</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>number_token_positions</code>
            </td>
            <td>
                  <code><span title="torch.BoolTensor">BoolTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D BoolTensor (BS x T) containing locations of number tokens at digit-level.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td>
                  <code><span title="torch.LongTensor">LongTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D LongTensor of shape BS x T with the target input IDs.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>y</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>yhat</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level
(includes predictions for non-number tokens).</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>number_token_positions</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-660">660</a></span>
<span class="normal"><a href="#__codelineno-0-661">661</a></span>
<span class="normal"><a href="#__codelineno-0-662">662</a></span>
<span class="normal"><a href="#__codelineno-0-663">663</a></span>
<span class="normal"><a href="#__codelineno-0-664">664</a></span>
<span class="normal"><a href="#__codelineno-0-665">665</a></span>
<span class="normal"><a href="#__codelineno-0-666">666</a></span>
<span class="normal"><a href="#__codelineno-0-667">667</a></span>
<span class="normal"><a href="#__codelineno-0-668">668</a></span>
<span class="normal"><a href="#__codelineno-0-669">669</a></span>
<span class="normal"><a href="#__codelineno-0-670">670</a></span>
<span class="normal"><a href="#__codelineno-0-671">671</a></span>
<span class="normal"><a href="#__codelineno-0-672">672</a></span>
<span class="normal"><a href="#__codelineno-0-673">673</a></span>
<span class="normal"><a href="#__codelineno-0-674">674</a></span>
<span class="normal"><a href="#__codelineno-0-675">675</a></span>
<span class="normal"><a href="#__codelineno-0-676">676</a></span>
<span class="normal"><a href="#__codelineno-0-677">677</a></span>
<span class="normal"><a href="#__codelineno-0-678">678</a></span>
<span class="normal"><a href="#__codelineno-0-679">679</a></span>
<span class="normal"><a href="#__codelineno-0-680">680</a></span>
<span class="normal"><a href="#__codelineno-0-681">681</a></span>
<span class="normal"><a href="#__codelineno-0-682">682</a></span>
<span class="normal"><a href="#__codelineno-0-683">683</a></span>
<span class="normal"><a href="#__codelineno-0-684">684</a></span>
<span class="normal"><a href="#__codelineno-0-685">685</a></span>
<span class="normal"><a href="#__codelineno-0-686">686</a></span>
<span class="normal"><a href="#__codelineno-0-687">687</a></span>
<span class="normal"><a href="#__codelineno-0-688">688</a></span>
<span class="normal"><a href="#__codelineno-0-689">689</a></span>
<span class="normal"><a href="#__codelineno-0-690">690</a></span>
<span class="normal"><a href="#__codelineno-0-691">691</a></span>
<span class="normal"><a href="#__codelineno-0-692">692</a></span>
<span class="normal"><a href="#__codelineno-0-693">693</a></span>
<span class="normal"><a href="#__codelineno-0-694">694</a></span>
<span class="normal"><a href="#__codelineno-0-695">695</a></span>
<span class="normal"><a href="#__codelineno-0-696">696</a></span>
<span class="normal"><a href="#__codelineno-0-697">697</a></span>
<span class="normal"><a href="#__codelineno-0-698">698</a></span>
<span class="normal"><a href="#__codelineno-0-699">699</a></span>
<span class="normal"><a href="#__codelineno-0-700">700</a></span>
<span class="normal"><a href="#__codelineno-0-701">701</a></span>
<span class="normal"><a href="#__codelineno-0-702">702</a></span>
<span class="normal"><a href="#__codelineno-0-703">703</a></span>
<span class="normal"><a href="#__codelineno-0-704">704</a></span>
<span class="normal"><a href="#__codelineno-0-705">705</a></span>
<span class="normal"><a href="#__codelineno-0-706">706</a></span>
<span class="normal"><a href="#__codelineno-0-707">707</a></span>
<span class="normal"><a href="#__codelineno-0-708">708</a></span>
<span class="normal"><a href="#__codelineno-0-709">709</a></span>
<span class="normal"><a href="#__codelineno-0-710">710</a></span>
<span class="normal"><a href="#__codelineno-0-711">711</a></span>
<span class="normal"><a href="#__codelineno-0-712">712</a></span>
<span class="normal"><a href="#__codelineno-0-713">713</a></span>
<span class="normal"><a href="#__codelineno-0-714">714</a></span>
<span class="normal"><a href="#__codelineno-0-715">715</a></span>
<span class="normal"><a href="#__codelineno-0-716">716</a></span>
<span class="normal"><a href="#__codelineno-0-717">717</a></span>
<span class="normal"><a href="#__codelineno-0-718">718</a></span>
<span class="normal"><a href="#__codelineno-0-719">719</a></span>
<span class="normal"><a href="#__codelineno-0-720">720</a></span>
<span class="normal"><a href="#__codelineno-0-721">721</a></span>
<span class="normal"><a href="#__codelineno-0-722">722</a></span>
<span class="normal"><a href="#__codelineno-0-723">723</a></span>
<span class="normal"><a href="#__codelineno-0-724">724</a></span>
<span class="normal"><a href="#__codelineno-0-725">725</a></span>
<span class="normal"><a href="#__codelineno-0-726">726</a></span>
<span class="normal"><a href="#__codelineno-0-727">727</a></span>
<span class="normal"><a href="#__codelineno-0-728">728</a></span>
<span class="normal"><a href="#__codelineno-0-729">729</a></span>
<span class="normal"><a href="#__codelineno-0-730">730</a></span>
<span class="normal"><a href="#__codelineno-0-731">731</a></span>
<span class="normal"><a href="#__codelineno-0-732">732</a></span>
<span class="normal"><a href="#__codelineno-0-733">733</a></span>
<span class="normal"><a href="#__codelineno-0-734">734</a></span>
<span class="normal"><a href="#__codelineno-0-735">735</a></span>
<span class="normal"><a href="#__codelineno-0-736">736</a></span>
<span class="normal"><a href="#__codelineno-0-737">737</a></span>
<span class="normal"><a href="#__codelineno-0-738">738</a></span>
<span class="normal"><a href="#__codelineno-0-739">739</a></span>
<span class="normal"><a href="#__codelineno-0-740">740</a></span>
<span class="normal"><a href="#__codelineno-0-741">741</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-660" name="__codelineno-0-660"></a><span class="k">def</span><span class="w"> </span><span class="nf">convert_digits_to_numbers</span><span class="p">(</span>
<a id="__codelineno-0-661" name="__codelineno-0-661"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-662" name="__codelineno-0-662"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-663" name="__codelineno-0-663"></a>    <span class="n">yhat</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-664" name="__codelineno-0-664"></a>    <span class="n">number_token_positions</span><span class="p">:</span> <span class="n">BoolTensor</span><span class="p">,</span>
<a id="__codelineno-0-665" name="__codelineno-0-665"></a>    <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-666" name="__codelineno-0-666"></a><span class="p">):</span>
<a id="__codelineno-0-667" name="__codelineno-0-667"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-668" name="__codelineno-0-668"></a><span class="sd">    Set up the order mask for the batch and convert digit-level number tokens to numerical values.</span>
<a id="__codelineno-0-669" name="__codelineno-0-669"></a>
<a id="__codelineno-0-670" name="__codelineno-0-670"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-671" name="__codelineno-0-671"></a><span class="sd">        y: 2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).</span>
<a id="__codelineno-0-672" name="__codelineno-0-672"></a><span class="sd">        yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level</span>
<a id="__codelineno-0-673" name="__codelineno-0-673"></a><span class="sd">            (includes predictions for non-number tokens).</span>
<a id="__codelineno-0-674" name="__codelineno-0-674"></a><span class="sd">        number_token_positions: 2D BoolTensor (BS x T) containing locations of number tokens at digit-level.</span>
<a id="__codelineno-0-675" name="__codelineno-0-675"></a><span class="sd">        labels: 2D LongTensor of shape BS x T with the target input IDs.</span>
<a id="__codelineno-0-676" name="__codelineno-0-676"></a>
<a id="__codelineno-0-677" name="__codelineno-0-677"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-678" name="__codelineno-0-678"></a><span class="sd">        y: 2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).</span>
<a id="__codelineno-0-679" name="__codelineno-0-679"></a><span class="sd">        yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level</span>
<a id="__codelineno-0-680" name="__codelineno-0-680"></a><span class="sd">            (includes predictions for non-number tokens).</span>
<a id="__codelineno-0-681" name="__codelineno-0-681"></a><span class="sd">        number_token_positions: 2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.</span>
<a id="__codelineno-0-682" name="__codelineno-0-682"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-683" name="__codelineno-0-683"></a>
<a id="__codelineno-0-684" name="__codelineno-0-684"></a>    <span class="c1"># Set up empty order_mask: will store power with which to scale digits</span>
<a id="__codelineno-0-685" name="__codelineno-0-685"></a>    <span class="n">order_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">yhat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-686" name="__codelineno-0-686"></a>
<a id="__codelineno-0-687" name="__codelineno-0-687"></a>    <span class="c1"># Extract numbers using number blocks</span>
<a id="__codelineno-0-688" name="__codelineno-0-688"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<a id="__codelineno-0-689" name="__codelineno-0-689"></a>        <span class="c1"># For every item in batch: assume not starting with number block</span>
<a id="__codelineno-0-690" name="__codelineno-0-690"></a>        <span class="n">in_number_block</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-691" name="__codelineno-0-691"></a>        <span class="n">end_digit</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-692" name="__codelineno-0-692"></a>
<a id="__codelineno-0-693" name="__codelineno-0-693"></a>        <span class="c1"># Loop from end of sequence to beginning to extract numbers</span>
<a id="__codelineno-0-694" name="__codelineno-0-694"></a>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<a id="__codelineno-0-695" name="__codelineno-0-695"></a>            <span class="c1"># Already in number block and a digit: increase order magnitude</span>
<a id="__codelineno-0-696" name="__codelineno-0-696"></a>            <span class="k">if</span> <span class="n">in_number_block</span> <span class="ow">and</span> <span class="n">number_token_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]:</span>
<a id="__codelineno-0-697" name="__codelineno-0-697"></a>                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">float_level</span> <span class="ow">or</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span><span class="p">:</span>
<a id="__codelineno-0-698" name="__codelineno-0-698"></a>                    <span class="n">previous_order_index</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span>
<a id="__codelineno-0-699" name="__codelineno-0-699"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-700" name="__codelineno-0-700"></a>                    <span class="n">previous_order_index</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">2</span>
<a id="__codelineno-0-701" name="__codelineno-0-701"></a>                <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_order_index</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
<a id="__codelineno-0-702" name="__codelineno-0-702"></a>
<a id="__codelineno-0-703" name="__codelineno-0-703"></a>            <span class="c1"># Not in number block: first instance of number = end digit</span>
<a id="__codelineno-0-704" name="__codelineno-0-704"></a>            <span class="k">elif</span> <span class="n">number_token_positions</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]:</span>
<a id="__codelineno-0-705" name="__codelineno-0-705"></a>                <span class="n">in_number_block</span> <span class="o">=</span> <span class="kc">True</span>
<a id="__codelineno-0-706" name="__codelineno-0-706"></a>                <span class="n">end_digit</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span>
<a id="__codelineno-0-707" name="__codelineno-0-707"></a>
<a id="__codelineno-0-708" name="__codelineno-0-708"></a>            <span class="c1"># A dot can be considered part of a number if self.float_level</span>
<a id="__codelineno-0-709" name="__codelineno-0-709"></a>            <span class="k">elif</span> <span class="p">(</span>
<a id="__codelineno-0-710" name="__codelineno-0-710"></a>                <span class="n">in_number_block</span>
<a id="__codelineno-0-711" name="__codelineno-0-711"></a>                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">float_level</span>
<a id="__codelineno-0-712" name="__codelineno-0-712"></a>                <span class="ow">and</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span>
<a id="__codelineno-0-713" name="__codelineno-0-713"></a>                <span class="ow">and</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span>
<a id="__codelineno-0-714" name="__codelineno-0-714"></a>            <span class="p">):</span>
<a id="__codelineno-0-715" name="__codelineno-0-715"></a>                <span class="c1"># exp(-inf) = 0, thus, the dot does not contribute to the GT number calculation</span>
<a id="__codelineno-0-716" name="__codelineno-0-716"></a>                <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
<a id="__codelineno-0-717" name="__codelineno-0-717"></a>                <span class="c1"># Necessary to avoid having NaN when summing</span>
<a id="__codelineno-0-718" name="__codelineno-0-718"></a>                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-719" name="__codelineno-0-719"></a>                <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-720" name="__codelineno-0-720"></a>
<a id="__codelineno-0-721" name="__codelineno-0-721"></a>            <span class="c1"># In number block, but not a digit: end of number_block</span>
<a id="__codelineno-0-722" name="__codelineno-0-722"></a>            <span class="k">elif</span> <span class="n">in_number_block</span><span class="p">:</span>
<a id="__codelineno-0-723" name="__codelineno-0-723"></a>                <span class="n">in_number_block</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-724" name="__codelineno-0-724"></a>
<a id="__codelineno-0-725" name="__codelineno-0-725"></a>                <span class="c1"># Reuse y and yhat tensors to store full numbers</span>
<a id="__codelineno-0-726" name="__codelineno-0-726"></a>                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-727" name="__codelineno-0-727"></a>                    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">]</span>
<a id="__codelineno-0-728" name="__codelineno-0-728"></a>                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">])</span>
<a id="__codelineno-0-729" name="__codelineno-0-729"></a>                <span class="p">)</span>
<a id="__codelineno-0-730" name="__codelineno-0-730"></a>                <span class="c1"># Make sure non-relevant numerical values are turned into NaN</span>
<a id="__codelineno-0-731" name="__codelineno-0-731"></a>                <span class="c1"># This indicates non-number tokens</span>
<a id="__codelineno-0-732" name="__codelineno-0-732"></a>                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">2</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
<a id="__codelineno-0-733" name="__codelineno-0-733"></a>                <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-734" name="__codelineno-0-734"></a>                    <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">]</span>
<a id="__codelineno-0-735" name="__codelineno-0-735"></a>                    <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">order_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">end_digit</span><span class="p">])</span>
<a id="__codelineno-0-736" name="__codelineno-0-736"></a>                <span class="p">)</span>
<a id="__codelineno-0-737" name="__codelineno-0-737"></a>
<a id="__codelineno-0-738" name="__codelineno-0-738"></a>    <span class="c1"># Update mask with locations of number tokens</span>
<a id="__codelineno-0-739" name="__codelineno-0-739"></a>    <span class="n">number_token_positions</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<a id="__codelineno-0-740" name="__codelineno-0-740"></a>
<a id="__codelineno-0-741" name="__codelineno-0-741"></a>    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">number_token_positions</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="ntloss.core.NumberLevelLoss.forward" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span></code>

<a href="#ntloss.core.NumberLevelLoss.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Computes the NTL based on the dot product between token values and their probs.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td>
                  <code><span title="torch.FloatTensor">FloatTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>3D Tensor of shape BS x T x V.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td>
                  <code><span title="torch.LongTensor">LongTensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D Tensor of shape BS x T.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>loss_weights</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>2D Optional tensor of BS x T with token-wise loss weights.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>reduction</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional string specifying the reduction to apply to the
output. Defaults to "mean", options are "mean", "sum", "none".</p>
              </div>
            </td>
            <td>
                  <code>&#39;mean&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_index</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The token ID to ignore in the labels. Defaults to -100.</p>
              </div>
            </td>
            <td>
                  <code>-100</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Loss tensor
0-D if reduction=="mean"|"sum"
BS x T if reduction=="none"</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>ntloss/core.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-743">743</a></span>
<span class="normal"><a href="#__codelineno-0-744">744</a></span>
<span class="normal"><a href="#__codelineno-0-745">745</a></span>
<span class="normal"><a href="#__codelineno-0-746">746</a></span>
<span class="normal"><a href="#__codelineno-0-747">747</a></span>
<span class="normal"><a href="#__codelineno-0-748">748</a></span>
<span class="normal"><a href="#__codelineno-0-749">749</a></span>
<span class="normal"><a href="#__codelineno-0-750">750</a></span>
<span class="normal"><a href="#__codelineno-0-751">751</a></span>
<span class="normal"><a href="#__codelineno-0-752">752</a></span>
<span class="normal"><a href="#__codelineno-0-753">753</a></span>
<span class="normal"><a href="#__codelineno-0-754">754</a></span>
<span class="normal"><a href="#__codelineno-0-755">755</a></span>
<span class="normal"><a href="#__codelineno-0-756">756</a></span>
<span class="normal"><a href="#__codelineno-0-757">757</a></span>
<span class="normal"><a href="#__codelineno-0-758">758</a></span>
<span class="normal"><a href="#__codelineno-0-759">759</a></span>
<span class="normal"><a href="#__codelineno-0-760">760</a></span>
<span class="normal"><a href="#__codelineno-0-761">761</a></span>
<span class="normal"><a href="#__codelineno-0-762">762</a></span>
<span class="normal"><a href="#__codelineno-0-763">763</a></span>
<span class="normal"><a href="#__codelineno-0-764">764</a></span>
<span class="normal"><a href="#__codelineno-0-765">765</a></span>
<span class="normal"><a href="#__codelineno-0-766">766</a></span>
<span class="normal"><a href="#__codelineno-0-767">767</a></span>
<span class="normal"><a href="#__codelineno-0-768">768</a></span>
<span class="normal"><a href="#__codelineno-0-769">769</a></span>
<span class="normal"><a href="#__codelineno-0-770">770</a></span>
<span class="normal"><a href="#__codelineno-0-771">771</a></span>
<span class="normal"><a href="#__codelineno-0-772">772</a></span>
<span class="normal"><a href="#__codelineno-0-773">773</a></span>
<span class="normal"><a href="#__codelineno-0-774">774</a></span>
<span class="normal"><a href="#__codelineno-0-775">775</a></span>
<span class="normal"><a href="#__codelineno-0-776">776</a></span>
<span class="normal"><a href="#__codelineno-0-777">777</a></span>
<span class="normal"><a href="#__codelineno-0-778">778</a></span>
<span class="normal"><a href="#__codelineno-0-779">779</a></span>
<span class="normal"><a href="#__codelineno-0-780">780</a></span>
<span class="normal"><a href="#__codelineno-0-781">781</a></span>
<span class="normal"><a href="#__codelineno-0-782">782</a></span>
<span class="normal"><a href="#__codelineno-0-783">783</a></span>
<span class="normal"><a href="#__codelineno-0-784">784</a></span>
<span class="normal"><a href="#__codelineno-0-785">785</a></span>
<span class="normal"><a href="#__codelineno-0-786">786</a></span>
<span class="normal"><a href="#__codelineno-0-787">787</a></span>
<span class="normal"><a href="#__codelineno-0-788">788</a></span>
<span class="normal"><a href="#__codelineno-0-789">789</a></span>
<span class="normal"><a href="#__codelineno-0-790">790</a></span>
<span class="normal"><a href="#__codelineno-0-791">791</a></span>
<span class="normal"><a href="#__codelineno-0-792">792</a></span>
<span class="normal"><a href="#__codelineno-0-793">793</a></span>
<span class="normal"><a href="#__codelineno-0-794">794</a></span>
<span class="normal"><a href="#__codelineno-0-795">795</a></span>
<span class="normal"><a href="#__codelineno-0-796">796</a></span>
<span class="normal"><a href="#__codelineno-0-797">797</a></span>
<span class="normal"><a href="#__codelineno-0-798">798</a></span>
<span class="normal"><a href="#__codelineno-0-799">799</a></span>
<span class="normal"><a href="#__codelineno-0-800">800</a></span>
<span class="normal"><a href="#__codelineno-0-801">801</a></span>
<span class="normal"><a href="#__codelineno-0-802">802</a></span>
<span class="normal"><a href="#__codelineno-0-803">803</a></span>
<span class="normal"><a href="#__codelineno-0-804">804</a></span>
<span class="normal"><a href="#__codelineno-0-805">805</a></span>
<span class="normal"><a href="#__codelineno-0-806">806</a></span>
<span class="normal"><a href="#__codelineno-0-807">807</a></span>
<span class="normal"><a href="#__codelineno-0-808">808</a></span>
<span class="normal"><a href="#__codelineno-0-809">809</a></span>
<span class="normal"><a href="#__codelineno-0-810">810</a></span>
<span class="normal"><a href="#__codelineno-0-811">811</a></span>
<span class="normal"><a href="#__codelineno-0-812">812</a></span>
<span class="normal"><a href="#__codelineno-0-813">813</a></span>
<span class="normal"><a href="#__codelineno-0-814">814</a></span>
<span class="normal"><a href="#__codelineno-0-815">815</a></span>
<span class="normal"><a href="#__codelineno-0-816">816</a></span>
<span class="normal"><a href="#__codelineno-0-817">817</a></span>
<span class="normal"><a href="#__codelineno-0-818">818</a></span>
<span class="normal"><a href="#__codelineno-0-819">819</a></span>
<span class="normal"><a href="#__codelineno-0-820">820</a></span>
<span class="normal"><a href="#__codelineno-0-821">821</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-743" name="__codelineno-0-743"></a><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
<a id="__codelineno-0-744" name="__codelineno-0-744"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-745" name="__codelineno-0-745"></a>    <span class="n">logits</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">,</span>
<a id="__codelineno-0-746" name="__codelineno-0-746"></a>    <span class="n">labels</span><span class="p">:</span> <span class="n">LongTensor</span><span class="p">,</span>
<a id="__codelineno-0-747" name="__codelineno-0-747"></a>    <span class="n">loss_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-748" name="__codelineno-0-748"></a>    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
<a id="__codelineno-0-749" name="__codelineno-0-749"></a>    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-750" name="__codelineno-0-750"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-751" name="__codelineno-0-751"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-752" name="__codelineno-0-752"></a><span class="sd">    Computes the NTL based on the dot product between token values and their probs.</span>
<a id="__codelineno-0-753" name="__codelineno-0-753"></a>
<a id="__codelineno-0-754" name="__codelineno-0-754"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-755" name="__codelineno-0-755"></a><span class="sd">        logits: 3D Tensor of shape BS x T x V.</span>
<a id="__codelineno-0-756" name="__codelineno-0-756"></a><span class="sd">        labels: 2D Tensor of shape BS x T.</span>
<a id="__codelineno-0-757" name="__codelineno-0-757"></a><span class="sd">        loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.</span>
<a id="__codelineno-0-758" name="__codelineno-0-758"></a><span class="sd">        reduction: Optional string specifying the reduction to apply to the</span>
<a id="__codelineno-0-759" name="__codelineno-0-759"></a><span class="sd">            output. Defaults to &quot;mean&quot;, options are &quot;mean&quot;, &quot;sum&quot;, &quot;none&quot;.</span>
<a id="__codelineno-0-760" name="__codelineno-0-760"></a><span class="sd">        ignore_index: The token ID to ignore in the labels. Defaults to -100.</span>
<a id="__codelineno-0-761" name="__codelineno-0-761"></a>
<a id="__codelineno-0-762" name="__codelineno-0-762"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-763" name="__codelineno-0-763"></a><span class="sd">        Loss tensor</span>
<a id="__codelineno-0-764" name="__codelineno-0-764"></a><span class="sd">            0-D if reduction==&quot;mean&quot;|&quot;sum&quot;</span>
<a id="__codelineno-0-765" name="__codelineno-0-765"></a><span class="sd">            BS x T if reduction==&quot;none&quot;</span>
<a id="__codelineno-0-766" name="__codelineno-0-766"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-767" name="__codelineno-0-767"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_inputs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
<a id="__codelineno-0-768" name="__codelineno-0-768"></a>
<a id="__codelineno-0-769" name="__codelineno-0-769"></a>    <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_number_token_targets</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
<a id="__codelineno-0-770" name="__codelineno-0-770"></a>    <span class="n">number_token_positions</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<a id="__codelineno-0-771" name="__codelineno-0-771"></a>
<a id="__codelineno-0-772" name="__codelineno-0-772"></a>    <span class="c1"># If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations</span>
<a id="__codelineno-0-773" name="__codelineno-0-773"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">number_token_positions</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span>
<a id="__codelineno-0-774" name="__codelineno-0-774"></a>        <span class="n">loss_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
<a id="__codelineno-0-775" name="__codelineno-0-775"></a>    <span class="p">):</span>
<a id="__codelineno-0-776" name="__codelineno-0-776"></a>        <span class="k">if</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">):</span>
<a id="__codelineno-0-777" name="__codelineno-0-777"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<a id="__codelineno-0-778" name="__codelineno-0-778"></a>        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
<a id="__codelineno-0-779" name="__codelineno-0-779"></a>            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-780" name="__codelineno-0-780"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-781" name="__codelineno-0-781"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2"> is not a valid value for reduction&quot;</span><span class="p">)</span>
<a id="__codelineno-0-782" name="__codelineno-0-782"></a>
<a id="__codelineno-0-783" name="__codelineno-0-783"></a>        <span class="k">return</span> <span class="n">loss</span>
<a id="__codelineno-0-784" name="__codelineno-0-784"></a>
<a id="__codelineno-0-785" name="__codelineno-0-785"></a>    <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dot_product</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<a id="__codelineno-0-786" name="__codelineno-0-786"></a>
<a id="__codelineno-0-787" name="__codelineno-0-787"></a>    <span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">number_token_positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_digits_to_numbers</span><span class="p">(</span>
<a id="__codelineno-0-788" name="__codelineno-0-788"></a>        <span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="p">,</span> <span class="n">labels</span>
<a id="__codelineno-0-789" name="__codelineno-0-789"></a>    <span class="p">)</span>
<a id="__codelineno-0-790" name="__codelineno-0-790"></a>    <span class="k">if</span> <span class="n">loss_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-791" name="__codelineno-0-791"></a>        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-792" name="__codelineno-0-792"></a>    <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-793" name="__codelineno-0-793"></a>
<a id="__codelineno-0-794" name="__codelineno-0-794"></a>    <span class="c1"># NOTE: Alternative could be to apply specified loss function to normalized yhat</span>
<a id="__codelineno-0-795" name="__codelineno-0-795"></a>    <span class="c1"># loss = self.loss_function(torch.div(</span>
<a id="__codelineno-0-796" name="__codelineno-0-796"></a>    <span class="c1">#     yhat[number_token_positions],</span>
<a id="__codelineno-0-797" name="__codelineno-0-797"></a>    <span class="c1">#     y[number_token_positions].clamp_min(torch.finfo(y.dtype).eps),</span>
<a id="__codelineno-0-798" name="__codelineno-0-798"></a>    <span class="c1"># ), torch.ones_like(yhat), reduction=&quot;none&quot;)</span>
<a id="__codelineno-0-799" name="__codelineno-0-799"></a>
<a id="__codelineno-0-800" name="__codelineno-0-800"></a>    <span class="n">y_num</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-801" name="__codelineno-0-801"></a>    <span class="n">yh_num</span> <span class="o">=</span> <span class="n">yhat</span><span class="p">[</span><span class="n">number_token_positions</span><span class="p">]</span>
<a id="__codelineno-0-802" name="__codelineno-0-802"></a>    <span class="c1"># Calculate symmetric MAPE which is bounded in [0, 1]</span>
<a id="__codelineno-0-803" name="__codelineno-0-803"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">yh_num</span> <span class="o">-</span> <span class="n">y_num</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span>
<a id="__codelineno-0-804" name="__codelineno-0-804"></a>        <span class="n">yh_num</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="n">y_num</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
<a id="__codelineno-0-805" name="__codelineno-0-805"></a>    <span class="p">)</span>
<a id="__codelineno-0-806" name="__codelineno-0-806"></a>
<a id="__codelineno-0-807" name="__codelineno-0-807"></a>    <span class="c1"># If reweigh: compute weights for NTL based on logits</span>
<a id="__codelineno-0-808" name="__codelineno-0-808"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh</span><span class="p">:</span>
<a id="__codelineno-0-809" name="__codelineno-0-809"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reweigh_fn</span><span class="p">(</span>
<a id="__codelineno-0-810" name="__codelineno-0-810"></a>            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span>
<a id="__codelineno-0-811" name="__codelineno-0-811"></a>        <span class="p">)</span>
<a id="__codelineno-0-812" name="__codelineno-0-812"></a>
<a id="__codelineno-0-813" name="__codelineno-0-813"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span>
<a id="__codelineno-0-814" name="__codelineno-0-814"></a>        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-815" name="__codelineno-0-815"></a>        <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
<a id="__codelineno-0-816" name="__codelineno-0-816"></a>        <span class="n">loss_weights</span><span class="o">=</span><span class="n">loss_weights</span><span class="p">,</span>
<a id="__codelineno-0-817" name="__codelineno-0-817"></a>        <span class="n">number_token_positions</span><span class="o">=</span><span class="n">number_token_positions</span><span class="p">,</span>
<a id="__codelineno-0-818" name="__codelineno-0-818"></a>        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
<a id="__codelineno-0-819" name="__codelineno-0-819"></a>    <span class="p">)</span>
<a id="__codelineno-0-820" name="__codelineno-0-820"></a>
<a id="__codelineno-0-821" name="__codelineno-0-821"></a>    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      MIT License
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/AI4SD/number-token-loss" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="../../js/termynal.js"></script>
      
        <script src="../../js/custom.js"></script>
      
    
  </body>
</html>