{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Number Token Loss","text":"A regression-like loss that improves numerical reasoning in language models. Originally presented in \u201cRegress, Don\u2019t Guess\u201d (ICML 2025)."},{"location":"#getting-started","title":"Getting Started","text":"<p>Install from PyPI:</p> <pre><code>uv add ntloss\npip install ntloss # if you are oldschool\n</code></pre> <p>Use like this: <pre><code>from ntloss import NTLoss\nntl_fn = NTLoss(tokenizer=tokenizer)\nntl = ntl_fn(logits, labels)\n\n# We recommend\nloss = cross_entropy(logits, labels) + 0.3 * ntl\n</code></pre></p> <p><code>ntloss</code> is currently in alpha phase and pre-release. Feedback &amp; PRs are very welcome.</p>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use <code>ntloss</code>, please cite our paper:</p> <pre><code>@inproceedings{zausinger2025regress,\n  title   = {Regress, Don't Guess \u2013 A Regression-like Loss on Number Tokens for Language Models},\n  author  = {Jonas Zausinger and Lars Pennig and Anamarija Kozina and Sean Sdahl\n             and Julian Sikora and Adrian Dendorfer and Timofey Kuznetsov\n             and Mohamad Hagog and Nina Wiedemann and Kacper Chlodny\n             and Vincent Limbach and Anna Ketteler and Thorben Prein\n             and Vishwa Mohan Singh and Michael Danziger and Jannis Born},\n  booktitle = {Proc. of the 42nd International Conference on Machine Learning (ICML)},\n  year    = {2025},\n  url     = {https://tum-ai.github.io/number-token-loss/}\n}\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#ntlosscore","title":"ntloss.core","text":""},{"location":"api/#ntloss.core","title":"<code>ntloss.core</code>","text":""},{"location":"api/#ntloss.core.AbstractNTLoss","title":"<code>AbstractNTLoss</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>ntloss/core.py</code> <pre><code>class AbstractNTLoss(ABC):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        digit_level: bool = True,\n        reweigh: bool = True,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: Standard HF tokenizer\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabalizing training with NTL. Defaults to True.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.digit_level = digit_level\n        self.reweigh = reweigh\n\n        self.setup_number_tokens()\n\n        self.max_dist = torch.tensor(0.0)\n\n    def setup_number_tokens(self):\n        \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n        # Add digits to vocab if not there yet.\n        vocab_size = len(self.tokenizer)\n        if self.digit_level:\n            new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n        if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n            logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n        vocab = self.tokenizer.get_vocab()\n        self.number_values = torch.full((len(vocab),), float(\"nan\"))\n\n        # Try to convert each token to a float after stripping the space prefix\n        for token, id in vocab.items():\n            if is_number(token, finite=True):\n                if self.digit_level:\n                    # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                    # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                    if (\n                        token.strip().isascii()  # Exclude tokens that are numbers in other languages like \u1098\n                        and -1 &lt;= float(token) &lt;= 9\n                        and len(token.lstrip(\" \")) == 1\n                    ):\n                        self.number_values[id] = float(token)\n                else:\n                    self.number_values[id] = float(token)\n\n        self.is_number_token = ~torch.isnan(self.number_values)\n        self.number_values_dense = self.number_values[self.is_number_token]\n\n        if self.digit_level:\n            assert len(self.number_values_dense) == 10, (\n                f\"You requested digit-level but more than 10 number tokens were identified: {self.number_values_dense}\"\n            )\n\n    @abstractmethod\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor: ...\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Alias to self.forward\"\"\"\n        return self.forward(*args, **kwargs)\n\n    def reweigh_fn(\n        self,\n        logits: Tensor,\n        loss: Tensor,\n        valid_positions: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Scale the NT loss element-wise using the logit weight on number tokens.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            loss: 1D Tensor of size BS*NT with the computed NT losses\n            valid_positions: Tensor of shape BS x T indicating for which tokens\n                the NT loss should be computed\n\n        Returns:\n            A 1D Tensor of size BS*NT with the scaled NT losses\n\n        \"\"\"\n\n        # Take softmax over logits of all tokens in vocab and compute NT logit weight\n        softmax_probs_all = F.softmax(logits, dim=-1)\n        nt_logit_weight = torch.sum(\n            softmax_probs_all[:, :, self.is_number_token], dim=-1\n        )[valid_positions]\n\n        # Apply weights for NTL element-wise\n        loss *= nt_logit_weight\n\n        # Apply regularization\n        loss += (\n            1.01\n            * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n            * (1 - nt_logit_weight)\n        )\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, digit_level: bool = True, reweigh: bool = True)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Standard HF tokenizer</p> required <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabalizing training with NTL. Defaults to True.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True.</p> <code>True</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    digit_level: bool = True,\n    reweigh: bool = True,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: Standard HF tokenizer\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabalizing training with NTL. Defaults to True.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n\n    \"\"\"\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.digit_level = digit_level\n    self.reweigh = reweigh\n\n    self.setup_number_tokens()\n\n    self.max_dist = torch.tensor(0.0)\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.setup_number_tokens","title":"<code>setup_number_tokens()</code>","text":"<p>Setting up attributes needed by NT loss</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_number_tokens(self):\n    \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n    # Add digits to vocab if not there yet.\n    vocab_size = len(self.tokenizer)\n    if self.digit_level:\n        new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n    if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n        logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n    vocab = self.tokenizer.get_vocab()\n    self.number_values = torch.full((len(vocab),), float(\"nan\"))\n\n    # Try to convert each token to a float after stripping the space prefix\n    for token, id in vocab.items():\n        if is_number(token, finite=True):\n            if self.digit_level:\n                # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                if (\n                    token.strip().isascii()  # Exclude tokens that are numbers in other languages like \u1098\n                    and -1 &lt;= float(token) &lt;= 9\n                    and len(token.lstrip(\" \")) == 1\n                ):\n                    self.number_values[id] = float(token)\n            else:\n                self.number_values[id] = float(token)\n\n    self.is_number_token = ~torch.isnan(self.number_values)\n    self.number_values_dense = self.number_values[self.is_number_token]\n\n    if self.digit_level:\n        assert len(self.number_values_dense) == 10, (\n            f\"You requested digit-level but more than 10 number tokens were identified: {self.number_values_dense}\"\n        )\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Alias to self.forward</p> Source code in <code>ntloss/core.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Alias to self.forward\"\"\"\n    return self.forward(*args, **kwargs)\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.reweigh_fn","title":"<code>reweigh_fn(logits: Tensor, loss: Tensor, valid_positions: Tensor) -&gt; Tensor</code>","text":"<p>Scale the NT loss element-wise using the logit weight on number tokens.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>loss</code> <code>Tensor</code> <p>1D Tensor of size BS*NT with the computed NT losses</p> required <code>valid_positions</code> <code>Tensor</code> <p>Tensor of shape BS x T indicating for which tokens the NT loss should be computed</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D Tensor of size BS*NT with the scaled NT losses</p> Source code in <code>ntloss/core.py</code> <pre><code>def reweigh_fn(\n    self,\n    logits: Tensor,\n    loss: Tensor,\n    valid_positions: Tensor,\n) -&gt; Tensor:\n    \"\"\"\n    Scale the NT loss element-wise using the logit weight on number tokens.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        loss: 1D Tensor of size BS*NT with the computed NT losses\n        valid_positions: Tensor of shape BS x T indicating for which tokens\n            the NT loss should be computed\n\n    Returns:\n        A 1D Tensor of size BS*NT with the scaled NT losses\n\n    \"\"\"\n\n    # Take softmax over logits of all tokens in vocab and compute NT logit weight\n    softmax_probs_all = F.softmax(logits, dim=-1)\n    nt_logit_weight = torch.sum(\n        softmax_probs_all[:, :, self.is_number_token], dim=-1\n    )[valid_positions]\n\n    # Apply weights for NTL element-wise\n    loss *= nt_logit_weight\n\n    # Apply regularization\n    loss += (\n        1.01\n        * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n        * (1 - nt_logit_weight)\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct","title":"<code>NTLossDotProduct</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for NT losses that produce a token-wise numerical output</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLossDotProduct(AbstractNTLoss):\n    \"\"\"Class for NT losses that produce a token-wise numerical output\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        loss_function: Callable = F.mse_loss,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            digit_level: Whether to ensure only digit tokens are considered number tokens,\n                stabalizing training with NTL. Defaults to True.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n            loss_function: Function to apply on the delta between the ground truth number\n                and the obtained dot product (nt-probs * token-values).\n\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n        self.loss_function = loss_function\n\n        self.setup_max_dist()\n\n    def setup_max_dist(self):\n        \"\"\"\n        Set up the maximum distance between the number tokens based on the selected loss function.\n        \"\"\"\n\n        # Extract the number token values and get the minimum and maximum\n        vals = self.number_values_dense.unsqueeze(0)\n        max_val = vals.max()\n        min_val = vals.min()\n\n        # Compute the largest value the loss function used in NT loss computation can get\n        # Make sure to account for possibility of assymetrical loss function\n        self.max_dist = torch.maximum(\n            torch.abs(self.loss_function(min_val, max_val)),\n            torch.abs(self.loss_function(max_val, min_val)),\n        )\n\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            labels: Tensor of shape BS x T\n            loss_mask: Optional tensor of BS x T\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        if logits.numel() == 0:\n            raise ValueError(\"Logits passed to the NTLossDotProduct are empty!\")\n        if labels.numel() == 0:\n            raise ValueError(\"Labels passed to the NTLossDotProduct are empty!\")\n\n        labels = labels.masked_fill(labels == -100, 0)\n\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values[labels]\n        valid_positions = ~torch.isnan(y)\n\n        # Apply the loss_mask to lower importance of number tokens before the final answer\n        label_mask = (\n            loss_mask[valid_positions]\n            if loss_mask is not None\n            else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n        )\n\n        # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n        if (torch.count_nonzero(valid_positions) == 0) or (\n            torch.count_nonzero(label_mask) == 0\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(labels, dtype=logits.dtype)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        bs, seq_len, _ = logits.size()\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # compute the weighted average of number tokens\n        yhat = torch.sum(\n            softmax_probs[valid_positions] * self.number_values_dense, dim=-1\n        )\n\n        # Apply specified loss function to y and yhat\n        loss = self.loss_function(yhat, y[valid_positions], reduction=\"none\")\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, valid_positions=valid_positions\n            )\n\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), label_mask.flatten()\n            ) / torch.count_nonzero(label_mask)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), label_mask.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n            loss_[valid_positions.view(-1)] = loss * label_mask\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~valid_positions]) == 0, (\n                \"NTLossDotProduct computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, digit_level: bool = True, reweigh: bool = True, loss_function: Callable = F.mse_loss)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digit tokens are considered number tokens, stabalizing training with NTL. Defaults to True.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True.</p> <code>True</code> <code>loss_function</code> <code>Callable</code> <p>Function to apply on the delta between the ground truth number and the obtained dot product (nt-probs * token-values).</p> <code>mse_loss</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    loss_function: Callable = F.mse_loss,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        digit_level: Whether to ensure only digit tokens are considered number tokens,\n            stabalizing training with NTL. Defaults to True.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n        loss_function: Function to apply on the delta between the ground truth number\n            and the obtained dot product (nt-probs * token-values).\n\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n    self.loss_function = loss_function\n\n    self.setup_max_dist()\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.setup_max_dist","title":"<code>setup_max_dist()</code>","text":"<p>Set up the maximum distance between the number tokens based on the selected loss function.</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_max_dist(self):\n    \"\"\"\n    Set up the maximum distance between the number tokens based on the selected loss function.\n    \"\"\"\n\n    # Extract the number token values and get the minimum and maximum\n    vals = self.number_values_dense.unsqueeze(0)\n    max_val = vals.max()\n    min_val = vals.min()\n\n    # Compute the largest value the loss function used in NT loss computation can get\n    # Make sure to account for possibility of assymetrical loss function\n    self.max_dist = torch.maximum(\n        torch.abs(self.loss_function(min_val, max_val)),\n        torch.abs(self.loss_function(max_val, min_val)),\n    )\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.forward","title":"<code>forward(logits: Tensor, labels: Tensor, loss_mask: Optional[Tensor] = None, reduction: str = 'mean') -&gt; Tensor</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>labels</code> <code>Tensor</code> <p>Tensor of shape BS x T</p> required <code>loss_mask</code> <code>Optional[Tensor]</code> <p>Optional tensor of BS x T</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    labels: Tensor,\n    loss_mask: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        labels: Tensor of shape BS x T\n        loss_mask: Optional tensor of BS x T\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    if logits.numel() == 0:\n        raise ValueError(\"Logits passed to the NTLossDotProduct are empty!\")\n    if labels.numel() == 0:\n        raise ValueError(\"Labels passed to the NTLossDotProduct are empty!\")\n\n    labels = labels.masked_fill(labels == -100, 0)\n\n    # Create a mask to filter out non-digit tokens\n    y = self.number_values[labels]\n    valid_positions = ~torch.isnan(y)\n\n    # Apply the loss_mask to lower importance of number tokens before the final answer\n    label_mask = (\n        loss_mask[valid_positions]\n        if loss_mask is not None\n        else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n    )\n\n    # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n    if (torch.count_nonzero(valid_positions) == 0) or (\n        torch.count_nonzero(label_mask) == 0\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(labels, dtype=logits.dtype)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    bs, seq_len, _ = logits.size()\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # compute the weighted average of number tokens\n    yhat = torch.sum(\n        softmax_probs[valid_positions] * self.number_values_dense, dim=-1\n    )\n\n    # Apply specified loss function to y and yhat\n    loss = self.loss_function(yhat, y[valid_positions], reduction=\"none\")\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, valid_positions=valid_positions\n        )\n\n    if reduction == \"mean\":\n        # Mean pooling (weighted by loss mask)\n        loss = torch.dot(\n            loss.flatten(), label_mask.flatten()\n        ) / torch.count_nonzero(label_mask)\n    elif reduction == \"sum\":\n        loss = torch.dot(loss.flatten(), label_mask.flatten())\n    elif reduction == \"none\":\n        # Cast loss for number tokens back to Tensor of size BS x T\n        loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n        loss_[valid_positions.view(-1)] = loss * label_mask\n        loss = loss_.view(bs, seq_len)\n\n        assert torch.sum(loss[~valid_positions]) == 0, (\n            \"NTLossDotProduct computed for non-digit tokens!\"\n        )\n\n    else:\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss","title":"<code>NTLoss</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for Wasserstein-based NTLoss. This is the default as per our paper.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLoss(AbstractNTLoss):\n    \"\"\"Class for Wasserstein-based NTLoss. This is the default as per our paper.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        squash_factor: Optional[float] = None,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            digit_level: Whether to ensure only digit tokens are considered number tokens,\n                stabalizing training with NTL. Defaults to True.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n            squash_factor: The optional squashing factor for the NTL.\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n\n        self.squash_factor = squash_factor\n        self.setup_distance_lookup(squash_factor)\n\n    def setup_distance_lookup(\n        self,\n        squash_factor: Optional[float] = None,\n    ):\n        \"\"\"\n        Set up a lookup table for the distances between the number tokens.\n        Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n        If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n        NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n        Args:\n            squash_factor: The optional squashing factor used.\n\n        \"\"\"\n\n        # Get token ids for number tokens\n        num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n        # Create mapping from number token ids to their index in order of appearance in vocab:\n        # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n        vocab_to_dist_idx = torch.full((len(self.tokenizer),), -1, dtype=torch.long)\n        # Use arange to ensure order of appearance\n        vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n        # Build NxN abs-diff matrix\n        vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n        diff = torch.abs(vals - vals.t())  # (N x N)\n\n        if isinstance(squash_factor, Number):\n            assert squash_factor &gt; 1, (\n                f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n            )\n\n            # Mask out zeros to find the smallest nonzero diff\n            inf = torch.finfo(diff.dtype).max\n            diff_nonzero = diff.masked_fill(diff == 0, inf)\n            global_min_nz = diff_nonzero.min()\n            # Find largest diff\n            global_max = diff.max()\n\n            # Compute scaling factor based on indicated squash factor\n            scale = (squash_factor - 1) / (global_max - global_min_nz)\n            # Scale the absolute differences using scaling factor\n            lookup = 1 + (diff - global_min_nz) * scale\n            lookup[diff == 0] = 0.0\n\n            additional_log_info = f\", used a squashing factor of {squash_factor}.\"\n        else:\n            lookup = diff\n            additional_log_info = \"\"\n\n        self.vocab_to_dist_idx = vocab_to_dist_idx\n        self.dist_lookup = lookup\n        self.max_dist = lookup.max()\n\n        logger.info(f\"Done setting up the distance lookup table{additional_log_info}\")\n\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            labels: Tensor of shape BS x T\n            loss_mask: Optional tensor of BS x T\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n\n        \"\"\"\n\n        if logits.numel() == 0:\n            raise ValueError(\"Logits passed to the NumberTokenLoss are empty!\")\n        if labels.numel() == 0:\n            raise ValueError(\"Labels passed to the NumberTokenLoss are empty!\")\n\n        labels = labels.clone().masked_fill(labels == ignore_index, 0)\n\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values[labels]\n        valid_positions = ~torch.isnan(y)\n\n        # Apply the loss_mask to lower importance of number tokens before the final answer\n        label_mask = (\n            loss_mask[valid_positions]\n            if loss_mask is not None\n            else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n        )\n\n        # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n        if (torch.count_nonzero(valid_positions) == 0) or (\n            torch.count_nonzero(label_mask) == 0\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(labels, dtype=logits.dtype)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        bs, seq_len, _ = logits.size()\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # get distance between the true numbers and all possible number values from lookup table\n        abs_diff = self.dist_lookup.to(device=labels.device)[\n            self.vocab_to_dist_idx.to(device=labels.device)[labels[valid_positions]]\n        ]\n\n        # loss is the absolute difference weighted by the softmax probs\n        loss = (abs_diff * softmax_probs[valid_positions]).sum(dim=-1)\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, valid_positions=valid_positions\n            )\n\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), label_mask.flatten()\n            ) / torch.count_nonzero(label_mask)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), label_mask.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n            loss_[valid_positions.view(-1)] = loss * label_mask\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~valid_positions]) == 0, (\n                \"NumberTokenLoss computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, digit_level: bool = True, reweigh: bool = True, squash_factor: Optional[float] = None)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digit tokens are considered number tokens, stabalizing training with NTL. Defaults to True.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True.</p> <code>True</code> <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor for the NTL.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    squash_factor: Optional[float] = None,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        digit_level: Whether to ensure only digit tokens are considered number tokens,\n            stabalizing training with NTL. Defaults to True.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n        squash_factor: The optional squashing factor for the NTL.\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n\n    self.squash_factor = squash_factor\n    self.setup_distance_lookup(squash_factor)\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss.setup_distance_lookup","title":"<code>setup_distance_lookup(squash_factor: Optional[float] = None)</code>","text":"<p>Set up a lookup table for the distances between the number tokens. Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token. If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9. NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</p> <p>Parameters:</p> Name Type Description Default <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor used.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def setup_distance_lookup(\n    self,\n    squash_factor: Optional[float] = None,\n):\n    \"\"\"\n    Set up a lookup table for the distances between the number tokens.\n    Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n    If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n    NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n    Args:\n        squash_factor: The optional squashing factor used.\n\n    \"\"\"\n\n    # Get token ids for number tokens\n    num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n    # Create mapping from number token ids to their index in order of appearance in vocab:\n    # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n    vocab_to_dist_idx = torch.full((len(self.tokenizer),), -1, dtype=torch.long)\n    # Use arange to ensure order of appearance\n    vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n    # Build NxN abs-diff matrix\n    vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n    diff = torch.abs(vals - vals.t())  # (N x N)\n\n    if isinstance(squash_factor, Number):\n        assert squash_factor &gt; 1, (\n            f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n        )\n\n        # Mask out zeros to find the smallest nonzero diff\n        inf = torch.finfo(diff.dtype).max\n        diff_nonzero = diff.masked_fill(diff == 0, inf)\n        global_min_nz = diff_nonzero.min()\n        # Find largest diff\n        global_max = diff.max()\n\n        # Compute scaling factor based on indicated squash factor\n        scale = (squash_factor - 1) / (global_max - global_min_nz)\n        # Scale the absolute differences using scaling factor\n        lookup = 1 + (diff - global_min_nz) * scale\n        lookup[diff == 0] = 0.0\n\n        additional_log_info = f\", used a squashing factor of {squash_factor}.\"\n    else:\n        lookup = diff\n        additional_log_info = \"\"\n\n    self.vocab_to_dist_idx = vocab_to_dist_idx\n    self.dist_lookup = lookup\n    self.max_dist = lookup.max()\n\n    logger.info(f\"Done setting up the distance lookup table{additional_log_info}\")\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss.forward","title":"<code>forward(logits: Tensor, labels: Tensor, loss_mask: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>labels</code> <code>Tensor</code> <p>Tensor of shape BS x T</p> required <code>loss_mask</code> <code>Optional[Tensor]</code> <p>Optional tensor of BS x T</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    labels: Tensor,\n    loss_mask: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        labels: Tensor of shape BS x T\n        loss_mask: Optional tensor of BS x T\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n\n    \"\"\"\n\n    if logits.numel() == 0:\n        raise ValueError(\"Logits passed to the NumberTokenLoss are empty!\")\n    if labels.numel() == 0:\n        raise ValueError(\"Labels passed to the NumberTokenLoss are empty!\")\n\n    labels = labels.clone().masked_fill(labels == ignore_index, 0)\n\n    # Create a mask to filter out non-digit tokens\n    y = self.number_values[labels]\n    valid_positions = ~torch.isnan(y)\n\n    # Apply the loss_mask to lower importance of number tokens before the final answer\n    label_mask = (\n        loss_mask[valid_positions]\n        if loss_mask is not None\n        else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n    )\n\n    # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n    if (torch.count_nonzero(valid_positions) == 0) or (\n        torch.count_nonzero(label_mask) == 0\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(labels, dtype=logits.dtype)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    bs, seq_len, _ = logits.size()\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # get distance between the true numbers and all possible number values from lookup table\n    abs_diff = self.dist_lookup.to(device=labels.device)[\n        self.vocab_to_dist_idx.to(device=labels.device)[labels[valid_positions]]\n    ]\n\n    # loss is the absolute difference weighted by the softmax probs\n    loss = (abs_diff * softmax_probs[valid_positions]).sum(dim=-1)\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, valid_positions=valid_positions\n        )\n\n    if reduction == \"mean\":\n        # Mean pooling (weighted by loss mask)\n        loss = torch.dot(\n            loss.flatten(), label_mask.flatten()\n        ) / torch.count_nonzero(label_mask)\n    elif reduction == \"sum\":\n        loss = torch.dot(loss.flatten(), label_mask.flatten())\n    elif reduction == \"none\":\n        # Cast loss for number tokens back to Tensor of size BS x T\n        loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n        loss_[valid_positions.view(-1)] = loss * label_mask\n        loss = loss_.view(bs, seq_len)\n\n        assert torch.sum(loss[~valid_positions]) == 0, (\n            \"NumberTokenLoss computed for non-digit tokens!\"\n        )\n\n    else:\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    return loss\n</code></pre>"},{"location":"api/#ntlossutils","title":"ntloss.utils","text":""},{"location":"api/#ntloss.utils","title":"<code>ntloss.utils</code>","text":""},{"location":"api/#ntloss.utils.is_number","title":"<code>is_number(something: Any, finite: bool = True) -&gt; bool</code>","text":"<p>Check whether something is convertible to a float</p> <p>Parameters:</p> Name Type Description Default <code>something</code> <code>Any</code> <p>something to test for float casting.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not it's a number</p> Source code in <code>ntloss/utils.py</code> <pre><code>def is_number(something: Any, finite: bool = True) -&gt; bool:\n    \"\"\"Check whether something is convertible to a float\n\n    Args:\n        something: something to test for float casting.\n\n    Returns:\n        Whether or not it's a number\n    \"\"\"\n    try:\n        f = float(something)\n        if finite and not math.isfinite(f):\n            return False\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api/core/","title":"ntloss.core","text":""},{"location":"api/core/#ntloss.core","title":"<code>ntloss.core</code>","text":""},{"location":"api/core/#ntloss.core.AbstractNTLoss","title":"<code>AbstractNTLoss</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>ntloss/core.py</code> <pre><code>class AbstractNTLoss(ABC):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        digit_level: bool = True,\n        reweigh: bool = True,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: Standard HF tokenizer\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabalizing training with NTL. Defaults to True.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.digit_level = digit_level\n        self.reweigh = reweigh\n\n        self.setup_number_tokens()\n\n        self.max_dist = torch.tensor(0.0)\n\n    def setup_number_tokens(self):\n        \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n        # Add digits to vocab if not there yet.\n        vocab_size = len(self.tokenizer)\n        if self.digit_level:\n            new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n        if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n            logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n        vocab = self.tokenizer.get_vocab()\n        self.number_values = torch.full((len(vocab),), float(\"nan\"))\n\n        # Try to convert each token to a float after stripping the space prefix\n        for token, id in vocab.items():\n            if is_number(token, finite=True):\n                if self.digit_level:\n                    # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                    # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                    if (\n                        token.strip().isascii()  # Exclude tokens that are numbers in other languages like \u1098\n                        and -1 &lt;= float(token) &lt;= 9\n                        and len(token.lstrip(\" \")) == 1\n                    ):\n                        self.number_values[id] = float(token)\n                else:\n                    self.number_values[id] = float(token)\n\n        self.is_number_token = ~torch.isnan(self.number_values)\n        self.number_values_dense = self.number_values[self.is_number_token]\n\n        if self.digit_level:\n            assert len(self.number_values_dense) == 10, (\n                f\"You requested digit-level but more than 10 number tokens were identified: {self.number_values_dense}\"\n            )\n\n    @abstractmethod\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor: ...\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Alias to self.forward\"\"\"\n        return self.forward(*args, **kwargs)\n\n    def reweigh_fn(\n        self,\n        logits: Tensor,\n        loss: Tensor,\n        valid_positions: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Scale the NT loss element-wise using the logit weight on number tokens.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            loss: 1D Tensor of size BS*NT with the computed NT losses\n            valid_positions: Tensor of shape BS x T indicating for which tokens\n                the NT loss should be computed\n\n        Returns:\n            A 1D Tensor of size BS*NT with the scaled NT losses\n\n        \"\"\"\n\n        # Take softmax over logits of all tokens in vocab and compute NT logit weight\n        softmax_probs_all = F.softmax(logits, dim=-1)\n        nt_logit_weight = torch.sum(\n            softmax_probs_all[:, :, self.is_number_token], dim=-1\n        )[valid_positions]\n\n        # Apply weights for NTL element-wise\n        loss *= nt_logit_weight\n\n        # Apply regularization\n        loss += (\n            1.01\n            * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n            * (1 - nt_logit_weight)\n        )\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, digit_level: bool = True, reweigh: bool = True)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Standard HF tokenizer</p> required <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabalizing training with NTL. Defaults to True.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True.</p> <code>True</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    digit_level: bool = True,\n    reweigh: bool = True,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: Standard HF tokenizer\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabalizing training with NTL. Defaults to True.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n\n    \"\"\"\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.digit_level = digit_level\n    self.reweigh = reweigh\n\n    self.setup_number_tokens()\n\n    self.max_dist = torch.tensor(0.0)\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.setup_number_tokens","title":"<code>setup_number_tokens()</code>","text":"<p>Setting up attributes needed by NT loss</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_number_tokens(self):\n    \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n    # Add digits to vocab if not there yet.\n    vocab_size = len(self.tokenizer)\n    if self.digit_level:\n        new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n    if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n        logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n    vocab = self.tokenizer.get_vocab()\n    self.number_values = torch.full((len(vocab),), float(\"nan\"))\n\n    # Try to convert each token to a float after stripping the space prefix\n    for token, id in vocab.items():\n        if is_number(token, finite=True):\n            if self.digit_level:\n                # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                if (\n                    token.strip().isascii()  # Exclude tokens that are numbers in other languages like \u1098\n                    and -1 &lt;= float(token) &lt;= 9\n                    and len(token.lstrip(\" \")) == 1\n                ):\n                    self.number_values[id] = float(token)\n            else:\n                self.number_values[id] = float(token)\n\n    self.is_number_token = ~torch.isnan(self.number_values)\n    self.number_values_dense = self.number_values[self.is_number_token]\n\n    if self.digit_level:\n        assert len(self.number_values_dense) == 10, (\n            f\"You requested digit-level but more than 10 number tokens were identified: {self.number_values_dense}\"\n        )\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Alias to self.forward</p> Source code in <code>ntloss/core.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Alias to self.forward\"\"\"\n    return self.forward(*args, **kwargs)\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.reweigh_fn","title":"<code>reweigh_fn(logits: Tensor, loss: Tensor, valid_positions: Tensor) -&gt; Tensor</code>","text":"<p>Scale the NT loss element-wise using the logit weight on number tokens.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>loss</code> <code>Tensor</code> <p>1D Tensor of size BS*NT with the computed NT losses</p> required <code>valid_positions</code> <code>Tensor</code> <p>Tensor of shape BS x T indicating for which tokens the NT loss should be computed</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D Tensor of size BS*NT with the scaled NT losses</p> Source code in <code>ntloss/core.py</code> <pre><code>def reweigh_fn(\n    self,\n    logits: Tensor,\n    loss: Tensor,\n    valid_positions: Tensor,\n) -&gt; Tensor:\n    \"\"\"\n    Scale the NT loss element-wise using the logit weight on number tokens.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        loss: 1D Tensor of size BS*NT with the computed NT losses\n        valid_positions: Tensor of shape BS x T indicating for which tokens\n            the NT loss should be computed\n\n    Returns:\n        A 1D Tensor of size BS*NT with the scaled NT losses\n\n    \"\"\"\n\n    # Take softmax over logits of all tokens in vocab and compute NT logit weight\n    softmax_probs_all = F.softmax(logits, dim=-1)\n    nt_logit_weight = torch.sum(\n        softmax_probs_all[:, :, self.is_number_token], dim=-1\n    )[valid_positions]\n\n    # Apply weights for NTL element-wise\n    loss *= nt_logit_weight\n\n    # Apply regularization\n    loss += (\n        1.01\n        * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n        * (1 - nt_logit_weight)\n    )\n\n    return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct","title":"<code>NTLossDotProduct</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for NT losses that produce a token-wise numerical output</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLossDotProduct(AbstractNTLoss):\n    \"\"\"Class for NT losses that produce a token-wise numerical output\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        loss_function: Callable = F.mse_loss,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            digit_level: Whether to ensure only digit tokens are considered number tokens,\n                stabalizing training with NTL. Defaults to True.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n            loss_function: Function to apply on the delta between the ground truth number\n                and the obtained dot product (nt-probs * token-values).\n\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n        self.loss_function = loss_function\n\n        self.setup_max_dist()\n\n    def setup_max_dist(self):\n        \"\"\"\n        Set up the maximum distance between the number tokens based on the selected loss function.\n        \"\"\"\n\n        # Extract the number token values and get the minimum and maximum\n        vals = self.number_values_dense.unsqueeze(0)\n        max_val = vals.max()\n        min_val = vals.min()\n\n        # Compute the largest value the loss function used in NT loss computation can get\n        # Make sure to account for possibility of assymetrical loss function\n        self.max_dist = torch.maximum(\n            torch.abs(self.loss_function(min_val, max_val)),\n            torch.abs(self.loss_function(max_val, min_val)),\n        )\n\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            labels: Tensor of shape BS x T\n            loss_mask: Optional tensor of BS x T\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        if logits.numel() == 0:\n            raise ValueError(\"Logits passed to the NTLossDotProduct are empty!\")\n        if labels.numel() == 0:\n            raise ValueError(\"Labels passed to the NTLossDotProduct are empty!\")\n\n        labels = labels.masked_fill(labels == -100, 0)\n\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values[labels]\n        valid_positions = ~torch.isnan(y)\n\n        # Apply the loss_mask to lower importance of number tokens before the final answer\n        label_mask = (\n            loss_mask[valid_positions]\n            if loss_mask is not None\n            else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n        )\n\n        # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n        if (torch.count_nonzero(valid_positions) == 0) or (\n            torch.count_nonzero(label_mask) == 0\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(labels, dtype=logits.dtype)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        bs, seq_len, _ = logits.size()\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # compute the weighted average of number tokens\n        yhat = torch.sum(\n            softmax_probs[valid_positions] * self.number_values_dense, dim=-1\n        )\n\n        # Apply specified loss function to y and yhat\n        loss = self.loss_function(yhat, y[valid_positions], reduction=\"none\")\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, valid_positions=valid_positions\n            )\n\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), label_mask.flatten()\n            ) / torch.count_nonzero(label_mask)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), label_mask.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n            loss_[valid_positions.view(-1)] = loss * label_mask\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~valid_positions]) == 0, (\n                \"NTLossDotProduct computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, digit_level: bool = True, reweigh: bool = True, loss_function: Callable = F.mse_loss)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digit tokens are considered number tokens, stabalizing training with NTL. Defaults to True.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True.</p> <code>True</code> <code>loss_function</code> <code>Callable</code> <p>Function to apply on the delta between the ground truth number and the obtained dot product (nt-probs * token-values).</p> <code>mse_loss</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    loss_function: Callable = F.mse_loss,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        digit_level: Whether to ensure only digit tokens are considered number tokens,\n            stabalizing training with NTL. Defaults to True.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n        loss_function: Function to apply on the delta between the ground truth number\n            and the obtained dot product (nt-probs * token-values).\n\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n    self.loss_function = loss_function\n\n    self.setup_max_dist()\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.setup_max_dist","title":"<code>setup_max_dist()</code>","text":"<p>Set up the maximum distance between the number tokens based on the selected loss function.</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_max_dist(self):\n    \"\"\"\n    Set up the maximum distance between the number tokens based on the selected loss function.\n    \"\"\"\n\n    # Extract the number token values and get the minimum and maximum\n    vals = self.number_values_dense.unsqueeze(0)\n    max_val = vals.max()\n    min_val = vals.min()\n\n    # Compute the largest value the loss function used in NT loss computation can get\n    # Make sure to account for possibility of assymetrical loss function\n    self.max_dist = torch.maximum(\n        torch.abs(self.loss_function(min_val, max_val)),\n        torch.abs(self.loss_function(max_val, min_val)),\n    )\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.forward","title":"<code>forward(logits: Tensor, labels: Tensor, loss_mask: Optional[Tensor] = None, reduction: str = 'mean') -&gt; Tensor</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>labels</code> <code>Tensor</code> <p>Tensor of shape BS x T</p> required <code>loss_mask</code> <code>Optional[Tensor]</code> <p>Optional tensor of BS x T</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    labels: Tensor,\n    loss_mask: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        labels: Tensor of shape BS x T\n        loss_mask: Optional tensor of BS x T\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    if logits.numel() == 0:\n        raise ValueError(\"Logits passed to the NTLossDotProduct are empty!\")\n    if labels.numel() == 0:\n        raise ValueError(\"Labels passed to the NTLossDotProduct are empty!\")\n\n    labels = labels.masked_fill(labels == -100, 0)\n\n    # Create a mask to filter out non-digit tokens\n    y = self.number_values[labels]\n    valid_positions = ~torch.isnan(y)\n\n    # Apply the loss_mask to lower importance of number tokens before the final answer\n    label_mask = (\n        loss_mask[valid_positions]\n        if loss_mask is not None\n        else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n    )\n\n    # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n    if (torch.count_nonzero(valid_positions) == 0) or (\n        torch.count_nonzero(label_mask) == 0\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(labels, dtype=logits.dtype)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    bs, seq_len, _ = logits.size()\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # compute the weighted average of number tokens\n    yhat = torch.sum(\n        softmax_probs[valid_positions] * self.number_values_dense, dim=-1\n    )\n\n    # Apply specified loss function to y and yhat\n    loss = self.loss_function(yhat, y[valid_positions], reduction=\"none\")\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, valid_positions=valid_positions\n        )\n\n    if reduction == \"mean\":\n        # Mean pooling (weighted by loss mask)\n        loss = torch.dot(\n            loss.flatten(), label_mask.flatten()\n        ) / torch.count_nonzero(label_mask)\n    elif reduction == \"sum\":\n        loss = torch.dot(loss.flatten(), label_mask.flatten())\n    elif reduction == \"none\":\n        # Cast loss for number tokens back to Tensor of size BS x T\n        loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n        loss_[valid_positions.view(-1)] = loss * label_mask\n        loss = loss_.view(bs, seq_len)\n\n        assert torch.sum(loss[~valid_positions]) == 0, (\n            \"NTLossDotProduct computed for non-digit tokens!\"\n        )\n\n    else:\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss","title":"<code>NTLoss</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for Wasserstein-based NTLoss. This is the default as per our paper.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLoss(AbstractNTLoss):\n    \"\"\"Class for Wasserstein-based NTLoss. This is the default as per our paper.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        squash_factor: Optional[float] = None,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            digit_level: Whether to ensure only digit tokens are considered number tokens,\n                stabalizing training with NTL. Defaults to True.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n            squash_factor: The optional squashing factor for the NTL.\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n\n        self.squash_factor = squash_factor\n        self.setup_distance_lookup(squash_factor)\n\n    def setup_distance_lookup(\n        self,\n        squash_factor: Optional[float] = None,\n    ):\n        \"\"\"\n        Set up a lookup table for the distances between the number tokens.\n        Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n        If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n        NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n        Args:\n            squash_factor: The optional squashing factor used.\n\n        \"\"\"\n\n        # Get token ids for number tokens\n        num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n        # Create mapping from number token ids to their index in order of appearance in vocab:\n        # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n        vocab_to_dist_idx = torch.full((len(self.tokenizer),), -1, dtype=torch.long)\n        # Use arange to ensure order of appearance\n        vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n        # Build NxN abs-diff matrix\n        vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n        diff = torch.abs(vals - vals.t())  # (N x N)\n\n        if isinstance(squash_factor, Number):\n            assert squash_factor &gt; 1, (\n                f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n            )\n\n            # Mask out zeros to find the smallest nonzero diff\n            inf = torch.finfo(diff.dtype).max\n            diff_nonzero = diff.masked_fill(diff == 0, inf)\n            global_min_nz = diff_nonzero.min()\n            # Find largest diff\n            global_max = diff.max()\n\n            # Compute scaling factor based on indicated squash factor\n            scale = (squash_factor - 1) / (global_max - global_min_nz)\n            # Scale the absolute differences using scaling factor\n            lookup = 1 + (diff - global_min_nz) * scale\n            lookup[diff == 0] = 0.0\n\n            additional_log_info = f\", used a squashing factor of {squash_factor}.\"\n        else:\n            lookup = diff\n            additional_log_info = \"\"\n\n        self.vocab_to_dist_idx = vocab_to_dist_idx\n        self.dist_lookup = lookup\n        self.max_dist = lookup.max()\n\n        logger.info(f\"Done setting up the distance lookup table{additional_log_info}\")\n\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            labels: Tensor of shape BS x T\n            loss_mask: Optional tensor of BS x T\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n\n        \"\"\"\n\n        if logits.numel() == 0:\n            raise ValueError(\"Logits passed to the NumberTokenLoss are empty!\")\n        if labels.numel() == 0:\n            raise ValueError(\"Labels passed to the NumberTokenLoss are empty!\")\n\n        labels = labels.clone().masked_fill(labels == ignore_index, 0)\n\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values[labels]\n        valid_positions = ~torch.isnan(y)\n\n        # Apply the loss_mask to lower importance of number tokens before the final answer\n        label_mask = (\n            loss_mask[valid_positions]\n            if loss_mask is not None\n            else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n        )\n\n        # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n        if (torch.count_nonzero(valid_positions) == 0) or (\n            torch.count_nonzero(label_mask) == 0\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(labels, dtype=logits.dtype)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        bs, seq_len, _ = logits.size()\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # get distance between the true numbers and all possible number values from lookup table\n        abs_diff = self.dist_lookup.to(device=labels.device)[\n            self.vocab_to_dist_idx.to(device=labels.device)[labels[valid_positions]]\n        ]\n\n        # loss is the absolute difference weighted by the softmax probs\n        loss = (abs_diff * softmax_probs[valid_positions]).sum(dim=-1)\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, valid_positions=valid_positions\n            )\n\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), label_mask.flatten()\n            ) / torch.count_nonzero(label_mask)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), label_mask.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n            loss_[valid_positions.view(-1)] = loss * label_mask\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~valid_positions]) == 0, (\n                \"NumberTokenLoss computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, digit_level: bool = True, reweigh: bool = True, squash_factor: Optional[float] = None)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digit tokens are considered number tokens, stabalizing training with NTL. Defaults to True.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True.</p> <code>True</code> <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor for the NTL.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    squash_factor: Optional[float] = None,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        digit_level: Whether to ensure only digit tokens are considered number tokens,\n            stabalizing training with NTL. Defaults to True.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n        squash_factor: The optional squashing factor for the NTL.\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n\n    self.squash_factor = squash_factor\n    self.setup_distance_lookup(squash_factor)\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss.setup_distance_lookup","title":"<code>setup_distance_lookup(squash_factor: Optional[float] = None)</code>","text":"<p>Set up a lookup table for the distances between the number tokens. Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token. If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9. NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</p> <p>Parameters:</p> Name Type Description Default <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor used.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def setup_distance_lookup(\n    self,\n    squash_factor: Optional[float] = None,\n):\n    \"\"\"\n    Set up a lookup table for the distances between the number tokens.\n    Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n    If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n    NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n    Args:\n        squash_factor: The optional squashing factor used.\n\n    \"\"\"\n\n    # Get token ids for number tokens\n    num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n    # Create mapping from number token ids to their index in order of appearance in vocab:\n    # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n    vocab_to_dist_idx = torch.full((len(self.tokenizer),), -1, dtype=torch.long)\n    # Use arange to ensure order of appearance\n    vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n    # Build NxN abs-diff matrix\n    vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n    diff = torch.abs(vals - vals.t())  # (N x N)\n\n    if isinstance(squash_factor, Number):\n        assert squash_factor &gt; 1, (\n            f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n        )\n\n        # Mask out zeros to find the smallest nonzero diff\n        inf = torch.finfo(diff.dtype).max\n        diff_nonzero = diff.masked_fill(diff == 0, inf)\n        global_min_nz = diff_nonzero.min()\n        # Find largest diff\n        global_max = diff.max()\n\n        # Compute scaling factor based on indicated squash factor\n        scale = (squash_factor - 1) / (global_max - global_min_nz)\n        # Scale the absolute differences using scaling factor\n        lookup = 1 + (diff - global_min_nz) * scale\n        lookup[diff == 0] = 0.0\n\n        additional_log_info = f\", used a squashing factor of {squash_factor}.\"\n    else:\n        lookup = diff\n        additional_log_info = \"\"\n\n    self.vocab_to_dist_idx = vocab_to_dist_idx\n    self.dist_lookup = lookup\n    self.max_dist = lookup.max()\n\n    logger.info(f\"Done setting up the distance lookup table{additional_log_info}\")\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss.forward","title":"<code>forward(logits: Tensor, labels: Tensor, loss_mask: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>labels</code> <code>Tensor</code> <p>Tensor of shape BS x T</p> required <code>loss_mask</code> <code>Optional[Tensor]</code> <p>Optional tensor of BS x T</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    labels: Tensor,\n    loss_mask: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        labels: Tensor of shape BS x T\n        loss_mask: Optional tensor of BS x T\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n\n    \"\"\"\n\n    if logits.numel() == 0:\n        raise ValueError(\"Logits passed to the NumberTokenLoss are empty!\")\n    if labels.numel() == 0:\n        raise ValueError(\"Labels passed to the NumberTokenLoss are empty!\")\n\n    labels = labels.clone().masked_fill(labels == ignore_index, 0)\n\n    # Create a mask to filter out non-digit tokens\n    y = self.number_values[labels]\n    valid_positions = ~torch.isnan(y)\n\n    # Apply the loss_mask to lower importance of number tokens before the final answer\n    label_mask = (\n        loss_mask[valid_positions]\n        if loss_mask is not None\n        else torch.ones_like(labels, dtype=logits.dtype)[valid_positions]\n    )\n\n    # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n    if (torch.count_nonzero(valid_positions) == 0) or (\n        torch.count_nonzero(label_mask) == 0\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(labels, dtype=logits.dtype)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    bs, seq_len, _ = logits.size()\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # get distance between the true numbers and all possible number values from lookup table\n    abs_diff = self.dist_lookup.to(device=labels.device)[\n        self.vocab_to_dist_idx.to(device=labels.device)[labels[valid_positions]]\n    ]\n\n    # loss is the absolute difference weighted by the softmax probs\n    loss = (abs_diff * softmax_probs[valid_positions]).sum(dim=-1)\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, valid_positions=valid_positions\n        )\n\n    if reduction == \"mean\":\n        # Mean pooling (weighted by loss mask)\n        loss = torch.dot(\n            loss.flatten(), label_mask.flatten()\n        ) / torch.count_nonzero(label_mask)\n    elif reduction == \"sum\":\n        loss = torch.dot(loss.flatten(), label_mask.flatten())\n    elif reduction == \"none\":\n        # Cast loss for number tokens back to Tensor of size BS x T\n        loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n        loss_[valid_positions.view(-1)] = loss * label_mask\n        loss = loss_.view(bs, seq_len)\n\n        assert torch.sum(loss[~valid_positions]) == 0, (\n            \"NumberTokenLoss computed for non-digit tokens!\"\n        )\n\n    else:\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    return loss\n</code></pre>"},{"location":"api/utils/","title":"ntloss.utils","text":""},{"location":"api/utils/#ntloss.utils","title":"<code>ntloss.utils</code>","text":""},{"location":"api/utils/#ntloss.utils.is_number","title":"<code>is_number(something: Any, finite: bool = True) -&gt; bool</code>","text":"<p>Check whether something is convertible to a float</p> <p>Parameters:</p> Name Type Description Default <code>something</code> <code>Any</code> <p>something to test for float casting.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not it's a number</p> Source code in <code>ntloss/utils.py</code> <pre><code>def is_number(something: Any, finite: bool = True) -&gt; bool:\n    \"\"\"Check whether something is convertible to a float\n\n    Args:\n        something: something to test for float casting.\n\n    Returns:\n        Whether or not it's a number\n    \"\"\"\n    try:\n        f = float(something)\n        if finite and not math.isfinite(f):\n            return False\n        return True\n    except ValueError:\n        return False\n</code></pre>"}]}