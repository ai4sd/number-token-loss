{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Number Token Loss","text":"A regression-like loss that improves numerical reasoning in language models. Originally presented in \u201cRegress, Don\u2019t Guess\u201d (ICML 2025)."},{"location":"#getting-started","title":"Getting Started","text":"<p>Install from PyPI:</p> <pre><code>uv add ntloss\npip install ntloss # if you are oldschool\n</code></pre> <p>Use like this: <pre><code>from ntloss import NTLoss\nntl_fn = NTLoss(tokenizer=tokenizer)\nntl = ntl_fn(logits, labels)\n\n# We recommend\nloss = cross_entropy(logits, labels) + 0.3 * ntl\n</code></pre></p> <p><code>ntloss</code> is currently in alpha phase and pre-release. Feedback &amp; PRs are very welcome.</p>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use <code>ntloss</code>, please cite our paper:</p> <pre><code>@inproceedings{zausinger2025regress,\n  title   = {Regress, Don't Guess \u2013 A Regression-like Loss on Number Tokens for Language Models},\n  author  = {Jonas Zausinger and Lars Pennig and Anamarija Kozina and Sean Sdahl\n             and Julian Sikora and Adrian Dendorfer and Timofey Kuznetsov\n             and Mohamad Hagog and Nina Wiedemann and Kacper Chlodny\n             and Vincent Limbach and Anna Ketteler and Thorben Prein\n             and Vishwa Mohan Singh and Michael Danziger and Jannis Born},\n  booktitle = {Proc. of the 42nd International Conference on Machine Learning (ICML)},\n  year    = {2025},\n  url     = {https://tum-ai.github.io/number-token-loss/}\n}\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#ntlosscore","title":"ntloss.core","text":""},{"location":"api/#ntloss.core","title":"<code>ntloss.core</code>","text":""},{"location":"api/#ntloss.core.AbstractNTLoss","title":"<code>AbstractNTLoss</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>ntloss/core.py</code> <pre><code>class AbstractNTLoss(ABC):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        digit_level: bool = True,\n        reweigh: bool = True,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: Standard HF tokenizer.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabilizing training with NTL. Defaults to True. Used for most\n                experiments in the ICML paper.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size if vocab_size is not None else len(self.tokenizer)\n        self._vocab_size_validated = False\n        self.digit_level = digit_level\n        self.reweigh = reweigh\n\n        self.setup_number_tokens()\n\n        self.max_dist = torch.tensor(0.0)\n\n    def setup_number_tokens(self):\n        \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n        # Add digits to vocab if not there yet.\n        vocab_size = len(self.tokenizer)\n        if self.digit_level:\n            new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n        if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n            logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n        vocab = self.tokenizer.get_vocab()\n        self.number_values: FloatTensor = torch.full((self.vocab_size,), float(\"nan\"))\n\n        # Try to convert each token to a float after stripping the space prefix\n        for token, id in vocab.items():\n            if is_number(token, finite=True):\n                if self.digit_level:\n                    # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                    # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                    # Excludes tokens that are numbers in other languages like \u1098 and tokens with space pre-/postfix like ` 2`.\n                    if token.isascii() and -1 &lt;= float(token) &lt;= 9 and len(token) == 1:\n                        self.number_values[id] = float(token)\n                else:\n                    self.number_values[id] = float(token)\n\n        self.is_number_token = ~torch.isnan(self.number_values)\n        if self.is_number_token.sum() == len(self.is_number_token):\n            raise ValueError(\n                \"At least one token needs to be not a number, otherwise `ignore_index` cannot be set up safely\"\n            )\n        self.nan_id = torch.where(~self.is_number_token)[0][0].item()\n        self.number_values_dense = self.number_values[self.is_number_token]\n\n        if self.digit_level and (num_nts := len(self.number_values_dense)) != 10:\n            logger.error(\n                f\"You requested digit-level but {num_nts} number tokens were identified: {self.number_values_dense}\"\n            )\n\n    @abstractmethod\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor: ...\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Alias to self.forward\"\"\"\n        return self.forward(*args, **kwargs)\n\n    def reweigh_fn(\n        self,\n        logits: Tensor,\n        loss: Tensor,\n        number_token_positions: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Scale the NT loss element-wise using the logit weight on number tokens.\n        NOTE: This reweighing ensures that if ground truth is a number token\n            but most probability mass is on text tokens, the loss will be *higher*\n            than the worst possible number token. This is an edge case in practice.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            loss: 1D Tensor over all number tokens in batch.\n            number_token_positions: 2D Tensor of shape BS x T indicating for which tokens\n                the NT loss was computed.\n\n        Returns:\n            A 1D Tensor over all number tokens in batch with the scaled NT losses.\n        \"\"\"\n\n        # Take softmax over logits of all tokens in vocab and compute NT logit weight\n        softmax_probs_all = F.softmax(logits, dim=-1)\n        nt_logit_weight = torch.sum(\n            softmax_probs_all[:, :, self.is_number_token], dim=-1\n        )[number_token_positions]\n\n        # Apply weights for NTL element-wise\n        loss *= nt_logit_weight\n\n        # Apply regularization\n        # NOTE: We could consider reweighing here with the max for that label token\n        # rather than the global max\n        loss += (\n            1.01\n            * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n            * (1 - nt_logit_weight)\n        )\n\n        return loss\n\n    def _validate_inputs(\n        self,\n        logits: FloatTensor,\n        labels: Optional[LongTensor],\n        loss_weights: Optional[Tensor],\n    ):\n        \"\"\"Private method to perform size and type checks.\"\"\"\n        if (td := len(logits.shape)) != 3 or (ne := logits.numel()) == 0:\n            raise ValueError(\n                f\"Logits have to be non-empty 3D Tensor, not {td}D with {ne} elements\"\n            )\n        if not torch.is_floating_point(logits):\n            raise TypeError(\"Logits have to be FloatTensor.\")\n        if labels is None:\n            return\n        if not labels.dtype == torch.long:\n            raise TypeError(f\"Labels have to be LongTensor, not {type(labels)}\")\n        if (b := labels.shape) != (a := logits.shape[:-1]):\n            raise ValueError(\n                f\"Logit and label sizes of first 2 dims have to match: {a} vs {b}\"\n            )\n\n        if (td := len(labels.shape)) != 2 or (ne := labels.numel()) == 0:\n            raise ValueError(\n                f\"Labels have to be non-empty 2D Tensor, not {td}D with {ne} elements\"\n            )\n        if loss_weights is not None:\n            if loss_weights.shape != labels.shape:\n                raise ValueError(\n                    \"Loss mask has to be 2D Tensor of same shape as labels.\"\n                )\n            if torch.any(loss_weights &lt; 0):\n                raise ValueError(\"loss_mask must be \u2265 0.\")\n\n        if not self._vocab_size_validated:\n            logits_vocab_size = logits.shape[-1]\n            if logits_vocab_size != self.vocab_size:\n                raise ValueError(\n                        f\"The current `vocab_size` ({self.vocab_size}) does not match the model's vocab size\"\n                        f\"logit dimension ({logits_vocab_size}). Please check the value.\"\n                    )\n            self._vocab_size_validated = True\n\n    def _prepare_number_token_targets(\n        self, labels: LongTensor, loss_weights: Optional[Tensor], ignore_index: int\n    ) -&gt; Tuple[FloatTensor, Tensor]:\n        \"\"\"\n        Prepare number-token targets and masks.\n\n        Args:\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: Optional 2D Tensor of shape BS x T with loss weight for each token.\n            ignore_index: Label ID to ignore. Defaults to -100.\n\n        Returns:\n            y: 2D Float Tensor of shape BS x T with target numeric values (NaN for non-number tokens).\n            loss_weight: 1D Tensor with a potentially individual loss weight for each number token position.\n        \"\"\"\n        labels = cast(\n            LongTensor, labels.masked_fill(labels == ignore_index, self.nan_id)\n        )\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values.to(device=labels.device)[labels]\n        number_token_positions = ~torch.isnan(y)\n        loss_weights = (\n            loss_weights[number_token_positions]\n            if loss_weights is not None\n            else torch.ones_like(labels, device=labels.device)[number_token_positions]\n        )\n        return cast(FloatTensor, y), loss_weights\n\n    @staticmethod\n    def _apply_reduction(\n        loss: Tensor,\n        reduction: str,\n        loss_weights: Tensor,\n        number_token_positions: Tensor,\n        logits: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Applies the specified reduction type to the calculated loss.\n\n        This method handles 3 types of reduction: \"mean\", \"sum\", and \"none\".\n        For \"mean\" and \"sum\", it applies weighting using `loss_weights`.\n        For \"none\", it reshapes the loss back to the original batch and sequence\n        dimensions.\n\n        Args:\n            loss: 1D Tensor containing the loss for each number token in the batch.\n            reduction: The reduction method (\"mean\", \"sum\", or \"none\").\n            loss_weights: 1D Tensor with a loss weight for each number token.\n            number_token_positions: 2D boolean tensor of shape BS x T indicating\n                the positions of number tokens.\n            logits: 3D Tensor of shape BS x T x V, used to get the original shape\n                for the \"none\" reduction.\n\n        Returns:\n            A Tensor representing the reduced loss:\n                - 0D tensor if `reduction` is \"mean\" or \"sum\".\n                - 2D Tensor of shape BS x T if `reduction` is \"none\".\n        \"\"\"\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), loss_weights.flatten()\n            ) / loss_weights.sum().clamp_min(torch.finfo(loss.dtype).eps)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), loss_weights.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(number_token_positions.numel()).to(loss.device)\n            loss_[number_token_positions.view(-1)] = loss * loss_weights\n            bs, seq_len, _ = logits.size()\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~number_token_positions]) == 0, (\n                \"NumberTokenLoss computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, digit_level: bool = True, reweigh: bool = True)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Standard HF tokenizer.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabilizing training with NTL. Defaults to True. Used for most experiments in the ICML paper.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens.</p> <code>True</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    digit_level: bool = True,\n    reweigh: bool = True,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: Standard HF tokenizer.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabilizing training with NTL. Defaults to True. Used for most\n            experiments in the ICML paper.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n\n    \"\"\"\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.vocab_size = vocab_size if vocab_size is not None else len(self.tokenizer)\n    self._vocab_size_validated = False\n    self.digit_level = digit_level\n    self.reweigh = reweigh\n\n    self.setup_number_tokens()\n\n    self.max_dist = torch.tensor(0.0)\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.setup_number_tokens","title":"<code>setup_number_tokens()</code>","text":"<p>Setting up attributes needed by NT loss</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_number_tokens(self):\n    \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n    # Add digits to vocab if not there yet.\n    vocab_size = len(self.tokenizer)\n    if self.digit_level:\n        new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n    if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n        logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n    vocab = self.tokenizer.get_vocab()\n    self.number_values: FloatTensor = torch.full((self.vocab_size,), float(\"nan\"))\n\n    # Try to convert each token to a float after stripping the space prefix\n    for token, id in vocab.items():\n        if is_number(token, finite=True):\n            if self.digit_level:\n                # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                # Excludes tokens that are numbers in other languages like \u1098 and tokens with space pre-/postfix like ` 2`.\n                if token.isascii() and -1 &lt;= float(token) &lt;= 9 and len(token) == 1:\n                    self.number_values[id] = float(token)\n            else:\n                self.number_values[id] = float(token)\n\n    self.is_number_token = ~torch.isnan(self.number_values)\n    if self.is_number_token.sum() == len(self.is_number_token):\n        raise ValueError(\n            \"At least one token needs to be not a number, otherwise `ignore_index` cannot be set up safely\"\n        )\n    self.nan_id = torch.where(~self.is_number_token)[0][0].item()\n    self.number_values_dense = self.number_values[self.is_number_token]\n\n    if self.digit_level and (num_nts := len(self.number_values_dense)) != 10:\n        logger.error(\n            f\"You requested digit-level but {num_nts} number tokens were identified: {self.number_values_dense}\"\n        )\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Alias to self.forward</p> Source code in <code>ntloss/core.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Alias to self.forward\"\"\"\n    return self.forward(*args, **kwargs)\n</code></pre>"},{"location":"api/#ntloss.core.AbstractNTLoss.reweigh_fn","title":"<code>reweigh_fn(logits: Tensor, loss: Tensor, number_token_positions: Tensor) -&gt; Tensor</code>","text":"<p>Scale the NT loss element-wise using the logit weight on number tokens. NOTE: This reweighing ensures that if ground truth is a number token     but most probability mass is on text tokens, the loss will be higher     than the worst possible number token. This is an edge case in practice.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>loss</code> <code>Tensor</code> <p>1D Tensor over all number tokens in batch.</p> required <code>number_token_positions</code> <code>Tensor</code> <p>2D Tensor of shape BS x T indicating for which tokens the NT loss was computed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D Tensor over all number tokens in batch with the scaled NT losses.</p> Source code in <code>ntloss/core.py</code> <pre><code>def reweigh_fn(\n    self,\n    logits: Tensor,\n    loss: Tensor,\n    number_token_positions: Tensor,\n) -&gt; Tensor:\n    \"\"\"\n    Scale the NT loss element-wise using the logit weight on number tokens.\n    NOTE: This reweighing ensures that if ground truth is a number token\n        but most probability mass is on text tokens, the loss will be *higher*\n        than the worst possible number token. This is an edge case in practice.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        loss: 1D Tensor over all number tokens in batch.\n        number_token_positions: 2D Tensor of shape BS x T indicating for which tokens\n            the NT loss was computed.\n\n    Returns:\n        A 1D Tensor over all number tokens in batch with the scaled NT losses.\n    \"\"\"\n\n    # Take softmax over logits of all tokens in vocab and compute NT logit weight\n    softmax_probs_all = F.softmax(logits, dim=-1)\n    nt_logit_weight = torch.sum(\n        softmax_probs_all[:, :, self.is_number_token], dim=-1\n    )[number_token_positions]\n\n    # Apply weights for NTL element-wise\n    loss *= nt_logit_weight\n\n    # Apply regularization\n    # NOTE: We could consider reweighing here with the max for that label token\n    # rather than the global max\n    loss += (\n        1.01\n        * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n        * (1 - nt_logit_weight)\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct","title":"<code>NTLossDotProduct</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for NT losses that produce a token-wise numerical output.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLossDotProduct(AbstractNTLoss):\n    \"\"\"Class for NT losses that produce a token-wise numerical output.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        loss_function: Callable = F.mse_loss,\n    ):\n        \"\"\"\n        Referred to as NTL-L_p in the paper.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabilizing training with NTL. Defaults to True. Used for most\n                experiments in the ICML paper.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n            loss_function: Function to apply on the delta between the ground truth number\n                and the obtained dot product (nt-probs * token-values). Defaults to\n                MSE, but MAE, Huber etc are also compatible.\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            vocab_size=vocab_size,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n        self.loss_function = loss_function\n        self.setup_max_dist()\n\n    def setup_max_dist(self):\n        \"\"\"\n        Set up the maximum distance between the number tokens based on the selected loss function.\n        \"\"\"\n\n        # Extract the number token values and get the minimum and maximum\n        vals = self.number_values_dense.unsqueeze(0)\n        max_val = vals.max()\n        min_val = vals.min()\n\n        # Compute the largest value the loss function used in NT loss computation can get\n        # Make sure to account for possibility of asymmetrical loss function\n        self.max_dist = torch.maximum(\n            torch.abs(self.loss_function(min_val, max_val)),\n            torch.abs(self.loss_function(max_val, min_val)),\n        )\n\n    def predict_numbers(self, logits: FloatTensor) -&gt; Tuple[FloatTensor, FloatTensor]:\n        \"\"\"\n        Calculates token-level numerical prediction.\n        NOTE: This calculates numerical predictions for *all* tokens, not just where\n        label is a number token.\n\n        Args:\n            logits: 3D FloatTensor of shape BS x T x V.\n\n        Returns:\n            yhat: 2D FloatTensor BS x T containing numerical predictions.\n            nt_mass: 2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.\n        \"\"\"\n        self._validate_inputs(logits, labels=None, loss_weights=None)\n\n        # Calculate the token-level predictions\n        yhat = self._get_dot_product(logits=logits)\n\n        probs_all = F.softmax(logits, dim=-1)\n        probs_nt = probs_all[:, :, self.is_number_token]\n        nt_mass = probs_nt.sum(dim=-1)\n        return yhat, cast(FloatTensor, nt_mass)\n\n    def _get_dot_product(\n        self, logits: FloatTensor, number_token_positions: Optional[BoolTensor] = None\n    ) -&gt; FloatTensor:\n        \"\"\"\n        Applies dot product of number token values and their predicted probabilites.\n\n        Args:\n            logits: 3D FloatTensor of shape BS x T x V.\n            number_token_positions: Optional 2D BoolTensor (BS x T) containing locations\n                of number tokens.\n\n        Returns:\n            If `number_token_positions` is None, 2D FloatTensor of shape BS x T.\n            Otherwise, 1D FloatTensor containing the predictions for the number tokens.\n        \"\"\"\n        # apply softmax solely over the number token indices\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n        values = self.number_values_dense.to(device=logits.device, dtype=logits.dtype)\n\n        # compute the weighted average of number tokens\n        if number_token_positions is None:\n            # Calculate for all tokens\n            yhat = torch.sum(softmax_probs * values, dim=-1)\n        else:\n            # Calculate selectively where labels are number tokens\n            yhat = torch.sum(softmax_probs[number_token_positions] * values, dim=-1)\n        return cast(FloatTensor, yhat)\n\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n            ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n        Returns:\n            Loss tensor\n                OD if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        self._validate_inputs(logits, labels, loss_weights)\n\n        y, loss_weights = self._prepare_number_token_targets(\n            labels, loss_weights, ignore_index\n        )\n        loss_weights = loss_weights.to(logits.dtype)\n        number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n        # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n        if not number_token_positions.any() or not loss_weights.any():\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(\n                    labels, dtype=logits.dtype, device=labels.device\n                )\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        yhat = self._get_dot_product(\n            logits=logits, number_token_positions=number_token_positions\n        )\n\n        # Apply specified loss function to y and yhat\n        loss = self.loss_function(yhat, y[number_token_positions], reduction=\"none\")\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, number_token_positions=number_token_positions\n            )\n\n        loss = self._apply_reduction(\n            loss=loss,\n            reduction=reduction,\n            loss_weights=loss_weights,\n            number_token_positions=number_token_positions,\n            logits=logits,\n        )\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, digit_level: bool = True, reweigh: bool = True, loss_function: Callable = F.mse_loss)</code>","text":"<p>Referred to as NTL-L_p in the paper.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabilizing training with NTL. Defaults to True. Used for most experiments in the ICML paper.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens.</p> <code>True</code> <code>loss_function</code> <code>Callable</code> <p>Function to apply on the delta between the ground truth number and the obtained dot product (nt-probs * token-values). Defaults to MSE, but MAE, Huber etc are also compatible.</p> <code>mse_loss</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    loss_function: Callable = F.mse_loss,\n):\n    \"\"\"\n    Referred to as NTL-L_p in the paper.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabilizing training with NTL. Defaults to True. Used for most\n            experiments in the ICML paper.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n        loss_function: Function to apply on the delta between the ground truth number\n            and the obtained dot product (nt-probs * token-values). Defaults to\n            MSE, but MAE, Huber etc are also compatible.\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        vocab_size=vocab_size,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n    self.loss_function = loss_function\n    self.setup_max_dist()\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.setup_max_dist","title":"<code>setup_max_dist()</code>","text":"<p>Set up the maximum distance between the number tokens based on the selected loss function.</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_max_dist(self):\n    \"\"\"\n    Set up the maximum distance between the number tokens based on the selected loss function.\n    \"\"\"\n\n    # Extract the number token values and get the minimum and maximum\n    vals = self.number_values_dense.unsqueeze(0)\n    max_val = vals.max()\n    min_val = vals.min()\n\n    # Compute the largest value the loss function used in NT loss computation can get\n    # Make sure to account for possibility of asymmetrical loss function\n    self.max_dist = torch.maximum(\n        torch.abs(self.loss_function(min_val, max_val)),\n        torch.abs(self.loss_function(max_val, min_val)),\n    )\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.predict_numbers","title":"<code>predict_numbers(logits: FloatTensor) -&gt; Tuple[FloatTensor, FloatTensor]</code>","text":"<p>Calculates token-level numerical prediction. NOTE: This calculates numerical predictions for all tokens, not just where label is a number token.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D FloatTensor of shape BS x T x V.</p> required <p>Returns:</p> Name Type Description <code>yhat</code> <code>FloatTensor</code> <p>2D FloatTensor BS x T containing numerical predictions.</p> <code>nt_mass</code> <code>FloatTensor</code> <p>2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.</p> Source code in <code>ntloss/core.py</code> <pre><code>def predict_numbers(self, logits: FloatTensor) -&gt; Tuple[FloatTensor, FloatTensor]:\n    \"\"\"\n    Calculates token-level numerical prediction.\n    NOTE: This calculates numerical predictions for *all* tokens, not just where\n    label is a number token.\n\n    Args:\n        logits: 3D FloatTensor of shape BS x T x V.\n\n    Returns:\n        yhat: 2D FloatTensor BS x T containing numerical predictions.\n        nt_mass: 2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.\n    \"\"\"\n    self._validate_inputs(logits, labels=None, loss_weights=None)\n\n    # Calculate the token-level predictions\n    yhat = self._get_dot_product(logits=logits)\n\n    probs_all = F.softmax(logits, dim=-1)\n    probs_nt = probs_all[:, :, self.is_number_token]\n    nt_mass = probs_nt.sum(dim=-1)\n    return yhat, cast(FloatTensor, nt_mass)\n</code></pre>"},{"location":"api/#ntloss.core.NTLossDotProduct.forward","title":"<code>forward(logits: FloatTensor, labels: LongTensor, loss_weights: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>labels</code> <code>LongTensor</code> <p>2D Tensor of shape BS x T.</p> required <code>loss_weights</code> <code>Optional[Tensor]</code> <p>2D Optional tensor of BS x T with token-wise loss weights.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <code>ignore_index</code> <code>int</code> <p>The token ID to ignore in the labels. Defaults to -100.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor OD if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: FloatTensor,\n    labels: LongTensor,\n    loss_weights: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        labels: 2D Tensor of shape BS x T.\n        loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n        ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n    Returns:\n        Loss tensor\n            OD if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    self._validate_inputs(logits, labels, loss_weights)\n\n    y, loss_weights = self._prepare_number_token_targets(\n        labels, loss_weights, ignore_index\n    )\n    loss_weights = loss_weights.to(logits.dtype)\n    number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n    # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n    if not number_token_positions.any() or not loss_weights.any():\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(\n                labels, dtype=logits.dtype, device=labels.device\n            )\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    yhat = self._get_dot_product(\n        logits=logits, number_token_positions=number_token_positions\n    )\n\n    # Apply specified loss function to y and yhat\n    loss = self.loss_function(yhat, y[number_token_positions], reduction=\"none\")\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, number_token_positions=number_token_positions\n        )\n\n    loss = self._apply_reduction(\n        loss=loss,\n        reduction=reduction,\n        loss_weights=loss_weights,\n        number_token_positions=number_token_positions,\n        logits=logits,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss","title":"<code>NTLoss</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for Wasserstein-based NTLoss. This is the default in the ICML paper.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLoss(AbstractNTLoss):\n    \"\"\"Class for Wasserstein-based NTLoss. This is the default in the ICML paper.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        squash_factor: Optional[float] = None,\n    ):\n        \"\"\"\n        NTL constructor for the Wasserstein-based NTLoss.\n\n        Args:\n            tokenizer: Any HuggingFace tokenizer.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabilizing training with NTL. Defaults to True. Used for most\n                experiments in the ICML paper.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n            squash_factor: The optional squashing factor for the NTL. If provided,\n                this number denotes the factor by which predicting the largest number\n                token is worse than predicting the closest incorrect number token.\n                E.g., with digit-level tokenization this factor is 9. Setting this\n                to 1 will recover cross entropy. This argument is intended to handle\n                irregular vocabs with large numerical token values.\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            vocab_size=vocab_size,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n\n        self.squash_factor = squash_factor\n        self.setup_distance_lookup(squash_factor)\n\n    def setup_distance_lookup(\n        self,\n        squash_factor: Optional[float] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set up a lookup table for the distances between the number tokens.\n        Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n        If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n        NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n        Args:\n            squash_factor: The optional squashing factor used.\n        \"\"\"\n\n        # Get token ids for number tokens\n        num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n        # Create mapping from number token ids to their index in order of appearance in vocab:\n        # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n        final_vocab_size = self.number_values.shape[0]\n        vocab_to_dist_idx = torch.full((final_vocab_size,), -1, dtype=torch.long)\n        # Use arange to ensure order of appearance\n        vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n        # Build NxN abs-diff matrix\n        vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n        diff = torch.abs(vals - vals.t())  # (N x N)\n\n        if isinstance(squash_factor, Number):\n            assert squash_factor &gt; 1, (\n                f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n            )\n\n            # Mask out zeros to find the smallest nonzero diff\n            inf = torch.finfo(diff.dtype).max\n            diff_nonzero = diff.masked_fill(diff == 0, inf)\n            global_min_nz = diff_nonzero.min()\n            # Find largest diff\n            global_max = diff.max()\n\n            # Compute scaling factor based on indicated squash factor\n            scale = (squash_factor - 1) / (global_max - global_min_nz)\n            # Scale the absolute differences using scaling factor\n            lookup = 1 + (diff - global_min_nz) * scale\n            lookup[diff == 0] = 0.0\n\n        else:\n            lookup = diff\n\n        self.vocab_to_dist_idx = vocab_to_dist_idx\n        self.dist_lookup = lookup\n        self.max_dist = lookup.max()\n\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: Optional 2D tensor of BS x T with token-specific weights.\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n            ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n        Returns:\n            Loss tensor\n                OD if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n\n        \"\"\"\n        self._validate_inputs(logits, labels, loss_weights)\n\n        y, loss_weights = self._prepare_number_token_targets(\n            labels, loss_weights, ignore_index\n        )\n        loss_weights = loss_weights.to(logits.dtype)\n        number_token_positions = ~torch.isnan(y)\n\n        # If no digit tokens in batch, or total of the relevant loss_weights is zero, no need for upcoming calculations\n        if not number_token_positions.any() or not loss_weights.any():\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(\n                    labels, dtype=logits.dtype, device=labels.device\n                )\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # get distance between the true numbers and all possible number values from lookup table\n        abs_diff = self.dist_lookup.to(dtype=logits.dtype, device=logits.device)[\n            self.vocab_to_dist_idx.to(device=labels.device)[\n                labels[number_token_positions]\n            ]\n        ]\n\n        # loss is the absolute difference weighted by the softmax probs\n        loss = (abs_diff * softmax_probs[number_token_positions]).sum(dim=-1)\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, number_token_positions=number_token_positions\n            )\n\n        loss = self._apply_reduction(\n            loss=loss,\n            reduction=reduction,\n            loss_weights=loss_weights,\n            number_token_positions=number_token_positions,\n            logits=logits,\n        )\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, digit_level: bool = True, reweigh: bool = True, squash_factor: Optional[float] = None)</code>","text":"<p>NTL constructor for the Wasserstein-based NTLoss.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Any HuggingFace tokenizer.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabilizing training with NTL. Defaults to True. Used for most experiments in the ICML paper.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens.</p> <code>True</code> <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor for the NTL. If provided, this number denotes the factor by which predicting the largest number token is worse than predicting the closest incorrect number token. E.g., with digit-level tokenization this factor is 9. Setting this to 1 will recover cross entropy. This argument is intended to handle irregular vocabs with large numerical token values.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    squash_factor: Optional[float] = None,\n):\n    \"\"\"\n    NTL constructor for the Wasserstein-based NTLoss.\n\n    Args:\n        tokenizer: Any HuggingFace tokenizer.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabilizing training with NTL. Defaults to True. Used for most\n            experiments in the ICML paper.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n        squash_factor: The optional squashing factor for the NTL. If provided,\n            this number denotes the factor by which predicting the largest number\n            token is worse than predicting the closest incorrect number token.\n            E.g., with digit-level tokenization this factor is 9. Setting this\n            to 1 will recover cross entropy. This argument is intended to handle\n            irregular vocabs with large numerical token values.\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        vocab_size=vocab_size,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n\n    self.squash_factor = squash_factor\n    self.setup_distance_lookup(squash_factor)\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss.setup_distance_lookup","title":"<code>setup_distance_lookup(squash_factor: Optional[float] = None) -&gt; None</code>","text":"<p>Set up a lookup table for the distances between the number tokens. Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token. If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9. NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</p> <p>Parameters:</p> Name Type Description Default <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor used.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def setup_distance_lookup(\n    self,\n    squash_factor: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Set up a lookup table for the distances between the number tokens.\n    Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n    If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n    NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n    Args:\n        squash_factor: The optional squashing factor used.\n    \"\"\"\n\n    # Get token ids for number tokens\n    num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n    # Create mapping from number token ids to their index in order of appearance in vocab:\n    # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n    final_vocab_size = self.number_values.shape[0]\n    vocab_to_dist_idx = torch.full((final_vocab_size,), -1, dtype=torch.long)\n    # Use arange to ensure order of appearance\n    vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n    # Build NxN abs-diff matrix\n    vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n    diff = torch.abs(vals - vals.t())  # (N x N)\n\n    if isinstance(squash_factor, Number):\n        assert squash_factor &gt; 1, (\n            f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n        )\n\n        # Mask out zeros to find the smallest nonzero diff\n        inf = torch.finfo(diff.dtype).max\n        diff_nonzero = diff.masked_fill(diff == 0, inf)\n        global_min_nz = diff_nonzero.min()\n        # Find largest diff\n        global_max = diff.max()\n\n        # Compute scaling factor based on indicated squash factor\n        scale = (squash_factor - 1) / (global_max - global_min_nz)\n        # Scale the absolute differences using scaling factor\n        lookup = 1 + (diff - global_min_nz) * scale\n        lookup[diff == 0] = 0.0\n\n    else:\n        lookup = diff\n\n    self.vocab_to_dist_idx = vocab_to_dist_idx\n    self.dist_lookup = lookup\n    self.max_dist = lookup.max()\n</code></pre>"},{"location":"api/#ntloss.core.NTLoss.forward","title":"<code>forward(logits: FloatTensor, labels: LongTensor, loss_weights: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>labels</code> <code>LongTensor</code> <p>2D Tensor of shape BS x T.</p> required <code>loss_weights</code> <code>Optional[Tensor]</code> <p>Optional 2D tensor of BS x T with token-specific weights.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <code>ignore_index</code> <code>int</code> <p>The token ID to ignore in the labels. Defaults to -100.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor OD if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: FloatTensor,\n    labels: LongTensor,\n    loss_weights: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        labels: 2D Tensor of shape BS x T.\n        loss_weights: Optional 2D tensor of BS x T with token-specific weights.\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n        ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n    Returns:\n        Loss tensor\n            OD if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n\n    \"\"\"\n    self._validate_inputs(logits, labels, loss_weights)\n\n    y, loss_weights = self._prepare_number_token_targets(\n        labels, loss_weights, ignore_index\n    )\n    loss_weights = loss_weights.to(logits.dtype)\n    number_token_positions = ~torch.isnan(y)\n\n    # If no digit tokens in batch, or total of the relevant loss_weights is zero, no need for upcoming calculations\n    if not number_token_positions.any() or not loss_weights.any():\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(\n                labels, dtype=logits.dtype, device=labels.device\n            )\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # get distance between the true numbers and all possible number values from lookup table\n    abs_diff = self.dist_lookup.to(dtype=logits.dtype, device=logits.device)[\n        self.vocab_to_dist_idx.to(device=labels.device)[\n            labels[number_token_positions]\n        ]\n    ]\n\n    # loss is the absolute difference weighted by the softmax probs\n    loss = (abs_diff * softmax_probs[number_token_positions]).sum(dim=-1)\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, number_token_positions=number_token_positions\n        )\n\n    loss = self._apply_reduction(\n        loss=loss,\n        reduction=reduction,\n        loss_weights=loss_weights,\n        number_token_positions=number_token_positions,\n        logits=logits,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#ntloss.core.NumberLevelLoss","title":"<code>NumberLevelLoss</code>","text":"<p>               Bases: <code>NTLossDotProduct</code></p> <p>Class to calculate NTL on a per-number (rather than per-token) basis.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NumberLevelLoss(NTLossDotProduct):\n    \"\"\"Class to calculate NTL on a per-number (rather than per-token) basis.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        float_level: bool = False,\n        reweigh: bool = True,\n    ):\n        \"\"\"\n        NTL constructor for the number-level NTLoss.\n\n        Args:\n            tokenizer: Any HuggingFace tokenizer.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            float_level: Whether to calculate the loss for every float or every\n                integer in the sequence. For `12.34`, if float_level=False, two\n                loss terms will be calculated, respectively for `12` and `34`.\n                If float_level=True, a single `.` does not break the contiguity\n                of the identified number. Defaults to False.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n                Using this will explode the NL-NTL in the current implementation,\n                so reweighing for the NL-NTL needs to be refined.\n\n        \"\"\"\n        # digit_level must be set to True.\n        super().__init__(\n            tokenizer=tokenizer,\n            vocab_size=vocab_size,\n            digit_level=True,\n            reweigh=reweigh,\n            loss_function=F.l1_loss,  # unused\n        )\n        self.float_level = float_level\n        self.dot = self.tokenizer.convert_tokens_to_ids(\".\")\n\n    def setup_max_dist(self):\n        \"\"\"\n        Due to the MAPE loss calculation, the max dist is limited to 1.0\n        \"\"\"\n        self.max_dist = torch.tensor(1.0)\n\n    def convert_digits_to_numbers(\n        self,\n        y: FloatTensor,\n        yhat: FloatTensor,\n        number_token_positions: BoolTensor,\n        labels: LongTensor,\n    ):\n        \"\"\"\n        Set up the order mask for the batch and convert digit-level number tokens to numerical values.\n\n        Args:\n            y: 2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).\n            yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level\n                (includes predictions for non-number tokens).\n            number_token_positions: 2D BoolTensor (BS x T) containing locations of number tokens at digit-level.\n            labels: 2D LongTensor of shape BS x T with the target input IDs.\n\n        Returns:\n            y: 2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).\n            yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level\n                (includes predictions for non-number tokens).\n            number_token_positions: 2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.\n        \"\"\"\n\n        # Set up empty order_mask: will store power with which to scale digits\n        order_mask = torch.zeros_like(y, dtype=yhat.dtype, device=y.device)\n\n        # Extract numbers using number blocks\n        for i in range(y.shape[0]):\n            # For every item in batch: assume not starting with number block\n            in_number_block = False\n            end_digit = -1\n\n            # Loop from end of sequence to beginning to extract numbers\n            for j in range(y.shape[1] - 1, -1, -1):\n                # Already in number block and a digit: increase order magnitude\n                if in_number_block and number_token_positions[i, j]:\n                    if not self.float_level or labels[i, j + 1] != self.dot:\n                        previous_order_index = j + 1\n                    else:\n                        previous_order_index = j + 2\n                    order_mask[i, j] = order_mask[i, previous_order_index] + 1\n\n                # Not in number block: first instance of number = end digit\n                elif number_token_positions[i, j]:\n                    in_number_block = True\n                    end_digit = j + 1\n\n                # A dot can be considered part of a number if self.float_level\n                elif (\n                    in_number_block\n                    and self.float_level\n                    and labels[i, j] == self.dot\n                    and labels[i, j + 1] != self.dot\n                ):\n                    # exp(-inf) = 0, thus, the dot does not contribute to the GT number calculation\n                    order_mask[i, j] = -torch.inf\n                    # Necessary to avoid having NaN when summing\n                    y[i, j] = 0\n                    yhat[i, j] = 0\n\n                # In number block, but not a digit: end of number_block\n                elif in_number_block:\n                    in_number_block = False\n\n                    # Reuse y and yhat tensors to store full numbers\n                    y[i, j + 1] = torch.sum(\n                        y[i, j + 1 : end_digit]\n                        * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                    )\n                    # Make sure non-relevant numerical values are turned into NaN\n                    # This indicates non-number tokens\n                    y[i, j + 2 : end_digit] = y[i, j]\n                    yhat[i, j + 1] = torch.sum(\n                        yhat[i, j + 1 : end_digit]\n                        * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                    )\n\n        # Update mask with locations of number tokens\n        number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n        return y, yhat, number_token_positions\n\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n            ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        self._validate_inputs(logits, labels, loss_weights)\n\n        y, _ = self._prepare_number_token_targets(labels, loss_weights, ignore_index)\n        number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n        # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n        if not number_token_positions.any() or (\n            loss_weights is not None and not loss_weights.any()\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(labels, dtype=logits.dtype)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        yhat = self._get_dot_product(logits=logits)\n\n        y, yhat, number_token_positions = self.convert_digits_to_numbers(\n            y, yhat, number_token_positions, labels\n        )\n        if loss_weights is None:\n            loss_weights = torch.ones_like(labels, dtype=logits.dtype)\n        loss_weights = loss_weights[number_token_positions]\n\n        # NOTE: Alternative could be to apply specified loss function to normalized yhat\n        # loss = self.loss_function(torch.div(\n        #     yhat[number_token_positions],\n        #     y[number_token_positions].clamp_min(torch.finfo(y.dtype).eps),\n        # ), torch.ones_like(yhat), reduction=\"none\")\n\n        y_num = y[number_token_positions]\n        yh_num = yhat[number_token_positions]\n        # Calculate symmetric MAPE which is bounded in [0, 1]\n        loss = (yh_num - y_num).abs() / (\n            yh_num.abs() + y_num.abs() + torch.finfo(y.dtype).eps\n        )\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, number_token_positions=number_token_positions\n            )\n\n        loss = self._apply_reduction(\n            loss=loss,\n            reduction=reduction,\n            loss_weights=loss_weights,\n            number_token_positions=number_token_positions,\n            logits=logits,\n        )\n\n        return loss\n</code></pre>"},{"location":"api/#ntloss.core.NumberLevelLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, float_level: bool = False, reweigh: bool = True)</code>","text":"<p>NTL constructor for the number-level NTLoss.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Any HuggingFace tokenizer.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>float_level</code> <code>bool</code> <p>Whether to calculate the loss for every float or every integer in the sequence. For <code>12.34</code>, if float_level=False, two loss terms will be calculated, respectively for <code>12</code> and <code>34</code>. If float_level=True, a single <code>.</code> does not break the contiguity of the identified number. Defaults to False.</p> <code>False</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens. Using this will explode the NL-NTL in the current implementation, so reweighing for the NL-NTL needs to be refined.</p> <code>True</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    float_level: bool = False,\n    reweigh: bool = True,\n):\n    \"\"\"\n    NTL constructor for the number-level NTLoss.\n\n    Args:\n        tokenizer: Any HuggingFace tokenizer.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        float_level: Whether to calculate the loss for every float or every\n            integer in the sequence. For `12.34`, if float_level=False, two\n            loss terms will be calculated, respectively for `12` and `34`.\n            If float_level=True, a single `.` does not break the contiguity\n            of the identified number. Defaults to False.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n            Using this will explode the NL-NTL in the current implementation,\n            so reweighing for the NL-NTL needs to be refined.\n\n    \"\"\"\n    # digit_level must be set to True.\n    super().__init__(\n        tokenizer=tokenizer,\n        vocab_size=vocab_size,\n        digit_level=True,\n        reweigh=reweigh,\n        loss_function=F.l1_loss,  # unused\n    )\n    self.float_level = float_level\n    self.dot = self.tokenizer.convert_tokens_to_ids(\".\")\n</code></pre>"},{"location":"api/#ntloss.core.NumberLevelLoss.setup_max_dist","title":"<code>setup_max_dist()</code>","text":"<p>Due to the MAPE loss calculation, the max dist is limited to 1.0</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_max_dist(self):\n    \"\"\"\n    Due to the MAPE loss calculation, the max dist is limited to 1.0\n    \"\"\"\n    self.max_dist = torch.tensor(1.0)\n</code></pre>"},{"location":"api/#ntloss.core.NumberLevelLoss.convert_digits_to_numbers","title":"<code>convert_digits_to_numbers(y: FloatTensor, yhat: FloatTensor, number_token_positions: BoolTensor, labels: LongTensor)</code>","text":"<p>Set up the order mask for the batch and convert digit-level number tokens to numerical values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>FloatTensor</code> <p>2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).</p> required <code>yhat</code> <code>FloatTensor</code> <p>2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level (includes predictions for non-number tokens).</p> required <code>number_token_positions</code> <code>BoolTensor</code> <p>2D BoolTensor (BS x T) containing locations of number tokens at digit-level.</p> required <code>labels</code> <code>LongTensor</code> <p>2D LongTensor of shape BS x T with the target input IDs.</p> required <p>Returns:</p> Name Type Description <code>y</code> <p>2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).</p> <code>yhat</code> <p>2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level (includes predictions for non-number tokens).</p> <code>number_token_positions</code> <p>2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.</p> Source code in <code>ntloss/core.py</code> <pre><code>def convert_digits_to_numbers(\n    self,\n    y: FloatTensor,\n    yhat: FloatTensor,\n    number_token_positions: BoolTensor,\n    labels: LongTensor,\n):\n    \"\"\"\n    Set up the order mask for the batch and convert digit-level number tokens to numerical values.\n\n    Args:\n        y: 2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).\n        yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level\n            (includes predictions for non-number tokens).\n        number_token_positions: 2D BoolTensor (BS x T) containing locations of number tokens at digit-level.\n        labels: 2D LongTensor of shape BS x T with the target input IDs.\n\n    Returns:\n        y: 2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).\n        yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level\n            (includes predictions for non-number tokens).\n        number_token_positions: 2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.\n    \"\"\"\n\n    # Set up empty order_mask: will store power with which to scale digits\n    order_mask = torch.zeros_like(y, dtype=yhat.dtype, device=y.device)\n\n    # Extract numbers using number blocks\n    for i in range(y.shape[0]):\n        # For every item in batch: assume not starting with number block\n        in_number_block = False\n        end_digit = -1\n\n        # Loop from end of sequence to beginning to extract numbers\n        for j in range(y.shape[1] - 1, -1, -1):\n            # Already in number block and a digit: increase order magnitude\n            if in_number_block and number_token_positions[i, j]:\n                if not self.float_level or labels[i, j + 1] != self.dot:\n                    previous_order_index = j + 1\n                else:\n                    previous_order_index = j + 2\n                order_mask[i, j] = order_mask[i, previous_order_index] + 1\n\n            # Not in number block: first instance of number = end digit\n            elif number_token_positions[i, j]:\n                in_number_block = True\n                end_digit = j + 1\n\n            # A dot can be considered part of a number if self.float_level\n            elif (\n                in_number_block\n                and self.float_level\n                and labels[i, j] == self.dot\n                and labels[i, j + 1] != self.dot\n            ):\n                # exp(-inf) = 0, thus, the dot does not contribute to the GT number calculation\n                order_mask[i, j] = -torch.inf\n                # Necessary to avoid having NaN when summing\n                y[i, j] = 0\n                yhat[i, j] = 0\n\n            # In number block, but not a digit: end of number_block\n            elif in_number_block:\n                in_number_block = False\n\n                # Reuse y and yhat tensors to store full numbers\n                y[i, j + 1] = torch.sum(\n                    y[i, j + 1 : end_digit]\n                    * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                )\n                # Make sure non-relevant numerical values are turned into NaN\n                # This indicates non-number tokens\n                y[i, j + 2 : end_digit] = y[i, j]\n                yhat[i, j + 1] = torch.sum(\n                    yhat[i, j + 1 : end_digit]\n                    * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                )\n\n    # Update mask with locations of number tokens\n    number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n    return y, yhat, number_token_positions\n</code></pre>"},{"location":"api/#ntloss.core.NumberLevelLoss.forward","title":"<code>forward(logits: FloatTensor, labels: LongTensor, loss_weights: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>labels</code> <code>LongTensor</code> <p>2D Tensor of shape BS x T.</p> required <code>loss_weights</code> <code>Optional[Tensor]</code> <p>2D Optional tensor of BS x T with token-wise loss weights.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <code>ignore_index</code> <code>int</code> <p>The token ID to ignore in the labels. Defaults to -100.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: FloatTensor,\n    labels: LongTensor,\n    loss_weights: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        labels: 2D Tensor of shape BS x T.\n        loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n        ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    self._validate_inputs(logits, labels, loss_weights)\n\n    y, _ = self._prepare_number_token_targets(labels, loss_weights, ignore_index)\n    number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n    # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n    if not number_token_positions.any() or (\n        loss_weights is not None and not loss_weights.any()\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(labels, dtype=logits.dtype)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    yhat = self._get_dot_product(logits=logits)\n\n    y, yhat, number_token_positions = self.convert_digits_to_numbers(\n        y, yhat, number_token_positions, labels\n    )\n    if loss_weights is None:\n        loss_weights = torch.ones_like(labels, dtype=logits.dtype)\n    loss_weights = loss_weights[number_token_positions]\n\n    # NOTE: Alternative could be to apply specified loss function to normalized yhat\n    # loss = self.loss_function(torch.div(\n    #     yhat[number_token_positions],\n    #     y[number_token_positions].clamp_min(torch.finfo(y.dtype).eps),\n    # ), torch.ones_like(yhat), reduction=\"none\")\n\n    y_num = y[number_token_positions]\n    yh_num = yhat[number_token_positions]\n    # Calculate symmetric MAPE which is bounded in [0, 1]\n    loss = (yh_num - y_num).abs() / (\n        yh_num.abs() + y_num.abs() + torch.finfo(y.dtype).eps\n    )\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, number_token_positions=number_token_positions\n        )\n\n    loss = self._apply_reduction(\n        loss=loss,\n        reduction=reduction,\n        loss_weights=loss_weights,\n        number_token_positions=number_token_positions,\n        logits=logits,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#ntlossutils","title":"ntloss.utils","text":""},{"location":"api/#ntloss.utils","title":"<code>ntloss.utils</code>","text":""},{"location":"api/#ntloss.utils.is_number","title":"<code>is_number(something: Any, finite: bool = True) -&gt; bool</code>","text":"<p>Check whether something is convertible to a float</p> <p>Parameters:</p> Name Type Description Default <code>something</code> <code>Any</code> <p>something to test for float casting.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not it's a number</p> Source code in <code>ntloss/utils.py</code> <pre><code>def is_number(something: Any, finite: bool = True) -&gt; bool:\n    \"\"\"Check whether something is convertible to a float\n\n    Args:\n        something: something to test for float casting.\n\n    Returns:\n        Whether or not it's a number\n    \"\"\"\n    try:\n        f = float(something)\n        if finite and not math.isfinite(f):\n            return False\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api/core/","title":"ntloss.core","text":""},{"location":"api/core/#ntloss.core","title":"<code>ntloss.core</code>","text":""},{"location":"api/core/#ntloss.core.AbstractNTLoss","title":"<code>AbstractNTLoss</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>ntloss/core.py</code> <pre><code>class AbstractNTLoss(ABC):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        digit_level: bool = True,\n        reweigh: bool = True,\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: Standard HF tokenizer.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabilizing training with NTL. Defaults to True. Used for most\n                experiments in the ICML paper.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size if vocab_size is not None else len(self.tokenizer)\n        self._vocab_size_validated = False\n        self.digit_level = digit_level\n        self.reweigh = reweigh\n\n        self.setup_number_tokens()\n\n        self.max_dist = torch.tensor(0.0)\n\n    def setup_number_tokens(self):\n        \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n        # Add digits to vocab if not there yet.\n        vocab_size = len(self.tokenizer)\n        if self.digit_level:\n            new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n        if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n            logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n        vocab = self.tokenizer.get_vocab()\n        self.number_values: FloatTensor = torch.full((self.vocab_size,), float(\"nan\"))\n\n        # Try to convert each token to a float after stripping the space prefix\n        for token, id in vocab.items():\n            if is_number(token, finite=True):\n                if self.digit_level:\n                    # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                    # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                    # Excludes tokens that are numbers in other languages like \u1098 and tokens with space pre-/postfix like ` 2`.\n                    if token.isascii() and -1 &lt;= float(token) &lt;= 9 and len(token) == 1:\n                        self.number_values[id] = float(token)\n                else:\n                    self.number_values[id] = float(token)\n\n        self.is_number_token = ~torch.isnan(self.number_values)\n        if self.is_number_token.sum() == len(self.is_number_token):\n            raise ValueError(\n                \"At least one token needs to be not a number, otherwise `ignore_index` cannot be set up safely\"\n            )\n        self.nan_id = torch.where(~self.is_number_token)[0][0].item()\n        self.number_values_dense = self.number_values[self.is_number_token]\n\n        if self.digit_level and (num_nts := len(self.number_values_dense)) != 10:\n            logger.error(\n                f\"You requested digit-level but {num_nts} number tokens were identified: {self.number_values_dense}\"\n            )\n\n    @abstractmethod\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor: ...\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Alias to self.forward\"\"\"\n        return self.forward(*args, **kwargs)\n\n    def reweigh_fn(\n        self,\n        logits: Tensor,\n        loss: Tensor,\n        number_token_positions: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Scale the NT loss element-wise using the logit weight on number tokens.\n        NOTE: This reweighing ensures that if ground truth is a number token\n            but most probability mass is on text tokens, the loss will be *higher*\n            than the worst possible number token. This is an edge case in practice.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            loss: 1D Tensor over all number tokens in batch.\n            number_token_positions: 2D Tensor of shape BS x T indicating for which tokens\n                the NT loss was computed.\n\n        Returns:\n            A 1D Tensor over all number tokens in batch with the scaled NT losses.\n        \"\"\"\n\n        # Take softmax over logits of all tokens in vocab and compute NT logit weight\n        softmax_probs_all = F.softmax(logits, dim=-1)\n        nt_logit_weight = torch.sum(\n            softmax_probs_all[:, :, self.is_number_token], dim=-1\n        )[number_token_positions]\n\n        # Apply weights for NTL element-wise\n        loss *= nt_logit_weight\n\n        # Apply regularization\n        # NOTE: We could consider reweighing here with the max for that label token\n        # rather than the global max\n        loss += (\n            1.01\n            * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n            * (1 - nt_logit_weight)\n        )\n\n        return loss\n\n    def _validate_inputs(\n        self,\n        logits: FloatTensor,\n        labels: Optional[LongTensor],\n        loss_weights: Optional[Tensor],\n    ):\n        \"\"\"Private method to perform size and type checks.\"\"\"\n        if (td := len(logits.shape)) != 3 or (ne := logits.numel()) == 0:\n            raise ValueError(\n                f\"Logits have to be non-empty 3D Tensor, not {td}D with {ne} elements\"\n            )\n        if not torch.is_floating_point(logits):\n            raise TypeError(\"Logits have to be FloatTensor.\")\n        if labels is None:\n            return\n        if not labels.dtype == torch.long:\n            raise TypeError(f\"Labels have to be LongTensor, not {type(labels)}\")\n        if (b := labels.shape) != (a := logits.shape[:-1]):\n            raise ValueError(\n                f\"Logit and label sizes of first 2 dims have to match: {a} vs {b}\"\n            )\n\n        if (td := len(labels.shape)) != 2 or (ne := labels.numel()) == 0:\n            raise ValueError(\n                f\"Labels have to be non-empty 2D Tensor, not {td}D with {ne} elements\"\n            )\n        if loss_weights is not None:\n            if loss_weights.shape != labels.shape:\n                raise ValueError(\n                    \"Loss mask has to be 2D Tensor of same shape as labels.\"\n                )\n            if torch.any(loss_weights &lt; 0):\n                raise ValueError(\"loss_mask must be \u2265 0.\")\n\n        if not self._vocab_size_validated:\n            logits_vocab_size = logits.shape[-1]\n            if logits_vocab_size != self.vocab_size:\n                raise ValueError(\n                        f\"The current `vocab_size` ({self.vocab_size}) does not match the model's vocab size\"\n                        f\"logit dimension ({logits_vocab_size}). Please check the value.\"\n                    )\n            self._vocab_size_validated = True\n\n    def _prepare_number_token_targets(\n        self, labels: LongTensor, loss_weights: Optional[Tensor], ignore_index: int\n    ) -&gt; Tuple[FloatTensor, Tensor]:\n        \"\"\"\n        Prepare number-token targets and masks.\n\n        Args:\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: Optional 2D Tensor of shape BS x T with loss weight for each token.\n            ignore_index: Label ID to ignore. Defaults to -100.\n\n        Returns:\n            y: 2D Float Tensor of shape BS x T with target numeric values (NaN for non-number tokens).\n            loss_weight: 1D Tensor with a potentially individual loss weight for each number token position.\n        \"\"\"\n        labels = cast(\n            LongTensor, labels.masked_fill(labels == ignore_index, self.nan_id)\n        )\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values.to(device=labels.device)[labels]\n        number_token_positions = ~torch.isnan(y)\n        loss_weights = (\n            loss_weights[number_token_positions]\n            if loss_weights is not None\n            else torch.ones_like(labels, device=labels.device)[number_token_positions]\n        )\n        return cast(FloatTensor, y), loss_weights\n\n    @staticmethod\n    def _apply_reduction(\n        loss: Tensor,\n        reduction: str,\n        loss_weights: Tensor,\n        number_token_positions: Tensor,\n        logits: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"\n        Applies the specified reduction type to the calculated loss.\n\n        This method handles 3 types of reduction: \"mean\", \"sum\", and \"none\".\n        For \"mean\" and \"sum\", it applies weighting using `loss_weights`.\n        For \"none\", it reshapes the loss back to the original batch and sequence\n        dimensions.\n\n        Args:\n            loss: 1D Tensor containing the loss for each number token in the batch.\n            reduction: The reduction method (\"mean\", \"sum\", or \"none\").\n            loss_weights: 1D Tensor with a loss weight for each number token.\n            number_token_positions: 2D boolean tensor of shape BS x T indicating\n                the positions of number tokens.\n            logits: 3D Tensor of shape BS x T x V, used to get the original shape\n                for the \"none\" reduction.\n\n        Returns:\n            A Tensor representing the reduced loss:\n                - 0D tensor if `reduction` is \"mean\" or \"sum\".\n                - 2D Tensor of shape BS x T if `reduction` is \"none\".\n        \"\"\"\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), loss_weights.flatten()\n            ) / loss_weights.sum().clamp_min(torch.finfo(loss.dtype).eps)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), loss_weights.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(number_token_positions.numel()).to(loss.device)\n            loss_[number_token_positions.view(-1)] = loss * loss_weights\n            bs, seq_len, _ = logits.size()\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~number_token_positions]) == 0, (\n                \"NumberTokenLoss computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, digit_level: bool = True, reweigh: bool = True)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Standard HF tokenizer.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabilizing training with NTL. Defaults to True. Used for most experiments in the ICML paper.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens.</p> <code>True</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    digit_level: bool = True,\n    reweigh: bool = True,\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: Standard HF tokenizer.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabilizing training with NTL. Defaults to True. Used for most\n            experiments in the ICML paper.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n\n    \"\"\"\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.vocab_size = vocab_size if vocab_size is not None else len(self.tokenizer)\n    self._vocab_size_validated = False\n    self.digit_level = digit_level\n    self.reweigh = reweigh\n\n    self.setup_number_tokens()\n\n    self.max_dist = torch.tensor(0.0)\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.setup_number_tokens","title":"<code>setup_number_tokens()</code>","text":"<p>Setting up attributes needed by NT loss</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_number_tokens(self):\n    \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n    # Add digits to vocab if not there yet.\n    vocab_size = len(self.tokenizer)\n    if self.digit_level:\n        new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n    if vocab_size &lt; len(self.tokenizer) and new_tokens &gt; 0:\n        logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n    vocab = self.tokenizer.get_vocab()\n    self.number_values: FloatTensor = torch.full((self.vocab_size,), float(\"nan\"))\n\n    # Try to convert each token to a float after stripping the space prefix\n    for token, id in vocab.items():\n        if is_number(token, finite=True):\n            if self.digit_level:\n                # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                # Excludes tokens that are numbers in other languages like \u1098 and tokens with space pre-/postfix like ` 2`.\n                if token.isascii() and -1 &lt;= float(token) &lt;= 9 and len(token) == 1:\n                    self.number_values[id] = float(token)\n            else:\n                self.number_values[id] = float(token)\n\n    self.is_number_token = ~torch.isnan(self.number_values)\n    if self.is_number_token.sum() == len(self.is_number_token):\n        raise ValueError(\n            \"At least one token needs to be not a number, otherwise `ignore_index` cannot be set up safely\"\n        )\n    self.nan_id = torch.where(~self.is_number_token)[0][0].item()\n    self.number_values_dense = self.number_values[self.is_number_token]\n\n    if self.digit_level and (num_nts := len(self.number_values_dense)) != 10:\n        logger.error(\n            f\"You requested digit-level but {num_nts} number tokens were identified: {self.number_values_dense}\"\n        )\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Alias to self.forward</p> Source code in <code>ntloss/core.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Alias to self.forward\"\"\"\n    return self.forward(*args, **kwargs)\n</code></pre>"},{"location":"api/core/#ntloss.core.AbstractNTLoss.reweigh_fn","title":"<code>reweigh_fn(logits: Tensor, loss: Tensor, number_token_positions: Tensor) -&gt; Tensor</code>","text":"<p>Scale the NT loss element-wise using the logit weight on number tokens. NOTE: This reweighing ensures that if ground truth is a number token     but most probability mass is on text tokens, the loss will be higher     than the worst possible number token. This is an edge case in practice.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>loss</code> <code>Tensor</code> <p>1D Tensor over all number tokens in batch.</p> required <code>number_token_positions</code> <code>Tensor</code> <p>2D Tensor of shape BS x T indicating for which tokens the NT loss was computed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D Tensor over all number tokens in batch with the scaled NT losses.</p> Source code in <code>ntloss/core.py</code> <pre><code>def reweigh_fn(\n    self,\n    logits: Tensor,\n    loss: Tensor,\n    number_token_positions: Tensor,\n) -&gt; Tensor:\n    \"\"\"\n    Scale the NT loss element-wise using the logit weight on number tokens.\n    NOTE: This reweighing ensures that if ground truth is a number token\n        but most probability mass is on text tokens, the loss will be *higher*\n        than the worst possible number token. This is an edge case in practice.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        loss: 1D Tensor over all number tokens in batch.\n        number_token_positions: 2D Tensor of shape BS x T indicating for which tokens\n            the NT loss was computed.\n\n    Returns:\n        A 1D Tensor over all number tokens in batch with the scaled NT losses.\n    \"\"\"\n\n    # Take softmax over logits of all tokens in vocab and compute NT logit weight\n    softmax_probs_all = F.softmax(logits, dim=-1)\n    nt_logit_weight = torch.sum(\n        softmax_probs_all[:, :, self.is_number_token], dim=-1\n    )[number_token_positions]\n\n    # Apply weights for NTL element-wise\n    loss *= nt_logit_weight\n\n    # Apply regularization\n    # NOTE: We could consider reweighing here with the max for that label token\n    # rather than the global max\n    loss += (\n        1.01\n        * self.max_dist.to(dtype=loss.dtype, device=loss.device)\n        * (1 - nt_logit_weight)\n    )\n\n    return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct","title":"<code>NTLossDotProduct</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for NT losses that produce a token-wise numerical output.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLossDotProduct(AbstractNTLoss):\n    \"\"\"Class for NT losses that produce a token-wise numerical output.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        loss_function: Callable = F.mse_loss,\n    ):\n        \"\"\"\n        Referred to as NTL-L_p in the paper.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabilizing training with NTL. Defaults to True. Used for most\n                experiments in the ICML paper.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n            loss_function: Function to apply on the delta between the ground truth number\n                and the obtained dot product (nt-probs * token-values). Defaults to\n                MSE, but MAE, Huber etc are also compatible.\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            vocab_size=vocab_size,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n        self.loss_function = loss_function\n        self.setup_max_dist()\n\n    def setup_max_dist(self):\n        \"\"\"\n        Set up the maximum distance between the number tokens based on the selected loss function.\n        \"\"\"\n\n        # Extract the number token values and get the minimum and maximum\n        vals = self.number_values_dense.unsqueeze(0)\n        max_val = vals.max()\n        min_val = vals.min()\n\n        # Compute the largest value the loss function used in NT loss computation can get\n        # Make sure to account for possibility of asymmetrical loss function\n        self.max_dist = torch.maximum(\n            torch.abs(self.loss_function(min_val, max_val)),\n            torch.abs(self.loss_function(max_val, min_val)),\n        )\n\n    def predict_numbers(self, logits: FloatTensor) -&gt; Tuple[FloatTensor, FloatTensor]:\n        \"\"\"\n        Calculates token-level numerical prediction.\n        NOTE: This calculates numerical predictions for *all* tokens, not just where\n        label is a number token.\n\n        Args:\n            logits: 3D FloatTensor of shape BS x T x V.\n\n        Returns:\n            yhat: 2D FloatTensor BS x T containing numerical predictions.\n            nt_mass: 2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.\n        \"\"\"\n        self._validate_inputs(logits, labels=None, loss_weights=None)\n\n        # Calculate the token-level predictions\n        yhat = self._get_dot_product(logits=logits)\n\n        probs_all = F.softmax(logits, dim=-1)\n        probs_nt = probs_all[:, :, self.is_number_token]\n        nt_mass = probs_nt.sum(dim=-1)\n        return yhat, cast(FloatTensor, nt_mass)\n\n    def _get_dot_product(\n        self, logits: FloatTensor, number_token_positions: Optional[BoolTensor] = None\n    ) -&gt; FloatTensor:\n        \"\"\"\n        Applies dot product of number token values and their predicted probabilites.\n\n        Args:\n            logits: 3D FloatTensor of shape BS x T x V.\n            number_token_positions: Optional 2D BoolTensor (BS x T) containing locations\n                of number tokens.\n\n        Returns:\n            If `number_token_positions` is None, 2D FloatTensor of shape BS x T.\n            Otherwise, 1D FloatTensor containing the predictions for the number tokens.\n        \"\"\"\n        # apply softmax solely over the number token indices\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n        values = self.number_values_dense.to(device=logits.device, dtype=logits.dtype)\n\n        # compute the weighted average of number tokens\n        if number_token_positions is None:\n            # Calculate for all tokens\n            yhat = torch.sum(softmax_probs * values, dim=-1)\n        else:\n            # Calculate selectively where labels are number tokens\n            yhat = torch.sum(softmax_probs[number_token_positions] * values, dim=-1)\n        return cast(FloatTensor, yhat)\n\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n            ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n        Returns:\n            Loss tensor\n                OD if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        self._validate_inputs(logits, labels, loss_weights)\n\n        y, loss_weights = self._prepare_number_token_targets(\n            labels, loss_weights, ignore_index\n        )\n        loss_weights = loss_weights.to(logits.dtype)\n        number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n        # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n        if not number_token_positions.any() or not loss_weights.any():\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(\n                    labels, dtype=logits.dtype, device=labels.device\n                )\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        yhat = self._get_dot_product(\n            logits=logits, number_token_positions=number_token_positions\n        )\n\n        # Apply specified loss function to y and yhat\n        loss = self.loss_function(yhat, y[number_token_positions], reduction=\"none\")\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, number_token_positions=number_token_positions\n            )\n\n        loss = self._apply_reduction(\n            loss=loss,\n            reduction=reduction,\n            loss_weights=loss_weights,\n            number_token_positions=number_token_positions,\n            logits=logits,\n        )\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, digit_level: bool = True, reweigh: bool = True, loss_function: Callable = F.mse_loss)</code>","text":"<p>Referred to as NTL-L_p in the paper.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabilizing training with NTL. Defaults to True. Used for most experiments in the ICML paper.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens.</p> <code>True</code> <code>loss_function</code> <code>Callable</code> <p>Function to apply on the delta between the ground truth number and the obtained dot product (nt-probs * token-values). Defaults to MSE, but MAE, Huber etc are also compatible.</p> <code>mse_loss</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    loss_function: Callable = F.mse_loss,\n):\n    \"\"\"\n    Referred to as NTL-L_p in the paper.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabilizing training with NTL. Defaults to True. Used for most\n            experiments in the ICML paper.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n        loss_function: Function to apply on the delta between the ground truth number\n            and the obtained dot product (nt-probs * token-values). Defaults to\n            MSE, but MAE, Huber etc are also compatible.\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        vocab_size=vocab_size,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n    self.loss_function = loss_function\n    self.setup_max_dist()\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.setup_max_dist","title":"<code>setup_max_dist()</code>","text":"<p>Set up the maximum distance between the number tokens based on the selected loss function.</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_max_dist(self):\n    \"\"\"\n    Set up the maximum distance between the number tokens based on the selected loss function.\n    \"\"\"\n\n    # Extract the number token values and get the minimum and maximum\n    vals = self.number_values_dense.unsqueeze(0)\n    max_val = vals.max()\n    min_val = vals.min()\n\n    # Compute the largest value the loss function used in NT loss computation can get\n    # Make sure to account for possibility of asymmetrical loss function\n    self.max_dist = torch.maximum(\n        torch.abs(self.loss_function(min_val, max_val)),\n        torch.abs(self.loss_function(max_val, min_val)),\n    )\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.predict_numbers","title":"<code>predict_numbers(logits: FloatTensor) -&gt; Tuple[FloatTensor, FloatTensor]</code>","text":"<p>Calculates token-level numerical prediction. NOTE: This calculates numerical predictions for all tokens, not just where label is a number token.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D FloatTensor of shape BS x T x V.</p> required <p>Returns:</p> Name Type Description <code>yhat</code> <code>FloatTensor</code> <p>2D FloatTensor BS x T containing numerical predictions.</p> <code>nt_mass</code> <code>FloatTensor</code> <p>2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.</p> Source code in <code>ntloss/core.py</code> <pre><code>def predict_numbers(self, logits: FloatTensor) -&gt; Tuple[FloatTensor, FloatTensor]:\n    \"\"\"\n    Calculates token-level numerical prediction.\n    NOTE: This calculates numerical predictions for *all* tokens, not just where\n    label is a number token.\n\n    Args:\n        logits: 3D FloatTensor of shape BS x T x V.\n\n    Returns:\n        yhat: 2D FloatTensor BS x T containing numerical predictions.\n        nt_mass: 2D FloatTensor BS x T with the cumulated mass assigned to all number tokens.\n    \"\"\"\n    self._validate_inputs(logits, labels=None, loss_weights=None)\n\n    # Calculate the token-level predictions\n    yhat = self._get_dot_product(logits=logits)\n\n    probs_all = F.softmax(logits, dim=-1)\n    probs_nt = probs_all[:, :, self.is_number_token]\n    nt_mass = probs_nt.sum(dim=-1)\n    return yhat, cast(FloatTensor, nt_mass)\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLossDotProduct.forward","title":"<code>forward(logits: FloatTensor, labels: LongTensor, loss_weights: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>labels</code> <code>LongTensor</code> <p>2D Tensor of shape BS x T.</p> required <code>loss_weights</code> <code>Optional[Tensor]</code> <p>2D Optional tensor of BS x T with token-wise loss weights.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <code>ignore_index</code> <code>int</code> <p>The token ID to ignore in the labels. Defaults to -100.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor OD if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: FloatTensor,\n    labels: LongTensor,\n    loss_weights: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        labels: 2D Tensor of shape BS x T.\n        loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n        ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n    Returns:\n        Loss tensor\n            OD if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    self._validate_inputs(logits, labels, loss_weights)\n\n    y, loss_weights = self._prepare_number_token_targets(\n        labels, loss_weights, ignore_index\n    )\n    loss_weights = loss_weights.to(logits.dtype)\n    number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n    # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n    if not number_token_positions.any() or not loss_weights.any():\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(\n                labels, dtype=logits.dtype, device=labels.device\n            )\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    yhat = self._get_dot_product(\n        logits=logits, number_token_positions=number_token_positions\n    )\n\n    # Apply specified loss function to y and yhat\n    loss = self.loss_function(yhat, y[number_token_positions], reduction=\"none\")\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, number_token_positions=number_token_positions\n        )\n\n    loss = self._apply_reduction(\n        loss=loss,\n        reduction=reduction,\n        loss_weights=loss_weights,\n        number_token_positions=number_token_positions,\n        logits=logits,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss","title":"<code>NTLoss</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for Wasserstein-based NTLoss. This is the default in the ICML paper.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLoss(AbstractNTLoss):\n    \"\"\"Class for Wasserstein-based NTLoss. This is the default in the ICML paper.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        digit_level: bool = True,\n        reweigh: bool = True,\n        squash_factor: Optional[float] = None,\n    ):\n        \"\"\"\n        NTL constructor for the Wasserstein-based NTLoss.\n\n        Args:\n            tokenizer: Any HuggingFace tokenizer.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            digit_level: Whether to ensure only digits are considered number tokens,\n                stabilizing training with NTL. Defaults to True. Used for most\n                experiments in the ICML paper.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n            squash_factor: The optional squashing factor for the NTL. If provided,\n                this number denotes the factor by which predicting the largest number\n                token is worse than predicting the closest incorrect number token.\n                E.g., with digit-level tokenization this factor is 9. Setting this\n                to 1 will recover cross entropy. This argument is intended to handle\n                irregular vocabs with large numerical token values.\n        \"\"\"\n        super().__init__(\n            tokenizer=tokenizer,\n            vocab_size=vocab_size,\n            digit_level=digit_level,\n            reweigh=reweigh,\n        )\n\n        self.squash_factor = squash_factor\n        self.setup_distance_lookup(squash_factor)\n\n    def setup_distance_lookup(\n        self,\n        squash_factor: Optional[float] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set up a lookup table for the distances between the number tokens.\n        Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n        If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n        NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n        Args:\n            squash_factor: The optional squashing factor used.\n        \"\"\"\n\n        # Get token ids for number tokens\n        num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n        # Create mapping from number token ids to their index in order of appearance in vocab:\n        # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n        final_vocab_size = self.number_values.shape[0]\n        vocab_to_dist_idx = torch.full((final_vocab_size,), -1, dtype=torch.long)\n        # Use arange to ensure order of appearance\n        vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n        # Build NxN abs-diff matrix\n        vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n        diff = torch.abs(vals - vals.t())  # (N x N)\n\n        if isinstance(squash_factor, Number):\n            assert squash_factor &gt; 1, (\n                f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n            )\n\n            # Mask out zeros to find the smallest nonzero diff\n            inf = torch.finfo(diff.dtype).max\n            diff_nonzero = diff.masked_fill(diff == 0, inf)\n            global_min_nz = diff_nonzero.min()\n            # Find largest diff\n            global_max = diff.max()\n\n            # Compute scaling factor based on indicated squash factor\n            scale = (squash_factor - 1) / (global_max - global_min_nz)\n            # Scale the absolute differences using scaling factor\n            lookup = 1 + (diff - global_min_nz) * scale\n            lookup[diff == 0] = 0.0\n\n        else:\n            lookup = diff\n\n        self.vocab_to_dist_idx = vocab_to_dist_idx\n        self.dist_lookup = lookup\n        self.max_dist = lookup.max()\n\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: Optional 2D tensor of BS x T with token-specific weights.\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n            ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n        Returns:\n            Loss tensor\n                OD if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n\n        \"\"\"\n        self._validate_inputs(logits, labels, loss_weights)\n\n        y, loss_weights = self._prepare_number_token_targets(\n            labels, loss_weights, ignore_index\n        )\n        loss_weights = loss_weights.to(logits.dtype)\n        number_token_positions = ~torch.isnan(y)\n\n        # If no digit tokens in batch, or total of the relevant loss_weights is zero, no need for upcoming calculations\n        if not number_token_positions.any() or not loss_weights.any():\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(\n                    labels, dtype=logits.dtype, device=labels.device\n                )\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # get distance between the true numbers and all possible number values from lookup table\n        abs_diff = self.dist_lookup.to(dtype=logits.dtype, device=logits.device)[\n            self.vocab_to_dist_idx.to(device=labels.device)[\n                labels[number_token_positions]\n            ]\n        ]\n\n        # loss is the absolute difference weighted by the softmax probs\n        loss = (abs_diff * softmax_probs[number_token_positions]).sum(dim=-1)\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, number_token_positions=number_token_positions\n            )\n\n        loss = self._apply_reduction(\n            loss=loss,\n            reduction=reduction,\n            loss_weights=loss_weights,\n            number_token_positions=number_token_positions,\n            logits=logits,\n        )\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, digit_level: bool = True, reweigh: bool = True, squash_factor: Optional[float] = None)</code>","text":"<p>NTL constructor for the Wasserstein-based NTLoss.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Any HuggingFace tokenizer.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>digit_level</code> <code>bool</code> <p>Whether to ensure only digits are considered number tokens, stabilizing training with NTL. Defaults to True. Used for most experiments in the ICML paper.</p> <code>True</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens.</p> <code>True</code> <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor for the NTL. If provided, this number denotes the factor by which predicting the largest number token is worse than predicting the closest incorrect number token. E.g., with digit-level tokenization this factor is 9. Setting this to 1 will recover cross entropy. This argument is intended to handle irregular vocabs with large numerical token values.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    digit_level: bool = True,\n    reweigh: bool = True,\n    squash_factor: Optional[float] = None,\n):\n    \"\"\"\n    NTL constructor for the Wasserstein-based NTLoss.\n\n    Args:\n        tokenizer: Any HuggingFace tokenizer.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        digit_level: Whether to ensure only digits are considered number tokens,\n            stabilizing training with NTL. Defaults to True. Used for most\n            experiments in the ICML paper.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n        squash_factor: The optional squashing factor for the NTL. If provided,\n            this number denotes the factor by which predicting the largest number\n            token is worse than predicting the closest incorrect number token.\n            E.g., with digit-level tokenization this factor is 9. Setting this\n            to 1 will recover cross entropy. This argument is intended to handle\n            irregular vocabs with large numerical token values.\n    \"\"\"\n    super().__init__(\n        tokenizer=tokenizer,\n        vocab_size=vocab_size,\n        digit_level=digit_level,\n        reweigh=reweigh,\n    )\n\n    self.squash_factor = squash_factor\n    self.setup_distance_lookup(squash_factor)\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss.setup_distance_lookup","title":"<code>setup_distance_lookup(squash_factor: Optional[float] = None) -&gt; None</code>","text":"<p>Set up a lookup table for the distances between the number tokens. Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token. If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9. NOTE: With a squashing factor of 1, this basically collapses to cross entropy.</p> <p>Parameters:</p> Name Type Description Default <code>squash_factor</code> <code>Optional[float]</code> <p>The optional squashing factor used.</p> <code>None</code> Source code in <code>ntloss/core.py</code> <pre><code>def setup_distance_lookup(\n    self,\n    squash_factor: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Set up a lookup table for the distances between the number tokens.\n    Use squash_factor to control by what factor the farthest number token is worse than the closest, incorrect number token.\n    If not squash_factor is not set: with 10 number tokens (0-9), the squashing factor is 9.\n    NOTE: With a squashing factor of 1, this basically collapses to cross entropy.\n\n    Args:\n        squash_factor: The optional squashing factor used.\n    \"\"\"\n\n    # Get token ids for number tokens\n    num_ids = torch.nonzero(self.is_number_token, as_tuple=True)[0]\n    # Create mapping from number token ids to their index in order of appearance in vocab:\n    # e.g. token \"3\" -&gt; id 519 -&gt; dist_idx 1, then abs dist to 3 for other NT values will be found in row/column 1\n    final_vocab_size = self.number_values.shape[0]\n    vocab_to_dist_idx = torch.full((final_vocab_size,), -1, dtype=torch.long)\n    # Use arange to ensure order of appearance\n    vocab_to_dist_idx[num_ids] = torch.arange(num_ids.size(0), dtype=torch.long)\n\n    # Build NxN abs-diff matrix\n    vals = self.number_values_dense.unsqueeze(0)  # (1 x N)\n    diff = torch.abs(vals - vals.t())  # (N x N)\n\n    if isinstance(squash_factor, Number):\n        assert squash_factor &gt; 1, (\n            f\"The squash factor can't be equal to or smaller than 1, please use a different squashing factor than {squash_factor}\"\n        )\n\n        # Mask out zeros to find the smallest nonzero diff\n        inf = torch.finfo(diff.dtype).max\n        diff_nonzero = diff.masked_fill(diff == 0, inf)\n        global_min_nz = diff_nonzero.min()\n        # Find largest diff\n        global_max = diff.max()\n\n        # Compute scaling factor based on indicated squash factor\n        scale = (squash_factor - 1) / (global_max - global_min_nz)\n        # Scale the absolute differences using scaling factor\n        lookup = 1 + (diff - global_min_nz) * scale\n        lookup[diff == 0] = 0.0\n\n    else:\n        lookup = diff\n\n    self.vocab_to_dist_idx = vocab_to_dist_idx\n    self.dist_lookup = lookup\n    self.max_dist = lookup.max()\n</code></pre>"},{"location":"api/core/#ntloss.core.NTLoss.forward","title":"<code>forward(logits: FloatTensor, labels: LongTensor, loss_weights: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>labels</code> <code>LongTensor</code> <p>2D Tensor of shape BS x T.</p> required <code>loss_weights</code> <code>Optional[Tensor]</code> <p>Optional 2D tensor of BS x T with token-specific weights.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <code>ignore_index</code> <code>int</code> <p>The token ID to ignore in the labels. Defaults to -100.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor OD if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: FloatTensor,\n    labels: LongTensor,\n    loss_weights: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        labels: 2D Tensor of shape BS x T.\n        loss_weights: Optional 2D tensor of BS x T with token-specific weights.\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n        ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n    Returns:\n        Loss tensor\n            OD if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n\n    \"\"\"\n    self._validate_inputs(logits, labels, loss_weights)\n\n    y, loss_weights = self._prepare_number_token_targets(\n        labels, loss_weights, ignore_index\n    )\n    loss_weights = loss_weights.to(logits.dtype)\n    number_token_positions = ~torch.isnan(y)\n\n    # If no digit tokens in batch, or total of the relevant loss_weights is zero, no need for upcoming calculations\n    if not number_token_positions.any() or not loss_weights.any():\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(\n                labels, dtype=logits.dtype, device=labels.device\n            )\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # get distance between the true numbers and all possible number values from lookup table\n    abs_diff = self.dist_lookup.to(dtype=logits.dtype, device=logits.device)[\n        self.vocab_to_dist_idx.to(device=labels.device)[\n            labels[number_token_positions]\n        ]\n    ]\n\n    # loss is the absolute difference weighted by the softmax probs\n    loss = (abs_diff * softmax_probs[number_token_positions]).sum(dim=-1)\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, number_token_positions=number_token_positions\n        )\n\n    loss = self._apply_reduction(\n        loss=loss,\n        reduction=reduction,\n        loss_weights=loss_weights,\n        number_token_positions=number_token_positions,\n        logits=logits,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NumberLevelLoss","title":"<code>NumberLevelLoss</code>","text":"<p>               Bases: <code>NTLossDotProduct</code></p> <p>Class to calculate NTL on a per-number (rather than per-token) basis.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NumberLevelLoss(NTLossDotProduct):\n    \"\"\"Class to calculate NTL on a per-number (rather than per-token) basis.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizer,\n        vocab_size: Optional[int] = None,\n        float_level: bool = False,\n        reweigh: bool = True,\n    ):\n        \"\"\"\n        NTL constructor for the number-level NTLoss.\n\n        Args:\n            tokenizer: Any HuggingFace tokenizer.\n            vocab_size: Optional user-provided vocab size. If not provided, the\n                tokenizer's vocab size is used.\n            float_level: Whether to calculate the loss for every float or every\n                integer in the sequence. For `12.34`, if float_level=False, two\n                loss terms will be calculated, respectively for `12` and `34`.\n                If float_level=True, a single `.` does not break the contiguity\n                of the identified number. Defaults to False.\n            reweigh: Whether to scale the NTL using the logit weight on\n                number tokens. Defaults to True.\n                NOTE: The ICML paper does *not* use this option which can lead to\n                incorrect loss if most mass is placed outside of the number tokens.\n                Using this will explode the NL-NTL in the current implementation,\n                so reweighing for the NL-NTL needs to be refined.\n\n        \"\"\"\n        # digit_level must be set to True.\n        super().__init__(\n            tokenizer=tokenizer,\n            vocab_size=vocab_size,\n            digit_level=True,\n            reweigh=reweigh,\n            loss_function=F.l1_loss,  # unused\n        )\n        self.float_level = float_level\n        self.dot = self.tokenizer.convert_tokens_to_ids(\".\")\n\n    def setup_max_dist(self):\n        \"\"\"\n        Due to the MAPE loss calculation, the max dist is limited to 1.0\n        \"\"\"\n        self.max_dist = torch.tensor(1.0)\n\n    def convert_digits_to_numbers(\n        self,\n        y: FloatTensor,\n        yhat: FloatTensor,\n        number_token_positions: BoolTensor,\n        labels: LongTensor,\n    ):\n        \"\"\"\n        Set up the order mask for the batch and convert digit-level number tokens to numerical values.\n\n        Args:\n            y: 2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).\n            yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level\n                (includes predictions for non-number tokens).\n            number_token_positions: 2D BoolTensor (BS x T) containing locations of number tokens at digit-level.\n            labels: 2D LongTensor of shape BS x T with the target input IDs.\n\n        Returns:\n            y: 2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).\n            yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level\n                (includes predictions for non-number tokens).\n            number_token_positions: 2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.\n        \"\"\"\n\n        # Set up empty order_mask: will store power with which to scale digits\n        order_mask = torch.zeros_like(y, dtype=yhat.dtype, device=y.device)\n\n        # Extract numbers using number blocks\n        for i in range(y.shape[0]):\n            # For every item in batch: assume not starting with number block\n            in_number_block = False\n            end_digit = -1\n\n            # Loop from end of sequence to beginning to extract numbers\n            for j in range(y.shape[1] - 1, -1, -1):\n                # Already in number block and a digit: increase order magnitude\n                if in_number_block and number_token_positions[i, j]:\n                    if not self.float_level or labels[i, j + 1] != self.dot:\n                        previous_order_index = j + 1\n                    else:\n                        previous_order_index = j + 2\n                    order_mask[i, j] = order_mask[i, previous_order_index] + 1\n\n                # Not in number block: first instance of number = end digit\n                elif number_token_positions[i, j]:\n                    in_number_block = True\n                    end_digit = j + 1\n\n                # A dot can be considered part of a number if self.float_level\n                elif (\n                    in_number_block\n                    and self.float_level\n                    and labels[i, j] == self.dot\n                    and labels[i, j + 1] != self.dot\n                ):\n                    # exp(-inf) = 0, thus, the dot does not contribute to the GT number calculation\n                    order_mask[i, j] = -torch.inf\n                    # Necessary to avoid having NaN when summing\n                    y[i, j] = 0\n                    yhat[i, j] = 0\n\n                # In number block, but not a digit: end of number_block\n                elif in_number_block:\n                    in_number_block = False\n\n                    # Reuse y and yhat tensors to store full numbers\n                    y[i, j + 1] = torch.sum(\n                        y[i, j + 1 : end_digit]\n                        * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                    )\n                    # Make sure non-relevant numerical values are turned into NaN\n                    # This indicates non-number tokens\n                    y[i, j + 2 : end_digit] = y[i, j]\n                    yhat[i, j + 1] = torch.sum(\n                        yhat[i, j + 1 : end_digit]\n                        * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                    )\n\n        # Update mask with locations of number tokens\n        number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n        return y, yhat, number_token_positions\n\n    def forward(\n        self,\n        logits: FloatTensor,\n        labels: LongTensor,\n        loss_weights: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: 3D Tensor of shape BS x T x V.\n            labels: 2D Tensor of shape BS x T.\n            loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n            ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        self._validate_inputs(logits, labels, loss_weights)\n\n        y, _ = self._prepare_number_token_targets(labels, loss_weights, ignore_index)\n        number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n        # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n        if not number_token_positions.any() or (\n            loss_weights is not None and not loss_weights.any()\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(labels, dtype=logits.dtype)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        yhat = self._get_dot_product(logits=logits)\n\n        y, yhat, number_token_positions = self.convert_digits_to_numbers(\n            y, yhat, number_token_positions, labels\n        )\n        if loss_weights is None:\n            loss_weights = torch.ones_like(labels, dtype=logits.dtype)\n        loss_weights = loss_weights[number_token_positions]\n\n        # NOTE: Alternative could be to apply specified loss function to normalized yhat\n        # loss = self.loss_function(torch.div(\n        #     yhat[number_token_positions],\n        #     y[number_token_positions].clamp_min(torch.finfo(y.dtype).eps),\n        # ), torch.ones_like(yhat), reduction=\"none\")\n\n        y_num = y[number_token_positions]\n        yh_num = yhat[number_token_positions]\n        # Calculate symmetric MAPE which is bounded in [0, 1]\n        loss = (yh_num - y_num).abs() / (\n            yh_num.abs() + y_num.abs() + torch.finfo(y.dtype).eps\n        )\n\n        # If reweigh: compute weights for NTL based on logits\n        if self.reweigh:\n            loss = self.reweigh_fn(\n                logits=logits, loss=loss, number_token_positions=number_token_positions\n            )\n\n        loss = self._apply_reduction(\n            loss=loss,\n            reduction=reduction,\n            loss_weights=loss_weights,\n            number_token_positions=number_token_positions,\n            logits=logits,\n        )\n\n        return loss\n</code></pre>"},{"location":"api/core/#ntloss.core.NumberLevelLoss.__init__","title":"<code>__init__(tokenizer: PreTrainedTokenizer, vocab_size: Optional[int] = None, float_level: bool = False, reweigh: bool = True)</code>","text":"<p>NTL constructor for the number-level NTLoss.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Any HuggingFace tokenizer.</p> required <code>vocab_size</code> <code>Optional[int]</code> <p>Optional user-provided vocab size. If not provided, the tokenizer's vocab size is used.</p> <code>None</code> <code>float_level</code> <code>bool</code> <p>Whether to calculate the loss for every float or every integer in the sequence. For <code>12.34</code>, if float_level=False, two loss terms will be calculated, respectively for <code>12</code> and <code>34</code>. If float_level=True, a single <code>.</code> does not break the contiguity of the identified number. Defaults to False.</p> <code>False</code> <code>reweigh</code> <code>bool</code> <p>Whether to scale the NTL using the logit weight on number tokens. Defaults to True. NOTE: The ICML paper does not use this option which can lead to incorrect loss if most mass is placed outside of the number tokens. Using this will explode the NL-NTL in the current implementation, so reweighing for the NL-NTL needs to be refined.</p> <code>True</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    vocab_size: Optional[int] = None,\n    float_level: bool = False,\n    reweigh: bool = True,\n):\n    \"\"\"\n    NTL constructor for the number-level NTLoss.\n\n    Args:\n        tokenizer: Any HuggingFace tokenizer.\n        vocab_size: Optional user-provided vocab size. If not provided, the\n            tokenizer's vocab size is used.\n        float_level: Whether to calculate the loss for every float or every\n            integer in the sequence. For `12.34`, if float_level=False, two\n            loss terms will be calculated, respectively for `12` and `34`.\n            If float_level=True, a single `.` does not break the contiguity\n            of the identified number. Defaults to False.\n        reweigh: Whether to scale the NTL using the logit weight on\n            number tokens. Defaults to True.\n            NOTE: The ICML paper does *not* use this option which can lead to\n            incorrect loss if most mass is placed outside of the number tokens.\n            Using this will explode the NL-NTL in the current implementation,\n            so reweighing for the NL-NTL needs to be refined.\n\n    \"\"\"\n    # digit_level must be set to True.\n    super().__init__(\n        tokenizer=tokenizer,\n        vocab_size=vocab_size,\n        digit_level=True,\n        reweigh=reweigh,\n        loss_function=F.l1_loss,  # unused\n    )\n    self.float_level = float_level\n    self.dot = self.tokenizer.convert_tokens_to_ids(\".\")\n</code></pre>"},{"location":"api/core/#ntloss.core.NumberLevelLoss.setup_max_dist","title":"<code>setup_max_dist()</code>","text":"<p>Due to the MAPE loss calculation, the max dist is limited to 1.0</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_max_dist(self):\n    \"\"\"\n    Due to the MAPE loss calculation, the max dist is limited to 1.0\n    \"\"\"\n    self.max_dist = torch.tensor(1.0)\n</code></pre>"},{"location":"api/core/#ntloss.core.NumberLevelLoss.convert_digits_to_numbers","title":"<code>convert_digits_to_numbers(y: FloatTensor, yhat: FloatTensor, number_token_positions: BoolTensor, labels: LongTensor)</code>","text":"<p>Set up the order mask for the batch and convert digit-level number tokens to numerical values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>FloatTensor</code> <p>2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).</p> required <code>yhat</code> <code>FloatTensor</code> <p>2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level (includes predictions for non-number tokens).</p> required <code>number_token_positions</code> <code>BoolTensor</code> <p>2D BoolTensor (BS x T) containing locations of number tokens at digit-level.</p> required <code>labels</code> <code>LongTensor</code> <p>2D LongTensor of shape BS x T with the target input IDs.</p> required <p>Returns:</p> Name Type Description <code>y</code> <p>2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).</p> <code>yhat</code> <p>2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level (includes predictions for non-number tokens).</p> <code>number_token_positions</code> <p>2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.</p> Source code in <code>ntloss/core.py</code> <pre><code>def convert_digits_to_numbers(\n    self,\n    y: FloatTensor,\n    yhat: FloatTensor,\n    number_token_positions: BoolTensor,\n    labels: LongTensor,\n):\n    \"\"\"\n    Set up the order mask for the batch and convert digit-level number tokens to numerical values.\n\n    Args:\n        y: 2D FloatTensor of shape BS x T with target numerical values at digit-level (NaN for non-number tokens).\n        yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at digit-level\n            (includes predictions for non-number tokens).\n        number_token_positions: 2D BoolTensor (BS x T) containing locations of number tokens at digit-level.\n        labels: 2D LongTensor of shape BS x T with the target input IDs.\n\n    Returns:\n        y: 2D FloatTensor of shape BS x T with target numerical values at number-level (NaN for non-number tokens).\n        yhat: 2D FloatTensor of shape BS x T containing the predictions for the number tokens at number-level\n            (includes predictions for non-number tokens).\n        number_token_positions: 2D BoolTensor (BS x T) containing locations of numerical values in y and yhat.\n    \"\"\"\n\n    # Set up empty order_mask: will store power with which to scale digits\n    order_mask = torch.zeros_like(y, dtype=yhat.dtype, device=y.device)\n\n    # Extract numbers using number blocks\n    for i in range(y.shape[0]):\n        # For every item in batch: assume not starting with number block\n        in_number_block = False\n        end_digit = -1\n\n        # Loop from end of sequence to beginning to extract numbers\n        for j in range(y.shape[1] - 1, -1, -1):\n            # Already in number block and a digit: increase order magnitude\n            if in_number_block and number_token_positions[i, j]:\n                if not self.float_level or labels[i, j + 1] != self.dot:\n                    previous_order_index = j + 1\n                else:\n                    previous_order_index = j + 2\n                order_mask[i, j] = order_mask[i, previous_order_index] + 1\n\n            # Not in number block: first instance of number = end digit\n            elif number_token_positions[i, j]:\n                in_number_block = True\n                end_digit = j + 1\n\n            # A dot can be considered part of a number if self.float_level\n            elif (\n                in_number_block\n                and self.float_level\n                and labels[i, j] == self.dot\n                and labels[i, j + 1] != self.dot\n            ):\n                # exp(-inf) = 0, thus, the dot does not contribute to the GT number calculation\n                order_mask[i, j] = -torch.inf\n                # Necessary to avoid having NaN when summing\n                y[i, j] = 0\n                yhat[i, j] = 0\n\n            # In number block, but not a digit: end of number_block\n            elif in_number_block:\n                in_number_block = False\n\n                # Reuse y and yhat tensors to store full numbers\n                y[i, j + 1] = torch.sum(\n                    y[i, j + 1 : end_digit]\n                    * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                )\n                # Make sure non-relevant numerical values are turned into NaN\n                # This indicates non-number tokens\n                y[i, j + 2 : end_digit] = y[i, j]\n                yhat[i, j + 1] = torch.sum(\n                    yhat[i, j + 1 : end_digit]\n                    * torch.pow(10, order_mask[i, j + 1 : end_digit])\n                )\n\n    # Update mask with locations of number tokens\n    number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n    return y, yhat, number_token_positions\n</code></pre>"},{"location":"api/core/#ntloss.core.NumberLevelLoss.forward","title":"<code>forward(logits: FloatTensor, labels: LongTensor, loss_weights: Optional[Tensor] = None, reduction: str = 'mean', ignore_index: int = -100) -&gt; Tensor</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>FloatTensor</code> <p>3D Tensor of shape BS x T x V.</p> required <code>labels</code> <code>LongTensor</code> <p>2D Tensor of shape BS x T.</p> required <code>loss_weights</code> <code>Optional[Tensor]</code> <p>2D Optional tensor of BS x T with token-wise loss weights.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <code>ignore_index</code> <code>int</code> <p>The token ID to ignore in the labels. Defaults to -100.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: FloatTensor,\n    labels: LongTensor,\n    loss_weights: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: 3D Tensor of shape BS x T x V.\n        labels: 2D Tensor of shape BS x T.\n        loss_weights: 2D Optional tensor of BS x T with token-wise loss weights.\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n        ignore_index: The token ID to ignore in the labels. Defaults to -100.\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    self._validate_inputs(logits, labels, loss_weights)\n\n    y, _ = self._prepare_number_token_targets(labels, loss_weights, ignore_index)\n    number_token_positions = cast(BoolTensor, ~torch.isnan(y))\n\n    # If no digit tokens in batch, or total of the relevant loss weights is zero, no need for upcoming calculations\n    if not number_token_positions.any() or (\n        loss_weights is not None and not loss_weights.any()\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=logits.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(labels, dtype=logits.dtype)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    yhat = self._get_dot_product(logits=logits)\n\n    y, yhat, number_token_positions = self.convert_digits_to_numbers(\n        y, yhat, number_token_positions, labels\n    )\n    if loss_weights is None:\n        loss_weights = torch.ones_like(labels, dtype=logits.dtype)\n    loss_weights = loss_weights[number_token_positions]\n\n    # NOTE: Alternative could be to apply specified loss function to normalized yhat\n    # loss = self.loss_function(torch.div(\n    #     yhat[number_token_positions],\n    #     y[number_token_positions].clamp_min(torch.finfo(y.dtype).eps),\n    # ), torch.ones_like(yhat), reduction=\"none\")\n\n    y_num = y[number_token_positions]\n    yh_num = yhat[number_token_positions]\n    # Calculate symmetric MAPE which is bounded in [0, 1]\n    loss = (yh_num - y_num).abs() / (\n        yh_num.abs() + y_num.abs() + torch.finfo(y.dtype).eps\n    )\n\n    # If reweigh: compute weights for NTL based on logits\n    if self.reweigh:\n        loss = self.reweigh_fn(\n            logits=logits, loss=loss, number_token_positions=number_token_positions\n        )\n\n    loss = self._apply_reduction(\n        loss=loss,\n        reduction=reduction,\n        loss_weights=loss_weights,\n        number_token_positions=number_token_positions,\n        logits=logits,\n    )\n\n    return loss\n</code></pre>"},{"location":"api/utils/","title":"ntloss.utils","text":""},{"location":"api/utils/#ntloss.utils","title":"<code>ntloss.utils</code>","text":""},{"location":"api/utils/#ntloss.utils.is_number","title":"<code>is_number(something: Any, finite: bool = True) -&gt; bool</code>","text":"<p>Check whether something is convertible to a float</p> <p>Parameters:</p> Name Type Description Default <code>something</code> <code>Any</code> <p>something to test for float casting.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not it's a number</p> Source code in <code>ntloss/utils.py</code> <pre><code>def is_number(something: Any, finite: bool = True) -&gt; bool:\n    \"\"\"Check whether something is convertible to a float\n\n    Args:\n        something: something to test for float casting.\n\n    Returns:\n        Whether or not it's a number\n    \"\"\"\n    try:\n        f = float(something)\n        if finite and not math.isfinite(f):\n            return False\n        return True\n    except ValueError:\n        return False\n</code></pre>"}]}