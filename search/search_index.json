{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Number Token Loss","text":"<p>A regression-like loss that improves numerical reasoning in language models. Originally presented in \u201cRegress, Don\u2019t Guess\u201d (ICML 2025).</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Install from PyPI:</p> <pre><code>uv add ntloss\npip install ntloss # if you are oldschool\n</code></pre> <p>Use like this: <pre><code>from ntloss import NTLoss\nntl_fn = NTLoss(tokenizer=tokenizer)\nntl = ntl_fn(logits, labels)\n\n# We recommend\ntotal_loss = cross_entropy(logits, labels) + 0.3 * ntl\n</code></pre></p> <p><code>ntloss</code> is currently in alpha phase and pre-release. Feedback &amp; PRs are very welcome.</p>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use <code>ntloss</code>, please cite our paper:</p> <pre><code>@inproceedings{zausinger2025regress,\n  title   = {Regress, Don't Guess \u2013 A Regression-like Loss on Number Tokens for Language Models},\n  author  = {Jonas Zausinger and Lars Pennig and Anamarija Kozina and Sean Sdahl\n             and Julian Sikora and Adrian Dendorfer and Timofey Kuznetsov\n             and Mohamad Hagog and Nina Wiedemann and Kacper Chlodny\n             and Vincent Limbach and Anna Ketteler and Thorben Prein\n             and Vishwa Mohan Singh and Michael Danziger and Jannis Born},\n  booktitle = {Proc. of the 42nd International Conference on Machine Learning (ICML)},\n  year    = {2025},\n  url     = {https://tum-ai.github.io/number-token-loss/}\n}\n</code></pre>"},{"location":"reference/ntloss/","title":"ntloss","text":""},{"location":"reference/ntloss/#ntloss.core.AbstractNTLoss","title":"<code>AbstractNTLoss</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>ntloss/core.py</code> <pre><code>class AbstractNTLoss(ABC):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: standard HF tokenizer\n\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.setup_number_tokens()\n\n    def setup_number_tokens(self):\n        \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n        # Add digits to vocab if not there yet.\n        new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n        if new_tokens &gt; 0:\n            logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n        vocab = self.tokenizer.get_vocab()\n        self.number_values = torch.full((len(vocab),), float(\"nan\"))\n\n        # Try to convert each token to a float after stripping the space prefix\n        for token, id in vocab.items():\n            if is_number(token, finite=True):\n                # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n                # This stabilizes training with NTL. Can be altered though, see paper experiments.\n                if -1 &lt;= float(token) &lt;= 9 and len(token.lstrip(\" \")) == 1:\n                    self.number_values[id] = float(token)\n\n        self.is_number_token = ~torch.isnan(self.number_values)\n        self.number_values_dense = self.number_values[self.is_number_token]\n\n    @abstractmethod\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor: ...\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"Alias to self.forward\"\"\"\n        return self.forward(*args, **kwargs)\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.AbstractNTLoss.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Alias to self.forward</p> Source code in <code>ntloss/core.py</code> <pre><code>def __call__(self, *args, **kwargs):\n    \"\"\"Alias to self.forward\"\"\"\n    return self.forward(*args, **kwargs)\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.AbstractNTLoss.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>standard HF tokenizer</p> required Source code in <code>ntloss/core.py</code> <pre><code>def __init__(self, tokenizer: PreTrainedTokenizer):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: standard HF tokenizer\n\n    \"\"\"\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.setup_number_tokens()\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.AbstractNTLoss.setup_number_tokens","title":"<code>setup_number_tokens()</code>","text":"<p>Setting up attributes needed by NT loss</p> Source code in <code>ntloss/core.py</code> <pre><code>def setup_number_tokens(self):\n    \"\"\"Setting up attributes needed by NT loss\"\"\"\n\n    # Add digits to vocab if not there yet.\n    new_tokens = self.tokenizer.add_tokens(list(map(str, range(10))))\n    if new_tokens &gt; 0:\n        logger.warning(f\"Added {new_tokens} new tokens for number token loss\")\n    vocab = self.tokenizer.get_vocab()\n    self.number_values = torch.full((len(vocab),), float(\"nan\"))\n\n    # Try to convert each token to a float after stripping the space prefix\n    for token, id in vocab.items():\n        if is_number(token, finite=True):\n            # NOTE: This check ensures number token value only occurs for digits, not for multi-digit numbers (123)\n            # This stabilizes training with NTL. Can be altered though, see paper experiments.\n            if -1 &lt;= float(token) &lt;= 9 and len(token.lstrip(\" \")) == 1:\n                self.number_values[id] = float(token)\n\n    self.is_number_token = ~torch.isnan(self.number_values)\n    self.number_values_dense = self.number_values[self.is_number_token]\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.NTLoss","title":"<code>NTLoss</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for Wasserstein-based NTLoss. This is the default as per our paper.</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLoss(AbstractNTLoss):\n    \"\"\"Class for Wasserstein-based NTLoss. This is the default as per our paper.\"\"\"\n\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n        ignore_index: int = -100,\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            labels: Tensor of shape BS x T\n            loss_mask: Optional tensor of BS x T\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n\n        \"\"\"\n\n        if logits.numel() == 0:\n            raise ValueError(\"Logits passed to the NumberTokenLoss are empty!\")\n        if labels.numel() == 0:\n            raise ValueError(\"Labels passed to the NumberTokenLoss are empty!\")\n\n        labels = labels.clone().masked_fill(labels == ignore_index, 0)\n\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values[labels]\n        valid_positions = ~torch.isnan(y)\n\n        # Apply the loss_mask to lower importance of number tokens before the final answer\n        label_mask = (\n            loss_mask[valid_positions]\n            if loss_mask is not None\n            else torch.ones(y.size(), dtype=logits.dtype, device=labels.device)[\n                valid_positions\n            ]\n        )\n\n        # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n        if (torch.count_nonzero(valid_positions) == 0) or (\n            torch.count_nonzero(label_mask) == 0\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=labels.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(valid_positions)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        bs, seq_len, _ = logits.size()\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # compute absolute difference between the true numbers and all possible number values\n        abs_diff = torch.abs(\n            y[valid_positions].unsqueeze(-1) - self.number_values_dense\n        )\n\n        # loss is the absolute difference weighted by the softmax probs\n        loss = (abs_diff * softmax_probs[valid_positions]).sum(dim=-1)\n\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), label_mask.flatten()\n            ) / torch.count_nonzero(label_mask)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), label_mask.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n            loss_[valid_positions.view(-1)] = loss * label_mask\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~valid_positions]) == 0, (\n                \"NumberTokenLoss computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.NTLoss.forward","title":"<code>forward(logits, labels, loss_mask=None, reduction='mean', ignore_index=-100)</code>","text":"<p>Computes the NTL.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>labels</code> <code>Tensor</code> <p>Tensor of shape BS x T</p> required <code>loss_mask</code> <code>Optional[Tensor]</code> <p>Optional tensor of BS x T</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    labels: Tensor,\n    loss_mask: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    ignore_index: int = -100,\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        labels: Tensor of shape BS x T\n        loss_mask: Optional tensor of BS x T\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n\n    \"\"\"\n\n    if logits.numel() == 0:\n        raise ValueError(\"Logits passed to the NumberTokenLoss are empty!\")\n    if labels.numel() == 0:\n        raise ValueError(\"Labels passed to the NumberTokenLoss are empty!\")\n\n    labels = labels.clone().masked_fill(labels == ignore_index, 0)\n\n    # Create a mask to filter out non-digit tokens\n    y = self.number_values[labels]\n    valid_positions = ~torch.isnan(y)\n\n    # Apply the loss_mask to lower importance of number tokens before the final answer\n    label_mask = (\n        loss_mask[valid_positions]\n        if loss_mask is not None\n        else torch.ones(y.size(), dtype=logits.dtype, device=labels.device)[\n            valid_positions\n        ]\n    )\n\n    # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n    if (torch.count_nonzero(valid_positions) == 0) or (\n        torch.count_nonzero(label_mask) == 0\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=labels.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(valid_positions)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    bs, seq_len, _ = logits.size()\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # compute absolute difference between the true numbers and all possible number values\n    abs_diff = torch.abs(\n        y[valid_positions].unsqueeze(-1) - self.number_values_dense\n    )\n\n    # loss is the absolute difference weighted by the softmax probs\n    loss = (abs_diff * softmax_probs[valid_positions]).sum(dim=-1)\n\n    if reduction == \"mean\":\n        # Mean pooling (weighted by loss mask)\n        loss = torch.dot(\n            loss.flatten(), label_mask.flatten()\n        ) / torch.count_nonzero(label_mask)\n    elif reduction == \"sum\":\n        loss = torch.dot(loss.flatten(), label_mask.flatten())\n    elif reduction == \"none\":\n        # Cast loss for number tokens back to Tensor of size BS x T\n        loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n        loss_[valid_positions.view(-1)] = loss * label_mask\n        loss = loss_.view(bs, seq_len)\n\n        assert torch.sum(loss[~valid_positions]) == 0, (\n            \"NumberTokenLoss computed for non-digit tokens!\"\n        )\n\n    else:\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    return loss\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.NTLossDotProduct","title":"<code>NTLossDotProduct</code>","text":"<p>               Bases: <code>AbstractNTLoss</code></p> <p>Class for NT losses that produce a token-wise numerical output</p> Source code in <code>ntloss/core.py</code> <pre><code>class NTLossDotProduct(AbstractNTLoss):\n    \"\"\"Class for NT losses that produce a token-wise numerical output\"\"\"\n\n    def __init__(\n        self, tokenizer: PreTrainedTokenizer, loss_function: Callable = F.mse_loss\n    ):\n        \"\"\"\n        NTL constructor.\n\n        Args:\n            tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n            loss_function: Function to apply on the delta between the ground truth number\n                and the obtained dot product (nt-probs * token-values).\n\n        \"\"\"\n        super().__init__(tokenizer=tokenizer)\n        self.loss_function = loss_function\n\n    def forward(\n        self,\n        logits: Tensor,\n        labels: Tensor,\n        loss_mask: Optional[Tensor] = None,\n        reduction: str = \"mean\",\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the NTL based on the dot product between token values and their probs.\n\n        Args:\n            logits: Tensor of shape BS x T x V\n            labels: Tensor of shape BS x T\n            loss_mask: Optional tensor of BS x T\n            reduction: Optional string specifying the reduction to apply to the\n                output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n        Returns:\n            Loss tensor\n                0-D if reduction==\"mean\"|\"sum\"\n                BS x T if reduction==\"none\"\n        \"\"\"\n        if logits.numel() == 0:\n            raise ValueError(\"Logits passed to the NTLossDotProduct are empty!\")\n        if labels.numel() == 0:\n            raise ValueError(\"Labels passed to the NTLossDotProduct are empty!\")\n\n        labels = labels.masked_fill(labels == -100, 0)\n\n        # Create a mask to filter out non-digit tokens\n        y = self.number_values[labels]\n        valid_positions = ~torch.isnan(y)\n\n        # Apply the loss_mask to lower importance of number tokens before the final answer\n        label_mask = (\n            loss_mask[valid_positions]\n            if loss_mask is not None\n            else torch.ones(y.size(), dtype=logits.dtype, device=labels.device)[\n                valid_positions\n            ]\n        )\n\n        # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n        if (torch.count_nonzero(valid_positions) == 0) or (\n            torch.count_nonzero(label_mask) == 0\n        ):\n            if (reduction == \"mean\") | (reduction == \"sum\"):\n                loss = torch.tensor(0, dtype=labels.dtype, device=labels.device)\n            elif reduction == \"none\":\n                loss = torch.zeros_like(valid_positions)\n            else:\n                raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n            return loss\n\n        # apply softmax and get number labels\n        bs, seq_len, _ = logits.size()\n        nt_logits = logits[:, :, self.is_number_token]\n        softmax_probs = F.softmax(nt_logits, dim=-1)\n\n        # compute the weighted average of number tokens\n        yhat = torch.sum(\n            softmax_probs[valid_positions] * self.number_values_dense, dim=-1\n        )\n\n        # Apply specified loss function to y and yhat\n        loss = self.loss_function(yhat, y[valid_positions], reduction=\"none\")\n\n        if reduction == \"mean\":\n            # Mean pooling (weighted by loss mask)\n            loss = torch.dot(\n                loss.flatten(), label_mask.flatten()\n            ) / torch.count_nonzero(label_mask)\n        elif reduction == \"sum\":\n            loss = torch.dot(loss.flatten(), label_mask.flatten())\n        elif reduction == \"none\":\n            # Cast loss for number tokens back to Tensor of size BS x T\n            loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n            loss_[valid_positions.view(-1)] = loss * label_mask\n            loss = loss_.view(bs, seq_len)\n\n            assert torch.sum(loss[~valid_positions]) == 0, (\n                \"NTLossDotProduct computed for non-digit tokens!\"\n            )\n\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.NTLossDotProduct.__init__","title":"<code>__init__(tokenizer, loss_function=F.mse_loss)</code>","text":"<p>NTL constructor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>NTLTokenizer with necessary attributes like is_number_token etc.</p> required <code>loss_function</code> <code>Callable</code> <p>Function to apply on the delta between the ground truth number and the obtained dot product (nt-probs * token-values).</p> <code>mse_loss</code> Source code in <code>ntloss/core.py</code> <pre><code>def __init__(\n    self, tokenizer: PreTrainedTokenizer, loss_function: Callable = F.mse_loss\n):\n    \"\"\"\n    NTL constructor.\n\n    Args:\n        tokenizer: NTLTokenizer with necessary attributes like is_number_token etc.\n        loss_function: Function to apply on the delta between the ground truth number\n            and the obtained dot product (nt-probs * token-values).\n\n    \"\"\"\n    super().__init__(tokenizer=tokenizer)\n    self.loss_function = loss_function\n</code></pre>"},{"location":"reference/ntloss/#ntloss.core.NTLossDotProduct.forward","title":"<code>forward(logits, labels, loss_mask=None, reduction='mean')</code>","text":"<p>Computes the NTL based on the dot product between token values and their probs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Tensor of shape BS x T x V</p> required <code>labels</code> <code>Tensor</code> <p>Tensor of shape BS x T</p> required <code>loss_mask</code> <code>Optional[Tensor]</code> <p>Optional tensor of BS x T</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Optional string specifying the reduction to apply to the output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor 0-D if reduction==\"mean\"|\"sum\" BS x T if reduction==\"none\"</p> Source code in <code>ntloss/core.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    labels: Tensor,\n    loss_mask: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n) -&gt; Tensor:\n    \"\"\"\n    Computes the NTL based on the dot product between token values and their probs.\n\n    Args:\n        logits: Tensor of shape BS x T x V\n        labels: Tensor of shape BS x T\n        loss_mask: Optional tensor of BS x T\n        reduction: Optional string specifying the reduction to apply to the\n            output. Defaults to \"mean\", options are \"mean\", \"sum\", \"none\".\n\n    Returns:\n        Loss tensor\n            0-D if reduction==\"mean\"|\"sum\"\n            BS x T if reduction==\"none\"\n    \"\"\"\n    if logits.numel() == 0:\n        raise ValueError(\"Logits passed to the NTLossDotProduct are empty!\")\n    if labels.numel() == 0:\n        raise ValueError(\"Labels passed to the NTLossDotProduct are empty!\")\n\n    labels = labels.masked_fill(labels == -100, 0)\n\n    # Create a mask to filter out non-digit tokens\n    y = self.number_values[labels]\n    valid_positions = ~torch.isnan(y)\n\n    # Apply the loss_mask to lower importance of number tokens before the final answer\n    label_mask = (\n        loss_mask[valid_positions]\n        if loss_mask is not None\n        else torch.ones(y.size(), dtype=logits.dtype, device=labels.device)[\n            valid_positions\n        ]\n    )\n\n    # If no digit tokens in batch, or total of the relevant loss_mask is zero, no need for upcoming calculations\n    if (torch.count_nonzero(valid_positions) == 0) or (\n        torch.count_nonzero(label_mask) == 0\n    ):\n        if (reduction == \"mean\") | (reduction == \"sum\"):\n            loss = torch.tensor(0, dtype=labels.dtype, device=labels.device)\n        elif reduction == \"none\":\n            loss = torch.zeros_like(valid_positions)\n        else:\n            raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n        return loss\n\n    # apply softmax and get number labels\n    bs, seq_len, _ = logits.size()\n    nt_logits = logits[:, :, self.is_number_token]\n    softmax_probs = F.softmax(nt_logits, dim=-1)\n\n    # compute the weighted average of number tokens\n    yhat = torch.sum(\n        softmax_probs[valid_positions] * self.number_values_dense, dim=-1\n    )\n\n    # Apply specified loss function to y and yhat\n    loss = self.loss_function(yhat, y[valid_positions], reduction=\"none\")\n\n    if reduction == \"mean\":\n        # Mean pooling (weighted by loss mask)\n        loss = torch.dot(\n            loss.flatten(), label_mask.flatten()\n        ) / torch.count_nonzero(label_mask)\n    elif reduction == \"sum\":\n        loss = torch.dot(loss.flatten(), label_mask.flatten())\n    elif reduction == \"none\":\n        # Cast loss for number tokens back to Tensor of size BS x T\n        loss_ = torch.zeros(valid_positions.view(-1).size()).to(loss.device)\n        loss_[valid_positions.view(-1)] = loss * label_mask\n        loss = loss_.view(bs, seq_len)\n\n        assert torch.sum(loss[~valid_positions]) == 0, (\n            \"NTLossDotProduct computed for non-digit tokens!\"\n        )\n\n    else:\n        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n\n    return loss\n</code></pre>"},{"location":"reference/ntloss/#ntloss.utils.is_number","title":"<code>is_number(something, finite=True)</code>","text":"<p>Check whether something is convertible to a float</p> <p>Parameters:</p> Name Type Description Default <code>something</code> <code>Any</code> <p>something to test for float casting.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not it's a number</p> Source code in <code>ntloss/utils.py</code> <pre><code>def is_number(something: Any, finite: bool = True) -&gt; bool:\n    \"\"\"Check whether something is convertible to a float\n\n    Args:\n        something: something to test for float casting.\n\n    Returns:\n        Whether or not it's a number\n    \"\"\"\n    try:\n        f = float(something)\n        if finite and not math.isfinite(f):\n            return False\n        return True\n    except ValueError:\n        return False\n</code></pre>"}]}